{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCL Detection — DeBERTa-v3-large with Multi-Task Learning\n",
    "\n",
    "Binary PCL classifier using DeBERTa-v3-large with:\n",
    "- Multi-task learning (PCL categories as auxiliary task)\n",
    "- Three training configurations: Focal Loss, Oversampling, Both\n",
    "- Early stopping on dev F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Auto-detect environment and set batch sizes accordingly\n",
    "ON_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ or DEVICE.type == 'cuda'\n",
    "\n",
    "if ON_COLAB:\n",
    "    BASE_DIR = '/content/drive/MyDrive/PCL_Detection'\n",
    "    BATCH_SIZE = 8\n",
    "    GRAD_ACCUM = 4\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    print('Running on Colab (CUDA) — batch_size=8, grad_accum=4')\n",
    "else:\n",
    "    BASE_DIR = '/Users/alexanderchow/Documents/Y3/60035_NLP/PCL_Detection'\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM = 16\n",
    "    EVAL_BATCH_SIZE = 4\n",
    "    print('Running locally (MPS/CPU) — batch_size=2, grad_accum=16')\n",
    "\n",
    "print(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM}')\n",
    "\n",
    "DATA_DIR = f'{BASE_DIR}/data'\n",
    "SPLITS_DIR = f'{BASE_DIR}/practice splits'\n",
    "CHECKPOINT_DIR = f'{BASE_DIR}/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main PCL dataset (skip 4 header lines)\n",
    "pcl_df = pd.read_csv(\n",
    "    f'{DATA_DIR}/dontpatronizeme_pcl.tsv',\n",
    "    sep='\\t', skiprows=4, header=None,\n",
    "    names=['par_id', 'art_id', 'keyword', 'country_code', 'text', 'label'],\n",
    "    quoting=3\n",
    ")\n",
    "pcl_df['par_id'] = pcl_df['par_id'].astype(int)\n",
    "pcl_df['label'] = pcl_df['label'].astype(int)\n",
    "\n",
    "# Binary label: {0,1}->0, {2,3,4}->1\n",
    "pcl_df['binary_label'] = (pcl_df['label'] >= 2).astype(int)\n",
    "\n",
    "# Clean text: strip <h> tags and HTML artifacts\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)       # remove HTML tags\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)      # remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # normalise whitespace\n",
    "    return text\n",
    "\n",
    "pcl_df['text'] = pcl_df['text'].apply(clean_text)\n",
    "\n",
    "# Load train/dev splits\n",
    "train_splits = pd.read_csv(f'{SPLITS_DIR}/train_semeval_parids-labels.csv')\n",
    "dev_splits = pd.read_csv(f'{SPLITS_DIR}/dev_semeval_parids-labels.csv')\n",
    "train_splits['par_id'] = train_splits['par_id'].astype(int)\n",
    "dev_splits['par_id'] = dev_splits['par_id'].astype(int)\n",
    "\n",
    "# Parse category labels from split files (7-dim multi-label vectors)\n",
    "def parse_category_label(label_str):\n",
    "    try:\n",
    "        return ast.literal_eval(label_str)\n",
    "    except:\n",
    "        return [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "train_splits['category_labels'] = train_splits['label'].apply(parse_category_label)\n",
    "dev_splits['category_labels'] = dev_splits['label'].apply(parse_category_label)\n",
    "\n",
    "# Merge with main data\n",
    "train_ids = set(train_splits['par_id'].values)\n",
    "dev_ids = set(dev_splits['par_id'].values)\n",
    "\n",
    "train_df = pcl_df[pcl_df['par_id'].isin(train_ids)].copy()\n",
    "dev_df = pcl_df[pcl_df['par_id'].isin(dev_ids)].copy()\n",
    "\n",
    "# Merge category labels\n",
    "cat_train = train_splits[['par_id', 'category_labels']].copy()\n",
    "cat_dev = dev_splits[['par_id', 'category_labels']].copy()\n",
    "\n",
    "train_df = train_df.merge(cat_train, on='par_id', how='left')\n",
    "dev_df = dev_df.merge(cat_dev, on='par_id', how='left')\n",
    "\n",
    "# Fill missing category labels with zeros\n",
    "train_df['category_labels'] = train_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "dev_df['category_labels'] = dev_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_df)} samples ({train_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'Dev:   {len(dev_df)} samples ({dev_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'\\nTrain class distribution:')\n",
    "print(train_df['binary_label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'microsoft/deberta-v3-large'\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, binary_labels, category_labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.binary_labels = binary_labels\n",
    "        self.category_labels = category_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'binary_label': torch.tensor(self.binary_labels[idx], dtype=torch.long),\n",
    "            'category_labels': torch.tensor(self.category_labels[idx], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def create_datasets(train_df, dev_df, tokenizer, max_length):\n",
    "    train_dataset = PCLDataset(\n",
    "        texts=train_df['text'].tolist(),\n",
    "        binary_labels=train_df['binary_label'].tolist(),\n",
    "        category_labels=train_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    dev_dataset = PCLDataset(\n",
    "        texts=dev_df['text'].tolist(),\n",
    "        binary_labels=dev_df['binary_label'].tolist(),\n",
    "        category_labels=dev_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return train_dataset, dev_dataset\n",
    "\n",
    "def create_oversampled_df(df, oversample_factor=4):\n",
    "    \"\"\"Oversample minority class (PCL=1) by duplicating examples.\"\"\"\n",
    "    minority = df[df['binary_label'] == 1]\n",
    "    majority = df[df['binary_label'] == 0]\n",
    "    minority_oversampled = pd.concat([minority] * oversample_factor, ignore_index=True)\n",
    "    oversampled = pd.concat([majority, minority_oversampled], ignore_index=True)\n",
    "    oversampled = oversampled.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    print(f'  Oversampled: {len(oversampled)} samples ({oversampled[\"binary_label\"].sum()} PCL)')\n",
    "    return oversampled\n",
    "\n",
    "print(f'Tokenizer loaded: {MODEL_NAME}')\n",
    "print(f'Max length: {MAX_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal loss for handling class imbalance.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float()\n",
    "        pt = (probs * targets_one_hot).sum(dim=1)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha.to(logits.device)\n",
    "            alpha_t = alpha[targets]\n",
    "            focal_weight = focal_weight * alpha_t\n",
    "\n",
    "        return (focal_weight * ce_loss).mean()\n",
    "\n",
    "\n",
    "class PCLMultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name, num_categories=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        self.binary_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "        self.category_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_categories)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        binary_logits = self.binary_head(cls_output)\n",
    "        category_logits = self.category_head(cls_output)\n",
    "\n",
    "        return binary_logits, category_logits\n",
    "\n",
    "print('Model class defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every_updates = 20\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on a dataset, return metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['binary_label']\n",
    "\n",
    "            binary_logits, _ = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(binary_logits, dim=1).cpu()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
    "    precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "\n",
    "    return {'f1': f1, 'precision': precision, 'recall': recall, 'preds': all_preds, 'labels': all_labels}\n",
    "\n",
    "\n",
    "def train_model(config_name, train_df, dev_df, tokenizer, use_focal_loss=True,\n",
    "                use_oversampling=False, oversample_factor=4,\n",
    "                num_epochs=10, batch_size=BATCH_SIZE, grad_accum_steps=GRAD_ACCUM,\n",
    "                lr=1e-5, weight_decay=0.01, patience=3, category_weight=0.3):\n",
    "    \"\"\"Train a PCLMultiTaskModel with the given configuration.\"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training Config: {config_name}')\n",
    "    print(f'  Focal Loss: {use_focal_loss} | Oversampling: {use_oversampling}')\n",
    "    print(f'  Epochs: {num_epochs} | Batch: {batch_size} | Grad Accum: {grad_accum_steps}')\n",
    "    print(f'  Effective batch size: {batch_size * grad_accum_steps}')\n",
    "    print(f'  LR: {lr} | Weight Decay: {weight_decay} | Patience: {patience}')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    # Prepare training data\n",
    "    if use_oversampling:\n",
    "        effective_train_df = create_oversampled_df(train_df, oversample_factor)\n",
    "    else:\n",
    "        effective_train_df = train_df.copy()\n",
    "\n",
    "    train_dataset, dev_dataset = create_datasets(effective_train_df, dev_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    model = PCLMultiTaskModel(MODEL_NAME).to(DEVICE).float()\n",
    "\n",
    "    # Loss functions\n",
    "    if use_focal_loss:\n",
    "        n_neg = (effective_train_df['binary_label'] == 0).sum()\n",
    "        n_pos = (effective_train_df['binary_label'] == 1).sum()\n",
    "        alpha_pos = n_neg / (n_neg + n_pos)\n",
    "        alpha_neg = n_pos / (n_neg + n_pos)\n",
    "        binary_criterion = FocalLoss(alpha=[alpha_neg, alpha_pos], gamma=2.0)\n",
    "        print(f'  Focal Loss alpha: [{alpha_neg:.3f}, {alpha_pos:.3f}]')\n",
    "    else:\n",
    "        n_neg = (effective_train_df['binary_label'] == 0).sum()\n",
    "        n_pos = (effective_train_df['binary_label'] == 1).sum()\n",
    "        weight = torch.tensor([1.0, n_neg / n_pos], dtype=torch.float).to(DEVICE)\n",
    "        binary_criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        print(f'  CE class weights: [{weight[0]:.3f}, {weight[1]:.3f}]')\n",
    "\n",
    "    category_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * num_epochs // grad_accum_steps\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            binary_labels = batch['binary_label'].to(DEVICE)\n",
    "            category_labels = batch['category_labels'].to(DEVICE)\n",
    "\n",
    "            binary_logits, category_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_binary = binary_criterion(binary_logits, binary_labels)\n",
    "            loss_category = category_criterion(category_logits, category_labels)\n",
    "            loss = loss_binary + category_weight * loss_category\n",
    "            loss = loss / grad_accum_steps\n",
    "\n",
    "            loss.backward()\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            if (step + 1) % grad_accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                update = (step + 1) // grad_accum_steps\n",
    "                if update % print_every_updates == 0:\n",
    "                    # average loss over the last `print_every_updates` updates (approx)\n",
    "                    avg_recent = total_loss / (step + 1)\n",
    "                    print(f\"    step {step+1}/{len(train_loader)} \"\n",
    "                          f\"(update {update}) | avg loss so far: {avg_recent:.4f}\")\n",
    "                if torch.device.type == \"cuda\":\n",
    "                  mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                  print(f\"    ... | GPU allocated: {mem:.2f} GiB\")\n",
    "\n",
    "        # Handle remaining gradients\n",
    "        if (step + 1) % grad_accum_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on dev\n",
    "        metrics = evaluate(model, dev_loader, DEVICE)\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'f1': metrics['f1'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall']\n",
    "        })\n",
    "\n",
    "        print(f'  Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f} | '\n",
    "              f'F1: {metrics[\"f1\"]:.4f} | P: {metrics[\"precision\"]:.4f} | R: {metrics[\"recall\"]:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            patience_counter = 0\n",
    "            save_path = f'{CHECKPOINT_DIR}/{config_name}_best.pt'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'  -> New best F1! Model saved to {save_path}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'  Early stopping at epoch {epoch+1} (patience={patience})')\n",
    "                break\n",
    "\n",
    "    # Load best model and get final dev metrics\n",
    "    model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{config_name}_best.pt', weights_only=True))\n",
    "    final_metrics = evaluate(model, dev_loader, DEVICE)\n",
    "    print(f'\\n  Final Dev Metrics ({config_name}):')\n",
    "    print(f'    F1: {final_metrics[\"f1\"]:.4f} | P: {final_metrics[\"precision\"]:.4f} | R: {final_metrics[\"recall\"]:.4f}')\n",
    "    print(classification_report(\n",
    "        final_metrics['labels'], final_metrics['preds'],\n",
    "        target_names=['No PCL', 'PCL'], digits=4\n",
    "    ))\n",
    "\n",
    "    return model, final_metrics, history\n",
    "\n",
    "print('Training function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Three Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config A: Focal Loss only (no oversampling)\n",
    "model_a, metrics_a, history_a = train_model(\n",
    "    config_name='config_A_focal',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=True,\n",
    "    use_oversampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config B: Oversampling only (standard weighted CE loss)\n",
    "model_b, metrics_b, history_b = train_model(\n",
    "    config_name='config_B_oversample',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=False,\n",
    "    use_oversampling=True,\n",
    "    oversample_factor=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config C: Focal Loss + Oversampling\n",
    "model_c, metrics_c, history_c = train_model(\n",
    "    config_name='config_C_focal_oversample',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=True,\n",
    "    use_oversampling=True,\n",
    "    oversample_factor=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Config': ['A: Focal Loss', 'B: Oversampling', 'C: Focal + Oversample', 'Baseline (RoBERTa-base)'],\n",
    "    'F1': [metrics_a['f1'], metrics_b['f1'], metrics_c['f1'], 0.48],\n",
    "    'Precision': [metrics_a['precision'], metrics_b['precision'], metrics_c['precision'], None],\n",
    "    'Recall': [metrics_a['recall'], metrics_b['recall'], metrics_c['recall'], None],\n",
    "})\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('RESULTS COMPARISON')\n",
    "print('='*60)\n",
    "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "config_metrics = {'A': metrics_a, 'B': metrics_b, 'C': metrics_c}\n",
    "best_config = max(config_metrics, key=lambda k: config_metrics[k]['f1'])\n",
    "print(f'\\nBest config: {best_config} (F1={config_metrics[best_config][\"f1\"]:.4f})')\n",
    "print(f'Beats baseline: {config_metrics[best_config][\"f1\"] > 0.48}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/task4_test.tsv', sep='\\t', header=None,\n",
    "                       names=['par_id', 'art_id', 'keyword', 'country_code', 'text'])\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "print(f'Test set: {len(test_df)} samples')\n",
    "\n",
    "# Load best model\n",
    "config_name_map = {'A': 'config_A_focal', 'B': 'config_B_oversample', 'C': 'config_C_focal_oversample'}\n",
    "best_model = PCLMultiTaskModel(MODEL_NAME).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(\n",
    "    f'{CHECKPOINT_DIR}/{config_name_map[best_config]}_best.pt',\n",
    "    weights_only=True,\n",
    "    map_location=DEVICE\n",
    "))\n",
    "best_model.eval()\n",
    "\n",
    "# Create test dataset (dummy labels)\n",
    "test_dataset = PCLDataset(\n",
    "    texts=test_df['text'].tolist(),\n",
    "    binary_labels=[0] * len(test_df),\n",
    "    category_labels=[[0]*7] * len(test_df),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Generate predictions\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        binary_logits, _ = best_model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(binary_logits, dim=1).cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "test_df['prediction'] = all_preds\n",
    "\n",
    "# Save predictions\n",
    "output_path = f'{BASE_DIR}/test_predictions.tsv'\n",
    "test_df[['par_id', 'prediction']].to_csv(output_path, sep='\\t', index=False, header=False)\n",
    "print(f'\\nPredictions saved to {output_path}')\n",
    "print(f'Prediction distribution: {pd.Series(all_preds).value_counts().sort_index().to_dict()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
