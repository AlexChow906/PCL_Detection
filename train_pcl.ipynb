{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCL Detection — DeBERTa-v3-large with Multi-Task Learning\n",
    "\n",
    "Binary PCL classifier using DeBERTa-v3-large with:\n",
    "- Multi-task learning (PCL categories as auxiliary task)\n",
    "- Three training configurations: Focal Loss, Oversampling, Both\n",
    "- Early stopping on dev F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport ast\nimport re\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve, confusion_matrix\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Device\nif torch.cuda.is_available():\n    DEVICE = torch.device('cuda')\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    DEVICE = torch.device('mps')\nelse:\n    DEVICE = torch.device('cpu')\n\nprint(f'Device: {DEVICE}')\n\n# Auto-detect environment and set batch sizes accordingly\nON_GPUDOJO = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ or DEVICE.type == 'cuda'\n\nif ON_GPUDOJO:\n    BASE_DIR = '/home/azureuser/PCL_Detection'\n    BATCH_SIZE = 2\n    GRAD_ACCUM = 16\n    EVAL_BATCH_SIZE = 16\n    print('Running on GPUDOJO (CUDA) — batch_size=2, grad_accum=16')\nelse:\n    BASE_DIR = '/Users/alexanderchow/Documents/Y3/60035_NLP/PCL_Detection'\n    BATCH_SIZE = 2\n    GRAD_ACCUM = 16\n    EVAL_BATCH_SIZE = 4\n    print('Running locally (MPS/CPU) — batch_size=2, grad_accum=16')\n\nprint(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM}')\n\nDATA_DIR = f'{BASE_DIR}/data'\nSPLITS_DIR = f'{BASE_DIR}/practice splits'\nCHECKPOINT_DIR = f'{BASE_DIR}/checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8375 samples (794 PCL)\n",
      "Dev:   2094 samples (199 PCL)\n",
      "\n",
      "Train class distribution:\n",
      "binary_label\n",
      "0    7581\n",
      "1     794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load main PCL dataset (skip 4 header lines)\n",
    "pcl_df = pd.read_csv(\n",
    "    f'{DATA_DIR}/dontpatronizeme_pcl.tsv',\n",
    "    sep='\\t', skiprows=4, header=None,\n",
    "    names=['par_id', 'art_id', 'keyword', 'country_code', 'text', 'label'],\n",
    "    quoting=3\n",
    ")\n",
    "pcl_df['par_id'] = pcl_df['par_id'].astype(int)\n",
    "pcl_df['label'] = pcl_df['label'].astype(int)\n",
    "\n",
    "# Binary label: {0,1}->0, {2,3,4}->1\n",
    "pcl_df['binary_label'] = (pcl_df['label'] >= 2).astype(int)\n",
    "\n",
    "# Clean text: strip <h> tags and HTML artifacts\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)       # remove HTML tags\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)      # remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # normalise whitespace\n",
    "    return text\n",
    "\n",
    "pcl_df['text'] = pcl_df['text'].apply(clean_text)\n",
    "\n",
    "# Load train/dev splits\n",
    "train_splits = pd.read_csv(f'{SPLITS_DIR}/train_semeval_parids-labels.csv')\n",
    "dev_splits = pd.read_csv(f'{SPLITS_DIR}/dev_semeval_parids-labels.csv')\n",
    "train_splits['par_id'] = train_splits['par_id'].astype(int)\n",
    "dev_splits['par_id'] = dev_splits['par_id'].astype(int)\n",
    "\n",
    "# Parse category labels from split files (7-dim multi-label vectors)\n",
    "def parse_category_label(label_str):\n",
    "    try:\n",
    "        return ast.literal_eval(label_str)\n",
    "    except:\n",
    "        return [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "train_splits['category_labels'] = train_splits['label'].apply(parse_category_label)\n",
    "dev_splits['category_labels'] = dev_splits['label'].apply(parse_category_label)\n",
    "\n",
    "# Merge with main data\n",
    "train_ids = set(train_splits['par_id'].values)\n",
    "dev_ids = set(dev_splits['par_id'].values)\n",
    "\n",
    "train_df = pcl_df[pcl_df['par_id'].isin(train_ids)].copy()\n",
    "dev_df = pcl_df[pcl_df['par_id'].isin(dev_ids)].copy()\n",
    "\n",
    "# Merge category labels\n",
    "cat_train = train_splits[['par_id', 'category_labels']].copy()\n",
    "cat_dev = dev_splits[['par_id', 'category_labels']].copy()\n",
    "\n",
    "train_df = train_df.merge(cat_train, on='par_id', how='left')\n",
    "dev_df = dev_df.merge(cat_dev, on='par_id', how='left')\n",
    "\n",
    "# Fill missing category labels with zeros\n",
    "train_df['category_labels'] = train_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "dev_df['category_labels'] = dev_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_df)} samples ({train_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'Dev:   {len(dev_df)} samples ({dev_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'\\nTrain class distribution:')\n",
    "print(train_df['binary_label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: microsoft/deberta-v3-large\n",
      "Max length: 256\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'microsoft/deberta-v3-large'\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, binary_labels, category_labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.binary_labels = binary_labels\n",
    "        self.category_labels = category_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'binary_label': torch.tensor(self.binary_labels[idx], dtype=torch.long),\n",
    "            'category_labels': torch.tensor(self.category_labels[idx], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def create_datasets(train_df, dev_df, tokenizer, max_length):\n",
    "    train_dataset = PCLDataset(\n",
    "        texts=train_df['text'].tolist(),\n",
    "        binary_labels=train_df['binary_label'].tolist(),\n",
    "        category_labels=train_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    dev_dataset = PCLDataset(\n",
    "        texts=dev_df['text'].tolist(),\n",
    "        binary_labels=dev_df['binary_label'].tolist(),\n",
    "        category_labels=dev_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return train_dataset, dev_dataset\n",
    "\n",
    "def create_oversampled_df(df, oversample_factor=4):\n",
    "    \"\"\"Oversample minority class (PCL=1) by duplicating examples.\"\"\"\n",
    "    minority = df[df['binary_label'] == 1]\n",
    "    majority = df[df['binary_label'] == 0]\n",
    "    minority_oversampled = pd.concat([minority] * oversample_factor, ignore_index=True)\n",
    "    oversampled = pd.concat([majority, minority_oversampled], ignore_index=True)\n",
    "    oversampled = oversampled.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    print(f'  Oversampled: {len(oversampled)} samples ({oversampled[\"binary_label\"].sum()} PCL)')\n",
    "    return oversampled\n",
    "\n",
    "print(f'Tokenizer loaded: {MODEL_NAME}')\n",
    "print(f'Max length: {MAX_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal loss for handling class imbalance.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float()\n",
    "        pt = (probs * targets_one_hot).sum(dim=1)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha.to(logits.device)\n",
    "            alpha_t = alpha[targets]\n",
    "            focal_weight = focal_weight * alpha_t\n",
    "\n",
    "        return (focal_weight * ce_loss).mean()\n",
    "\n",
    "\n",
    "class PCLMultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name, num_categories=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        self.binary_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "        self.category_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_categories)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        binary_logits = self.binary_head(cls_output)\n",
    "        category_logits = self.category_head(cls_output)\n",
    "\n",
    "        return binary_logits, category_logits\n",
    "\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"Simple RoBERTa-base binary classifier (baseline).\"\"\"\n",
    "    def __init__(self, model_name='roberta-base', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits, None  # None for compatibility with evaluate()\n",
    "\n",
    "print('Model classes defined: PCLMultiTaskModel, BaselineModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every_updates = 20\n",
    "\n",
    "def evaluate(model, dataloader, device, threshold=0.5):\n",
    "    \"\"\"Evaluate model on a dataset, return metrics and probabilities.\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['binary_label']\n",
    "\n",
    "            binary_logits, _ = model(input_ids, attention_mask)\n",
    "            probs = F.softmax(binary_logits, dim=1)[:, 1].cpu()\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    all_preds = [1 if p >= threshold else 0 for p in all_probs]\n",
    "    f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
    "    precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'f1': f1, 'precision': precision, 'recall': recall,\n",
    "        'preds': all_preds, 'labels': all_labels, 'probs': all_probs,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_threshold(probs, labels):\n",
    "    \"\"\"Sweep thresholds on probability outputs to maximise F1.\"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_threshold = 0.5\n",
    "    for t in np.arange(0.05, 0.95, 0.01):\n",
    "        preds = [1 if p >= t else 0 for p in probs]\n",
    "        f1 = f1_score(labels, preds, pos_label=1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "def train_model(config_name, train_df, dev_df, tokenizer, use_focal_loss=True,\n",
    "                use_oversampling=False, oversample_factor=4,\n",
    "                num_epochs=10, batch_size=BATCH_SIZE, grad_accum_steps=GRAD_ACCUM,\n",
    "                lr=1e-5, weight_decay=0.01, patience=3, category_weight=0.3,\n",
    "                model_class=PCLMultiTaskModel, model_name=MODEL_NAME,\n",
    "                use_multitask=True):\n",
    "    \"\"\"Train a model with the given configuration.\"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training Config: {config_name}')\n",
    "    print(f'  Model: {model_name}')\n",
    "    print(f'  Focal Loss: {use_focal_loss} | Oversampling: {use_oversampling} | Multi-task: {use_multitask}')\n",
    "    print(f'  Epochs: {num_epochs} | Batch: {batch_size} | Grad Accum: {grad_accum_steps}')\n",
    "    print(f'  Effective batch size: {batch_size * grad_accum_steps}')\n",
    "    print(f'  LR: {lr} | Weight Decay: {weight_decay} | Patience: {patience}')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    # Prepare training data\n",
    "    if use_oversampling:\n",
    "        effective_train_df = create_oversampled_df(train_df, oversample_factor)\n",
    "    else:\n",
    "        effective_train_df = train_df.copy()\n",
    "\n",
    "    # Create tokenizer for this model\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = PCLDataset(\n",
    "        texts=effective_train_df['text'].tolist(),\n",
    "        binary_labels=effective_train_df['binary_label'].tolist(),\n",
    "        category_labels=effective_train_df['category_labels'].tolist(),\n",
    "        tokenizer=tok, max_length=MAX_LENGTH\n",
    "    )\n",
    "    dev_dataset = PCLDataset(\n",
    "        texts=dev_df['text'].tolist(),\n",
    "        binary_labels=dev_df['binary_label'].tolist(),\n",
    "        category_labels=dev_df['category_labels'].tolist(),\n",
    "        tokenizer=tok, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    if model_class == BaselineModel:\n",
    "        model = BaselineModel(model_name=model_name).to(DEVICE).float()\n",
    "    else:\n",
    "        model = PCLMultiTaskModel(model_name=model_name).to(DEVICE).float()\n",
    "\n",
    "    # Loss functions\n",
    "    if use_focal_loss:\n",
    "        n_neg = (effective_train_df['binary_label'] == 0).sum()\n",
    "        n_pos = (effective_train_df['binary_label'] == 1).sum()\n",
    "        alpha_pos = n_neg / (n_neg + n_pos)\n",
    "        alpha_neg = n_pos / (n_neg + n_pos)\n",
    "        binary_criterion = FocalLoss(alpha=[alpha_neg, alpha_pos], gamma=2.0)\n",
    "        print(f'  Focal Loss alpha: [{alpha_neg:.3f}, {alpha_pos:.3f}]')\n",
    "    else:\n",
    "        n_neg = (effective_train_df['binary_label'] == 0).sum()\n",
    "        n_pos = (effective_train_df['binary_label'] == 1).sum()\n",
    "        weight = torch.tensor([1.0, n_neg / n_pos], dtype=torch.float).to(DEVICE)\n",
    "        binary_criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        print(f'  CE class weights: [{weight[0]:.3f}, {weight[1]:.3f}]')\n",
    "\n",
    "    category_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * num_epochs // grad_accum_steps\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            binary_labels = batch['binary_label'].to(DEVICE)\n",
    "            category_labels = batch['category_labels'].to(DEVICE)\n",
    "\n",
    "            binary_logits, category_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_binary = binary_criterion(binary_logits, binary_labels)\n",
    "            if use_multitask and category_logits is not None:\n",
    "                loss_category = category_criterion(category_logits, category_labels)\n",
    "                loss = loss_binary + category_weight * loss_category\n",
    "            else:\n",
    "                loss = loss_binary\n",
    "            loss = loss / grad_accum_steps\n",
    "\n",
    "            loss.backward()\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            if (step + 1) % grad_accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                update = (step + 1) // grad_accum_steps\n",
    "                if update % print_every_updates == 0:\n",
    "                    avg_recent = total_loss / (step + 1)\n",
    "                    print(f\"    step {step+1}/{len(train_loader)} \"\n",
    "                          f\"(update {update}) | avg loss so far: {avg_recent:.4f}\")\n",
    "\n",
    "        # Handle remaining gradients\n",
    "        if (step + 1) % grad_accum_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on dev (using default threshold 0.5 for early stopping)\n",
    "        metrics = evaluate(model, dev_loader, DEVICE, threshold=0.5)\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'f1': metrics['f1'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall']\n",
    "        })\n",
    "\n",
    "        print(f'  Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f} | '\n",
    "              f'F1: {metrics[\"f1\"]:.4f} | P: {metrics[\"precision\"]:.4f} | R: {metrics[\"recall\"]:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            patience_counter = 0\n",
    "            save_path = f'{CHECKPOINT_DIR}/{config_name}_best.pt'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'  -> New best F1! Model saved to {save_path}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'  Early stopping at epoch {epoch+1} (patience={patience})')\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{config_name}_best.pt', weights_only=True))\n",
    "\n",
    "    # Standard evaluation at threshold=0.5\n",
    "    final_metrics = evaluate(model, dev_loader, DEVICE, threshold=0.5)\n",
    "\n",
    "    # Threshold optimisation\n",
    "    best_thresh, best_thresh_f1 = find_best_threshold(final_metrics['probs'], final_metrics['labels'])\n",
    "    thresh_metrics = evaluate(model, dev_loader, DEVICE, threshold=best_thresh)\n",
    "\n",
    "    print(f'\\n  Dev Metrics @ threshold=0.50:')\n",
    "    print(f'    F1: {final_metrics[\"f1\"]:.4f} | P: {final_metrics[\"precision\"]:.4f} | R: {final_metrics[\"recall\"]:.4f}')\n",
    "    print(f'  Dev Metrics @ threshold={best_thresh:.2f} (optimised):')\n",
    "    print(f'    F1: {thresh_metrics[\"f1\"]:.4f} | P: {thresh_metrics[\"precision\"]:.4f} | R: {thresh_metrics[\"recall\"]:.4f}')\n",
    "    print(classification_report(\n",
    "        thresh_metrics['labels'], thresh_metrics['preds'],\n",
    "        target_names=['No PCL', 'PCL'], digits=4\n",
    "    ))\n",
    "\n",
    "    return model, final_metrics, thresh_metrics, history, best_thresh, tok\n",
    "\n",
    "print('Training function defined (with threshold optimisation).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RoBERTa-base Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa-base baseline: simple weighted CE, no multi-task, no oversampling\n",
    "BASELINE_MODEL = 'roberta-base'\n",
    "\n",
    "model_baseline, metrics_baseline, thresh_metrics_baseline, history_baseline, thresh_baseline, tok_baseline = train_model(\n",
    "    config_name='baseline_roberta',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=False,\n",
    "    use_oversampling=False,\n",
    "    model_class=BaselineModel,\n",
    "    model_name=BASELINE_MODEL,\n",
    "    use_multitask=False,\n",
    "    lr=2e-5,\n",
    "    num_epochs=10,\n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DeBERTa-v3-large Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config A: Focal Loss + Multi-task (no oversampling)\n",
    "model_a, metrics_a, thresh_metrics_a, history_a, thresh_a, tok_a = train_model(\n",
    "    config_name='config_A_focal',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=True,\n",
    "    use_oversampling=False,\n",
    "    use_multitask=True,\n",
    "    lr=1e-5,\n",
    "    num_epochs=10,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Config B: Oversampling + Multi-task (weighted CE)\n",
    "model_b, metrics_b, thresh_metrics_b, history_b, thresh_b, tok_b = train_model(\n",
    "    config_name='config_B_oversample',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=False,\n",
    "    use_oversampling=True,\n",
    "    oversample_factor=4,\n",
    "    use_multitask=True,\n",
    "    lr=1e-5,\n",
    "    num_epochs=10,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Config C: Focal Loss + Oversampling + Multi-task\n",
    "model_c, metrics_c, thresh_metrics_c, history_c, thresh_c, tok_c = train_model(\n",
    "    config_name='config_C_focal_oversample',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=True,\n",
    "    use_oversampling=True,\n",
    "    oversample_factor=4,\n",
    "    use_multitask=True,\n",
    "    lr=1e-5,\n",
    "    num_epochs=10,\n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ablation: DeBERTa without Multi-task Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation: best DeBERTa config WITHOUT multi-task learning\n",
    "# (determines how much the auxiliary category task helps)\n",
    "# We pick the best config from A/B/C and re-run without multi-task\n",
    "\n",
    "config_results = {'A': thresh_metrics_a, 'B': thresh_metrics_b, 'C': thresh_metrics_c}\n",
    "best_deberta_config = max(config_results, key=lambda k: config_results[k]['f1'])\n",
    "print(f'Best DeBERTa config: {best_deberta_config} — re-running without multi-task for ablation')\n",
    "\n",
    "ablation_params = {\n",
    "    'A': dict(use_focal_loss=True, use_oversampling=False),\n",
    "    'B': dict(use_focal_loss=False, use_oversampling=True, oversample_factor=4),\n",
    "    'C': dict(use_focal_loss=True, use_oversampling=True, oversample_factor=4),\n",
    "}\n",
    "\n",
    "model_abl, metrics_abl, thresh_metrics_abl, history_abl, thresh_abl, tok_abl = train_model(\n",
    "    config_name='ablation_no_multitask',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_multitask=False,\n",
    "    lr=1e-5,\n",
    "    num_epochs=10,\n",
    "    patience=3,\n",
    "    **ablation_params[best_deberta_config]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Comparison & Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Results comparison table ----\n",
    "results = pd.DataFrame([\n",
    "    {\n",
    "        'Config': 'Baseline: RoBERTa-base',\n",
    "        'Threshold': f'{thresh_baseline:.2f}',\n",
    "        'F1 (t=0.5)': metrics_baseline['f1'],\n",
    "        'F1 (optimised)': thresh_metrics_baseline['f1'],\n",
    "        'Precision': thresh_metrics_baseline['precision'],\n",
    "        'Recall': thresh_metrics_baseline['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'A: DeBERTa + Focal Loss',\n",
    "        'Threshold': f'{thresh_a:.2f}',\n",
    "        'F1 (t=0.5)': metrics_a['f1'],\n",
    "        'F1 (optimised)': thresh_metrics_a['f1'],\n",
    "        'Precision': thresh_metrics_a['precision'],\n",
    "        'Recall': thresh_metrics_a['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'B: DeBERTa + Oversampling',\n",
    "        'Threshold': f'{thresh_b:.2f}',\n",
    "        'F1 (t=0.5)': metrics_b['f1'],\n",
    "        'F1 (optimised)': thresh_metrics_b['f1'],\n",
    "        'Precision': thresh_metrics_b['precision'],\n",
    "        'Recall': thresh_metrics_b['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'C: DeBERTa + Focal + Oversample',\n",
    "        'Threshold': f'{thresh_c:.2f}',\n",
    "        'F1 (t=0.5)': metrics_c['f1'],\n",
    "        'F1 (optimised)': thresh_metrics_c['f1'],\n",
    "        'Precision': thresh_metrics_c['precision'],\n",
    "        'Recall': thresh_metrics_c['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': f'Ablation: Best ({best_deberta_config}) w/o multi-task',\n",
    "        'Threshold': f'{thresh_abl:.2f}',\n",
    "        'F1 (t=0.5)': metrics_abl['f1'],\n",
    "        'F1 (optimised)': thresh_metrics_abl['f1'],\n",
    "        'Precision': thresh_metrics_abl['precision'],\n",
    "        'Recall': thresh_metrics_abl['recall'],\n",
    "    },\n",
    "])\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('RESULTS COMPARISON (all models evaluated on dev set)')\n",
    "print('='*80)\n",
    "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "# Select best overall model\n",
    "all_configs = {\n",
    "    'baseline': (model_baseline, thresh_metrics_baseline, thresh_baseline, tok_baseline, 'baseline_roberta', BaselineModel, BASELINE_MODEL),\n",
    "    'A': (model_a, thresh_metrics_a, thresh_a, tok_a, 'config_A_focal', PCLMultiTaskModel, MODEL_NAME),\n",
    "    'B': (model_b, thresh_metrics_b, thresh_b, tok_b, 'config_B_oversample', PCLMultiTaskModel, MODEL_NAME),\n",
    "    'C': (model_c, thresh_metrics_c, thresh_c, tok_c, 'config_C_focal_oversample', PCLMultiTaskModel, MODEL_NAME),\n",
    "    'ablation': (model_abl, thresh_metrics_abl, thresh_abl, tok_abl, 'ablation_no_multitask', PCLMultiTaskModel, MODEL_NAME),\n",
    "}\n",
    "\n",
    "best_key = max(all_configs, key=lambda k: all_configs[k][1]['f1'])\n",
    "best_model, best_metrics, best_threshold, best_tok, best_ckpt_name, best_model_class, best_model_name = all_configs[best_key]\n",
    "\n",
    "print(f'\\n** Best model: {best_key} (F1={best_metrics[\"f1\"]:.4f} @ threshold={best_threshold:.2f}) **')\n",
    "improvement = best_metrics['f1'] - thresh_metrics_baseline['f1']\n",
    "print(f'   Improvement over baseline: +{improvement:.4f} F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate dev.txt and test.txt Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Dev predictions (using best model + optimised threshold) ----\n",
    "dev_preds = best_metrics['preds']\n",
    "dev_pred_path = f'{BASE_DIR}/dev.txt'\n",
    "with open(dev_pred_path, 'w') as f:\n",
    "    for p in dev_preds:\n",
    "        f.write(f'{p}\\n')\n",
    "print(f'Dev predictions saved to {dev_pred_path}')\n",
    "print(f'  {len(dev_preds)} predictions, {sum(dev_preds)} predicted PCL')\n",
    "\n",
    "# ---- Test predictions ----\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/task4_test.tsv', sep='\\t', header=None,\n",
    "                       names=['par_id', 'art_id', 'keyword', 'country_code', 'text'])\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "print(f'\\nTest set: {len(test_df)} samples')\n",
    "\n",
    "# Create test dataset with best model's tokenizer\n",
    "test_dataset = PCLDataset(\n",
    "    texts=test_df['text'].tolist(),\n",
    "    binary_labels=[0] * len(test_df),\n",
    "    category_labels=[[0]*7] * len(test_df),\n",
    "    tokenizer=best_tok,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Generate test predictions with optimised threshold\n",
    "best_model.eval()\n",
    "test_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        binary_logits, _ = best_model(input_ids, attention_mask)\n",
    "        probs = F.softmax(binary_logits, dim=1)[:, 1].cpu().tolist()\n",
    "        test_probs.extend(probs)\n",
    "\n",
    "test_preds = [1 if p >= best_threshold else 0 for p in test_probs]\n",
    "\n",
    "test_pred_path = f'{BASE_DIR}/test.txt'\n",
    "with open(test_pred_path, 'w') as f:\n",
    "    for p in test_preds:\n",
    "        f.write(f'{p}\\n')\n",
    "print(f'Test predictions saved to {test_pred_path}')\n",
    "print(f'  {len(test_preds)} predictions, {sum(test_preds)} predicted PCL')\n",
    "print(f'  Using threshold: {best_threshold:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Error analysis: compare baseline vs best model ----\n",
    "# Get baseline predictions on dev\n",
    "baseline_dev_metrics = evaluate(model_baseline, \n",
    "    DataLoader(PCLDataset(dev_df['text'].tolist(), dev_df['binary_label'].tolist(),\n",
    "                          dev_df['category_labels'].tolist(), tok_baseline, MAX_LENGTH),\n",
    "               batch_size=EVAL_BATCH_SIZE, shuffle=False),\n",
    "    DEVICE, threshold=thresh_baseline)\n",
    "\n",
    "baseline_preds = baseline_dev_metrics['preds']\n",
    "best_preds = best_metrics['preds']\n",
    "true_labels = best_metrics['labels']\n",
    "dev_texts = dev_df['text'].tolist()\n",
    "\n",
    "# Confusion categories\n",
    "print('='*60)\n",
    "print('ERROR ANALYSIS: Baseline vs Best Model on Dev Set')\n",
    "print('='*60)\n",
    "\n",
    "# Count error types\n",
    "baseline_fp = sum(1 for t, p in zip(true_labels, baseline_preds) if t == 0 and p == 1)\n",
    "baseline_fn = sum(1 for t, p in zip(true_labels, baseline_preds) if t == 1 and p == 0)\n",
    "baseline_tp = sum(1 for t, p in zip(true_labels, baseline_preds) if t == 1 and p == 1)\n",
    "best_fp = sum(1 for t, p in zip(true_labels, best_preds) if t == 0 and p == 1)\n",
    "best_fn = sum(1 for t, p in zip(true_labels, best_preds) if t == 1 and p == 0)\n",
    "best_tp = sum(1 for t, p in zip(true_labels, best_preds) if t == 1 and p == 1)\n",
    "\n",
    "print(f'\\n{\"Metric\":<25} {\"Baseline\":>10} {\"Best Model\":>12}')\n",
    "print('-' * 50)\n",
    "print(f'{\"True Positives\":<25} {baseline_tp:>10} {best_tp:>12}')\n",
    "print(f'{\"False Positives\":<25} {baseline_fp:>10} {best_fp:>12}')\n",
    "print(f'{\"False Negatives\":<25} {baseline_fn:>10} {best_fn:>12}')\n",
    "\n",
    "# Examples where best model is correct but baseline is wrong\n",
    "fixed_fn = []  # false negatives fixed by best model\n",
    "fixed_fp = []  # false positives fixed by best model\n",
    "for i, (t, bp, mp) in enumerate(zip(true_labels, baseline_preds, best_preds)):\n",
    "    if t == 1 and bp == 0 and mp == 1:\n",
    "        fixed_fn.append(i)\n",
    "    if t == 0 and bp == 1 and mp == 0:\n",
    "        fixed_fp.append(i)\n",
    "\n",
    "print(f'\\nBest model fixes {len(fixed_fn)} FN and {len(fixed_fp)} FP from baseline')\n",
    "\n",
    "# Show example false negatives that the best model catches\n",
    "print(f'\\n--- Examples: PCL missed by baseline but caught by best model ({min(5, len(fixed_fn))} shown) ---')\n",
    "for idx in fixed_fn[:5]:\n",
    "    print(f'  [{idx}] {dev_texts[idx][:150]}...')\n",
    "\n",
    "# Show remaining false negatives of best model\n",
    "remaining_fn = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 1 and p == 0]\n",
    "print(f'\\n--- Remaining false negatives of best model ({min(5, len(remaining_fn))}/{len(remaining_fn)} shown) ---')\n",
    "for idx in remaining_fn[:5]:\n",
    "    print(f'  [{idx}] {dev_texts[idx][:150]}...')\n",
    "\n",
    "# Text length analysis of errors\n",
    "fn_lengths = [len(dev_texts[i].split()) for i in remaining_fn]\n",
    "fp_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 0 and p == 1]\n",
    "fp_lengths = [len(dev_texts[i].split()) for i in fp_indices]\n",
    "all_lengths = [len(t.split()) for t in dev_texts]\n",
    "\n",
    "print(f'\\n--- Text length analysis ---')\n",
    "print(f'  Overall mean length:     {np.mean(all_lengths):.1f} words')\n",
    "print(f'  False negative mean:     {np.mean(fn_lengths):.1f} words' if fn_lengths else '  No false negatives')\n",
    "print(f'  False positive mean:     {np.mean(fp_lengths):.1f} words' if fp_lengths else '  No false positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Custom Metrics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ---- Precision-Recall Curve: Best Model vs Baseline ----\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# --- Left: Precision-Recall Curves ---\nax = axes[0]\n\n# Best model PR curve\nprec_best, rec_best, thresholds_best = precision_recall_curve(\n    best_metrics['labels'], best_metrics['probs'], pos_label=1)\nax.plot(rec_best, prec_best, label=f'Best model ({best_key})', color='tab:blue', linewidth=2)\n\n# Baseline PR curve\nbaseline_dev_probs = baseline_dev_metrics['probs']\nprec_base, rec_base, thresholds_base = precision_recall_curve(\n    baseline_dev_metrics['labels'], baseline_dev_probs, pos_label=1)\nax.plot(rec_base, prec_base, label='RoBERTa-base baseline', color='tab:orange', linewidth=2, linestyle='--')\n\n# Mark the operating points (optimised thresholds)\nax.scatter([best_metrics['recall']], [best_metrics['precision']],\n           marker='*', s=200, color='tab:blue', zorder=5, label=f'Best @ t={best_threshold:.2f}')\nax.scatter([thresh_metrics_baseline['recall']], [thresh_metrics_baseline['precision']],\n           marker='*', s=200, color='tab:orange', zorder=5, label=f'Baseline @ t={thresh_baseline:.2f}')\n\nax.set_xlabel('Recall', fontsize=12)\nax.set_ylabel('Precision', fontsize=12)\nax.set_title('Precision-Recall Curve', fontsize=13)\nax.legend(fontsize=9)\nax.set_xlim([0, 1.02])\nax.set_ylim([0, 1.02])\nax.grid(True, alpha=0.3)\n\n# --- Right: Confusion Matrix Heatmap (Best Model) ---\nax = axes[1]\n\ncm = confusion_matrix(best_metrics['labels'], best_metrics['preds'], labels=[0, 1])\nim = ax.imshow(cm, interpolation='nearest', cmap='Blues')\nfig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\nclasses = ['No PCL (0)', 'PCL (1)']\ntick_marks = [0, 1]\nax.set_xticks(tick_marks)\nax.set_xticklabels(classes, fontsize=11)\nax.set_yticks(tick_marks)\nax.set_yticklabels(classes, fontsize=11)\n\n# Annotate each cell with count\nfor i in range(2):\n    for j in range(2):\n        color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n        ax.text(j, i, f'{cm[i, j]}', ha='center', va='center', fontsize=16, fontweight='bold', color=color)\n\nax.set_xlabel('Predicted', fontsize=12)\nax.set_ylabel('Actual', fontsize=12)\nax.set_title(f'Confusion Matrix — Best Model ({best_key}, t={best_threshold:.2f})', fontsize=13)\n\nplt.tight_layout()\nplt.savefig(f'{BASE_DIR}/custom_metrics.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(f'Figure saved to {BASE_DIR}/custom_metrics.png')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. Ablation Study Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Ablation Study: contribution of each component ----\n",
    "ablation_table = pd.DataFrame([\n",
    "    {\n",
    "        'Component': 'RoBERTa-base (baseline)',\n",
    "        'Model': 'roberta-base',\n",
    "        'Focal Loss': '-',\n",
    "        'Oversampling': '-',\n",
    "        'Multi-task': '-',\n",
    "        'Threshold Opt': '-',\n",
    "        'Dev F1': f'{metrics_baseline[\"f1\"]:.4f}',\n",
    "    },\n",
    "    {\n",
    "        'Component': '+ DeBERTa-v3-large',\n",
    "        'Model': 'deberta-v3-large',\n",
    "        'Focal Loss': 'No',\n",
    "        'Oversampling': 'No',\n",
    "        'Multi-task': 'No',\n",
    "        'Threshold Opt': 'No',\n",
    "        'Dev F1': f'{metrics_abl[\"f1\"]:.4f}',\n",
    "    },\n",
    "    {\n",
    "        'Component': '+ Multi-task learning',\n",
    "        'Model': 'deberta-v3-large',\n",
    "        'Focal Loss': 'No' if not ablation_params[best_deberta_config].get('use_focal_loss', False) else 'Yes',\n",
    "        'Oversampling': 'No' if not ablation_params[best_deberta_config].get('use_oversampling', False) else 'Yes',\n",
    "        'Multi-task': 'Yes',\n",
    "        'Threshold Opt': 'No',\n",
    "        'Dev F1': f'{config_results[best_deberta_config][\"f1\"]:.4f}',\n",
    "    },\n",
    "    {\n",
    "        'Component': '+ Threshold optimisation',\n",
    "        'Model': 'deberta-v3-large',\n",
    "        'Focal Loss': 'Yes' if ablation_params[best_deberta_config].get('use_focal_loss', False) else 'No',\n",
    "        'Oversampling': 'Yes' if ablation_params[best_deberta_config].get('use_oversampling', False) else 'No',\n",
    "        'Multi-task': 'Yes',\n",
    "        'Threshold Opt': 'Yes',\n",
    "        'Dev F1': f'{thresh_metrics_a[\"f1\"]:.4f}' if best_deberta_config == 'A' else (f'{thresh_metrics_b[\"f1\"]:.4f}' if best_deberta_config == 'B' else f'{thresh_metrics_c[\"f1\"]:.4f}'),\n",
    "    },\n",
    "])\n",
    "\n",
    "print('='*80)\n",
    "print('ABLATION STUDY')\n",
    "print('='*80)\n",
    "print(ablation_table[['Component', 'Multi-task', 'Threshold Opt', 'Dev F1']].to_string(index=False))\n",
    "\n",
    "# Multi-task contribution\n",
    "mt_gain = config_results[best_deberta_config]['f1'] - metrics_abl['f1']\n",
    "print(f'\\nMulti-task learning contribution: {\"+\" if mt_gain >= 0 else \"\"}{mt_gain:.4f} F1')\n",
    "\n",
    "# Threshold opt contribution\n",
    "best_thresh_f1 = thresh_metrics_a['f1'] if best_deberta_config == 'A' else (thresh_metrics_b['f1'] if best_deberta_config == 'B' else thresh_metrics_c['f1'])\n",
    "thresh_gain = best_thresh_f1 - config_results[best_deberta_config]['f1']\n",
    "print(f'Threshold optimisation contribution: {\"+\" if thresh_gain >= 0 else \"\"}{thresh_gain:.4f} F1')\n",
    "\n",
    "total_gain = best_thresh_f1 - metrics_baseline['f1']\n",
    "print(f'Total improvement over baseline: {\"+\" if total_gain >= 0 else \"\"}{total_gain:.4f} F1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}