{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCL Detection — DeBERTa-v3-large with Multi-Task Learning\n",
    "\n",
    "Binary PCL classifier using DeBERTa-v3-large with:\n",
    "- Multi-task learning (PCL categories as auxiliary task)\n",
    "- Three training configurations: Focal Loss, Oversampling, Both\n",
    "- Early stopping on dev F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/PCL_Detection/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Running on GPUDOJO (CUDA) — batch_size=2, grad_accum=16\n",
      "Effective batch size: 32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Auto-detect environment and set batch sizes accordingly\n",
    "ON_GPUDOJO = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ or DEVICE.type == 'cuda'\n",
    "\n",
    "if ON_GPUDOJO:\n",
    "    BASE_DIR = '/home/azureuser/PCL_Detection'\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM = 16\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    print('Running on GPUDOJO (CUDA) — batch_size=2, grad_accum=16')\n",
    "else:\n",
    "    BASE_DIR = '/Users/alexanderchow/Documents/Y3/60035_NLP/PCL_Detection'\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM = 16\n",
    "    EVAL_BATCH_SIZE = 4\n",
    "    print('Running locally (MPS/CPU) — batch_size=2, grad_accum=16')\n",
    "\n",
    "print(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM}')\n",
    "\n",
    "DATA_DIR = f'{BASE_DIR}/data'\n",
    "SPLITS_DIR = f'{BASE_DIR}/practice splits'\n",
    "CHECKPOINT_DIR = f'{BASE_DIR}/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8375 samples (794 PCL)\n",
      "Dev:   2094 samples (199 PCL)\n",
      "\n",
      "Train class distribution:\n",
      "binary_label\n",
      "0    7581\n",
      "1     794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load main PCL dataset (skip 4 header lines)\n",
    "pcl_df = pd.read_csv(\n",
    "    f'{DATA_DIR}/dontpatronizeme_pcl.tsv',\n",
    "    sep='\\t', skiprows=4, header=None,\n",
    "    names=['par_id', 'art_id', 'keyword', 'country_code', 'text', 'label'],\n",
    "    quoting=3\n",
    ")\n",
    "pcl_df['par_id'] = pcl_df['par_id'].astype(int)\n",
    "pcl_df['label'] = pcl_df['label'].astype(int)\n",
    "\n",
    "# Binary label: {0,1}->0, {2,3,4}->1\n",
    "pcl_df['binary_label'] = (pcl_df['label'] >= 2).astype(int)\n",
    "\n",
    "# Clean text: strip <h> tags and HTML artifacts\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)       # remove HTML tags\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)      # remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # normalise whitespace\n",
    "    return text\n",
    "\n",
    "pcl_df['text'] = pcl_df['text'].apply(clean_text)\n",
    "\n",
    "# Load train/dev splits\n",
    "train_splits = pd.read_csv(f'{SPLITS_DIR}/train_semeval_parids-labels.csv')\n",
    "dev_splits = pd.read_csv(f'{SPLITS_DIR}/dev_semeval_parids-labels.csv')\n",
    "train_splits['par_id'] = train_splits['par_id'].astype(int)\n",
    "dev_splits['par_id'] = dev_splits['par_id'].astype(int)\n",
    "\n",
    "# Parse category labels from split files (7-dim multi-label vectors)\n",
    "def parse_category_label(label_str):\n",
    "    try:\n",
    "        return ast.literal_eval(label_str)\n",
    "    except:\n",
    "        return [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "train_splits['category_labels'] = train_splits['label'].apply(parse_category_label)\n",
    "dev_splits['category_labels'] = dev_splits['label'].apply(parse_category_label)\n",
    "\n",
    "# Merge with main data\n",
    "train_ids = set(train_splits['par_id'].values)\n",
    "dev_ids = set(dev_splits['par_id'].values)\n",
    "\n",
    "train_df = pcl_df[pcl_df['par_id'].isin(train_ids)].copy()\n",
    "dev_df = pcl_df[pcl_df['par_id'].isin(dev_ids)].copy()\n",
    "\n",
    "# Merge category labels\n",
    "cat_train = train_splits[['par_id', 'category_labels']].copy()\n",
    "cat_dev = dev_splits[['par_id', 'category_labels']].copy()\n",
    "\n",
    "train_df = train_df.merge(cat_train, on='par_id', how='left')\n",
    "dev_df = dev_df.merge(cat_dev, on='par_id', how='left')\n",
    "\n",
    "# Fill missing category labels with zeros\n",
    "train_df['category_labels'] = train_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "dev_df['category_labels'] = dev_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_df)} samples ({train_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'Dev:   {len(dev_df)} samples ({dev_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'\\nTrain class distribution:')\n",
    "print(train_df['binary_label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: microsoft/deberta-v3-large\n",
      "Max length: 256\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'microsoft/deberta-v3-large'\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, binary_labels, category_labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.binary_labels = binary_labels\n",
    "        self.category_labels = category_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'binary_label': torch.tensor(self.binary_labels[idx], dtype=torch.long),\n",
    "            'category_labels': torch.tensor(self.category_labels[idx], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def create_datasets(train_df, dev_df, tokenizer, max_length):\n",
    "    train_dataset = PCLDataset(\n",
    "        texts=train_df['text'].tolist(),\n",
    "        binary_labels=train_df['binary_label'].tolist(),\n",
    "        category_labels=train_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    dev_dataset = PCLDataset(\n",
    "        texts=dev_df['text'].tolist(),\n",
    "        binary_labels=dev_df['binary_label'].tolist(),\n",
    "        category_labels=dev_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return train_dataset, dev_dataset\n",
    "\n",
    "def create_oversampled_df(df, oversample_factor=4):\n",
    "    \"\"\"Oversample minority class (PCL=1) by duplicating examples.\"\"\"\n",
    "    minority = df[df['binary_label'] == 1]\n",
    "    majority = df[df['binary_label'] == 0]\n",
    "    minority_oversampled = pd.concat([minority] * oversample_factor, ignore_index=True)\n",
    "    oversampled = pd.concat([majority, minority_oversampled], ignore_index=True)\n",
    "    oversampled = oversampled.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    print(f'  Oversampled: {len(oversampled)} samples ({oversampled[\"binary_label\"].sum()} PCL)')\n",
    "    return oversampled\n",
    "\n",
    "print(f'Tokenizer loaded: {MODEL_NAME}')\n",
    "print(f'Max length: {MAX_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class defined.\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal loss for handling class imbalance.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float()\n",
    "        pt = (probs * targets_one_hot).sum(dim=1)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha.to(logits.device)\n",
    "            alpha_t = alpha[targets]\n",
    "            focal_weight = focal_weight * alpha_t\n",
    "\n",
    "        return (focal_weight * ce_loss).mean()\n",
    "\n",
    "\n",
    "class PCLMultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name, num_categories=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        self.binary_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "        self.category_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_categories)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        binary_logits = self.binary_head(cls_output)\n",
    "        category_logits = self.category_head(cls_output)\n",
    "\n",
    "        return binary_logits, category_logits\n",
    "\n",
    "print('Model class defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined.\n"
     ]
    }
   ],
   "source": [
    "print_every_updates = 20\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on a dataset, return metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['binary_label']\n",
    "\n",
    "            binary_logits, _ = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(binary_logits, dim=1).cpu()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
    "    precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "\n",
    "    return {'f1': f1, 'precision': precision, 'recall': recall, 'preds': all_preds, 'labels': all_labels}\n",
    "\n",
    "\n",
    "def train_model(config_name, train_df, dev_df, tokenizer, use_focal_loss=True,\n",
    "                use_oversampling=False, oversample_factor=4,\n",
    "                num_epochs=10, batch_size=BATCH_SIZE, grad_accum_steps=GRAD_ACCUM,\n",
    "                lr=1e-5, weight_decay=0.01, patience=3, category_weight=0.3):\n",
    "    \"\"\"Train a PCLMultiTaskModel with the given configuration.\"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training Config: {config_name}')\n",
    "    print(f'  Focal Loss: {use_focal_loss} | Oversampling: {use_oversampling}')\n",
    "    print(f'  Epochs: {num_epochs} | Batch: {batch_size} | Grad Accum: {grad_accum_steps}')\n",
    "    print(f'  Effective batch size: {batch_size * grad_accum_steps}')\n",
    "    print(f'  LR: {lr} | Weight Decay: {weight_decay} | Patience: {patience}')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    # Prepare training data\n",
    "    if use_oversampling:\n",
    "        effective_train_df = create_oversampled_df(train_df, oversample_factor)\n",
    "    else:\n",
    "        effective_train_df = train_df.copy()\n",
    "\n",
    "    train_dataset, dev_dataset = create_datasets(effective_train_df, dev_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    model = PCLMultiTaskModel(MODEL_NAME).to(DEVICE).float()\n",
    "\n",
    "    # Loss functions\n",
    "    if use_focal_loss:\n",
    "        n_neg = (effective_train_df['binary_label'] == 0).sum()\n",
    "        n_pos = (effective_train_df['binary_label'] == 1).sum()\n",
    "        alpha_pos = n_neg / (n_neg + n_pos)\n",
    "        alpha_neg = n_pos / (n_neg + n_pos)\n",
    "        binary_criterion = FocalLoss(alpha=[alpha_neg, alpha_pos], gamma=2.0)\n",
    "        print(f'  Focal Loss alpha: [{alpha_neg:.3f}, {alpha_pos:.3f}]')\n",
    "    else:\n",
    "        n_neg = (effective_train_df['binary_label'] == 0).sum()\n",
    "        n_pos = (effective_train_df['binary_label'] == 1).sum()\n",
    "        weight = torch.tensor([1.0, n_neg / n_pos], dtype=torch.float).to(DEVICE)\n",
    "        binary_criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        print(f'  CE class weights: [{weight[0]:.3f}, {weight[1]:.3f}]')\n",
    "\n",
    "    category_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * num_epochs // grad_accum_steps\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            binary_labels = batch['binary_label'].to(DEVICE)\n",
    "            category_labels = batch['category_labels'].to(DEVICE)\n",
    "\n",
    "            binary_logits, category_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_binary = binary_criterion(binary_logits, binary_labels)\n",
    "            loss_category = category_criterion(category_logits, category_labels)\n",
    "            loss = loss_binary + category_weight * loss_category\n",
    "            loss = loss / grad_accum_steps\n",
    "\n",
    "            loss.backward()\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            if (step + 1) % grad_accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                update = (step + 1) // grad_accum_steps\n",
    "                if update % print_every_updates == 0:\n",
    "                    # average loss over the last `print_every_updates` updates (approx)\n",
    "                    avg_recent = total_loss / (step + 1)\n",
    "                    print(f\"    step {step+1}/{len(train_loader)} \"\n",
    "                          f\"(update {update}) | avg loss so far: {avg_recent:.4f}\")\n",
    "                if torch.device.type == \"cuda\":\n",
    "                  mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                  print(f\"    ... | GPU allocated: {mem:.2f} GiB\")\n",
    "\n",
    "        # Handle remaining gradients\n",
    "        if (step + 1) % grad_accum_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on dev\n",
    "        metrics = evaluate(model, dev_loader, DEVICE)\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'f1': metrics['f1'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall']\n",
    "        })\n",
    "\n",
    "        print(f'  Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f} | '\n",
    "              f'F1: {metrics[\"f1\"]:.4f} | P: {metrics[\"precision\"]:.4f} | R: {metrics[\"recall\"]:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            patience_counter = 0\n",
    "            save_path = f'{CHECKPOINT_DIR}/{config_name}_best.pt'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'  -> New best F1! Model saved to {save_path}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'  Early stopping at epoch {epoch+1} (patience={patience})')\n",
    "                break\n",
    "\n",
    "    # Load best model and get final dev metrics\n",
    "    model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{config_name}_best.pt', weights_only=True))\n",
    "    final_metrics = evaluate(model, dev_loader, DEVICE)\n",
    "    print(f'\\n  Final Dev Metrics ({config_name}):')\n",
    "    print(f'    F1: {final_metrics[\"f1\"]:.4f} | P: {final_metrics[\"precision\"]:.4f} | R: {final_metrics[\"recall\"]:.4f}')\n",
    "    print(classification_report(\n",
    "        final_metrics['labels'], final_metrics['preds'],\n",
    "        target_names=['No PCL', 'PCL'], digits=4\n",
    "    ))\n",
    "\n",
    "    return model, final_metrics, history\n",
    "\n",
    "print('Training function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Three Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: config_A_focal\n",
      "  Focal Loss: True | Oversampling: False\n",
      "  Epochs: 10 | Batch: 2 | Grad Accum: 16\n",
      "  Effective batch size: 32\n",
      "  LR: 1e-05 | Weight Decay: 0.01 | Patience: 3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 390/390 [00:00<00:00, 1084.32it/s, Materializing param=encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Focal Loss alpha: [0.095, 0.905]\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.3030\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.2943\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.2820\n",
      "    step 1280/4188 (update 80) | avg loss so far: 0.2632\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.2370\n",
      "    step 1920/4188 (update 120) | avg loss so far: 0.2118\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.1910\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.1779\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.1667\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.1575\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.1507\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.1444\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.1384\n",
      "  Epoch 1/10 — Loss: 0.1379 | F1: 0.2188 | P: 0.1316 | R: 0.6482\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_A_focal_best.pt\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.0752\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0728\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0746\n",
      "    step 1280/4188 (update 80) | avg loss so far: 0.0735\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.0714\n",
      "    step 1920/4188 (update 120) | avg loss so far: 0.0707\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0696\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0697\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.0705\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0696\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.0693\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0689\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0683\n",
      "  Epoch 2/10 — Loss: 0.0680 | F1: 0.3988 | P: 0.2824 | R: 0.6784\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_A_focal_best.pt\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.0559\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0600\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0586\n",
      "    step 1920/4188 (update 120) | avg loss so far: 0.0571\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0562\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0548\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.0545\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0545\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.0545\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0548\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0551\n",
      "  Epoch 3/10 — Loss: 0.0554 | F1: 0.4555 | P: 0.3455 | R: 0.6683\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_A_focal_best.pt\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.0383\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0424\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0480\n",
      "    step 1280/4188 (update 80) | avg loss so far: 0.0476\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.0478\n",
      "    step 1920/4188 (update 120) | avg loss so far: 0.0476\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0473\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0469\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0475\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0469\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0473\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0397\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0398\n",
      "    step 1280/4188 (update 80) | avg loss so far: 0.0389\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.0389\n",
      "    step 1920/4188 (update 120) | avg loss so far: 0.0377\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0369\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0376\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.0371\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0369\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.0375\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0376\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0373\n",
      "  Epoch 5/10 — Loss: 0.0371 | F1: 0.4930 | P: 0.4079 | R: 0.6231\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_A_focal_best.pt\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.0320\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0335\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0332\n",
      "    step 1280/4188 (update 80) | avg loss so far: 0.0319\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.0332\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0319\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0317\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.0317\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0313\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.0315\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0315\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0309\n",
      "  Epoch 6/10 — Loss: 0.0309 | F1: 0.4575 | P: 0.3492 | R: 0.6633\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.0235\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0247\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0251\n",
      "    step 1280/4188 (update 80) | avg loss so far: 0.0264\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.0261\n",
      "    step 1920/4188 (update 120) | avg loss so far: 0.0267\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0262\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0252\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.0251\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0250\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.0248\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0245\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0246\n",
      "  Epoch 7/10 — Loss: 0.0247 | F1: 0.5010 | P: 0.4167 | R: 0.6281\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_A_focal_best.pt\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.0162\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0180\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0187\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.0207\n",
      "    step 1920/4188 (update 120) | avg loss so far: 0.0207\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0211\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0217\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.0216\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0212\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.0213\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0215\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0214\n",
      "  Epoch 8/10 — Loss: 0.0213 | F1: 0.4872 | P: 0.4526 | R: 0.5276\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.0213\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0210\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0228\n",
      "    step 1280/4188 (update 80) | avg loss so far: 0.0219\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.0224\n",
      "    step 1920/4188 (update 120) | avg loss so far: 0.0222\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0218\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0215\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.0212\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0206\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.0204\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0204\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0207\n",
      "  Epoch 9/10 — Loss: 0.0210 | F1: 0.4861 | P: 0.4898 | R: 0.4824\n",
      "    step 320/4188 (update 20) | avg loss so far: 0.0216\n",
      "    step 640/4188 (update 40) | avg loss so far: 0.0181\n",
      "    step 960/4188 (update 60) | avg loss so far: 0.0198\n",
      "    step 1280/4188 (update 80) | avg loss so far: 0.0194\n",
      "    step 1600/4188 (update 100) | avg loss so far: 0.0191\n",
      "    step 2240/4188 (update 140) | avg loss so far: 0.0195\n",
      "    step 2560/4188 (update 160) | avg loss so far: 0.0202\n",
      "    step 2880/4188 (update 180) | avg loss so far: 0.0202\n",
      "    step 3200/4188 (update 200) | avg loss so far: 0.0197\n",
      "    step 3520/4188 (update 220) | avg loss so far: 0.0194\n",
      "    step 3840/4188 (update 240) | avg loss so far: 0.0195\n",
      "    step 4160/4188 (update 260) | avg loss so far: 0.0197\n",
      "  Epoch 10/10 — Loss: 0.0196 | F1: 0.4937 | P: 0.4949 | R: 0.4925\n",
      "  Early stopping at epoch 10 (patience=3)\n",
      "\n",
      "  Final Dev Metrics (config_A_focal):\n",
      "    F1: 0.5010 | P: 0.4167 | R: 0.6281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9588    0.9077    0.9325      1895\n",
      "         PCL     0.4167    0.6281    0.5010       199\n",
      "\n",
      "    accuracy                         0.8811      2094\n",
      "   macro avg     0.6877    0.7679    0.7168      2094\n",
      "weighted avg     0.9072    0.8811    0.8915      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Config A: Focal Loss only (no oversampling)\n",
    "model_a, metrics_a, history_a = train_model(\n",
    "    config_name='config_A_focal',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=True,\n",
    "    use_oversampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: config_B_oversample\n",
      "  Focal Loss: False | Oversampling: True\n",
      "  Epochs: 10 | Batch: 2 | Grad Accum: 16\n",
      "  Effective batch size: 32\n",
      "  LR: 1e-05 | Weight Decay: 0.01 | Patience: 3\n",
      "============================================================\n",
      "  Oversampled: 10757 samples (3176 PCL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 390/390 [00:00<00:00, 1087.63it/s, Materializing param=encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CE class weights: [1.000, 2.387]\n",
      "    step 320/5379 (update 20) | avg loss so far: 1.0693\n",
      "    step 960/5379 (update 60) | avg loss so far: 1.0085\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.9907\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.9765\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.9681\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.9594\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.9501\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.9362\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.9123\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.9006\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.8836\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.8761\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.8686\n",
      "  Epoch 1/10 — Loss: 0.8634 | F1: 0.0000 | P: 0.0000 | R: 0.0000\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.7662\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.7606\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.7592\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.7596\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.7619\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.7603\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.7657\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.7668\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.7684\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.7676\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.7668\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.7659\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.7670\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.7656\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.7639\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.7639\n",
      "  Epoch 2/10 — Loss: 0.7649 | F1: 0.0000 | P: 0.0000 | R: 0.0000\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.7410\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.7534\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.7459\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.7522\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.7518\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.7551\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.7545\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.7523\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.7536\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.7562\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.7560\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.7565\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.7581\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.7574\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.7574\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.7562\n",
      "  Epoch 3/10 — Loss: 0.7577 | F1: 0.0000 | P: 0.0000 | R: 0.0000\n",
      "  Early stopping at epoch 3 (patience=3)\n",
      "\n",
      "  Final Dev Metrics (config_B_oversample):\n",
      "    F1: 0.4971 | P: 0.4071 | R: 0.6382\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9596    0.9024    0.9301      1895\n",
      "         PCL     0.4071    0.6382    0.4971       199\n",
      "\n",
      "    accuracy                         0.8773      2094\n",
      "   macro avg     0.6833    0.7703    0.7136      2094\n",
      "weighted avg     0.9071    0.8773    0.8890      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Config B: Oversampling only (standard weighted CE loss)\n",
    "model_b, metrics_b, history_b = train_model(\n",
    "    config_name='config_B_oversample',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=False,\n",
    "    use_oversampling=True,\n",
    "    oversample_factor=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: config_C_focal_oversample\n",
      "  Focal Loss: True | Oversampling: True\n",
      "  Epochs: 10 | Batch: 2 | Grad Accum: 16\n",
      "  Effective batch size: 32\n",
      "  LR: 1e-05 | Weight Decay: 0.01 | Patience: 3\n",
      "============================================================\n",
      "  Oversampled: 10757 samples (3176 PCL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 390/390 [00:00<00:00, 1107.70it/s, Materializing param=encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Focal Loss alpha: [0.295, 0.705]\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.3589\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.3489\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.3427\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.3359\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.3281\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.3085\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.2896\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.2752\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.2621\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.2522\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.2455\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.2388\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.2341\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.2288\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.2243\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.2201\n",
      "  Epoch 1/10 — Loss: 0.2173 | F1: 0.1736 | P: 0.0951 | R: 1.0000\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_C_focal_oversample_best.pt\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.1610\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.1615\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.1605\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.1617\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.1618\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.1623\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.1623\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.1626\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.1636\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.1630\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.1631\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.1626\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.1622\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.1609\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.1606\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.1601\n",
      "  Epoch 2/10 — Loss: 0.1596 | F1: 0.3206 | P: 0.1948 | R: 0.9045\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_C_focal_oversample_best.pt\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.1383\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.1371\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.1374\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.1391\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.1402\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.1401\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.1395\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.1371\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.1365\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.1356\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.1335\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.1317\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.1309\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.1298\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.1295\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.1283\n",
      "  Epoch 3/10 — Loss: 0.1280 | F1: 0.4463 | P: 0.3325 | R: 0.6784\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_C_focal_oversample_best.pt\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.0943\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.0945\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.0975\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.1014\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.1006\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.1013\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.1025\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.0994\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.0992\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.0986\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.0987\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.0988\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.0981\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.0982\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.0979\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.0975\n",
      "  Epoch 4/10 — Loss: 0.0973 | F1: 0.4444 | P: 0.3207 | R: 0.7236\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.0804\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.0829\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.0808\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.0811\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.0813\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.0821\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.0818\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.0819\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.0800\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.0794\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.0787\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.0782\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.0786\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.0787\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.0786\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.0785\n",
      "  Epoch 5/10 — Loss: 0.0789 | F1: 0.4704 | P: 0.3451 | R: 0.7387\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_C_focal_oversample_best.pt\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.0707\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.0698\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.0744\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.0734\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.0717\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.0729\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.0717\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.0722\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.0718\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.0714\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.0710\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.0710\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.0708\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.0706\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.0705\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.0708\n",
      "  Epoch 6/10 — Loss: 0.0711 | F1: 0.4792 | P: 0.4093 | R: 0.5779\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_C_focal_oversample_best.pt\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.0663\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.0689\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.0680\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.0686\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.0687\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.0695\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.0682\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.0684\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.0689\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.0694\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.0692\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.0686\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.0684\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.0677\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.0674\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.0671\n",
      "  Epoch 7/10 — Loss: 0.0667 | F1: 0.5000 | P: 0.5025 | R: 0.4975\n",
      "  -> New best F1! Model saved to /home/azureuser/PCL_Detection/checkpoints/config_C_focal_oversample_best.pt\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.0599\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.0577\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.0630\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.0631\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.0636\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.0627\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.0624\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.0620\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.0614\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.0611\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.0613\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.0613\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.0608\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.0609\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.0611\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.0619\n",
      "  Epoch 8/10 — Loss: 0.0622 | F1: 0.4767 | P: 0.4663 | R: 0.4874\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.0544\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.0564\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.0591\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.0580\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.0586\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.0574\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.0562\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.0565\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.0560\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.0566\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.0572\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.0566\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.0565\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.0565\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.0565\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.0563\n",
      "  Epoch 9/10 — Loss: 0.0562 | F1: 0.4802 | P: 0.4478 | R: 0.5176\n",
      "    step 320/5379 (update 20) | avg loss so far: 0.0530\n",
      "    step 640/5379 (update 40) | avg loss so far: 0.0536\n",
      "    step 960/5379 (update 60) | avg loss so far: 0.0536\n",
      "    step 1280/5379 (update 80) | avg loss so far: 0.0539\n",
      "    step 1600/5379 (update 100) | avg loss so far: 0.0541\n",
      "    step 1920/5379 (update 120) | avg loss so far: 0.0543\n",
      "    step 2240/5379 (update 140) | avg loss so far: 0.0546\n",
      "    step 2560/5379 (update 160) | avg loss so far: 0.0541\n",
      "    step 2880/5379 (update 180) | avg loss so far: 0.0538\n",
      "    step 3200/5379 (update 200) | avg loss so far: 0.0533\n",
      "    step 3520/5379 (update 220) | avg loss so far: 0.0531\n",
      "    step 3840/5379 (update 240) | avg loss so far: 0.0525\n",
      "    step 4160/5379 (update 260) | avg loss so far: 0.0523\n",
      "    step 4480/5379 (update 280) | avg loss so far: 0.0524\n",
      "    step 4800/5379 (update 300) | avg loss so far: 0.0525\n",
      "    step 5120/5379 (update 320) | avg loss so far: 0.0521\n",
      "  Epoch 10/10 — Loss: 0.0520 | F1: 0.4726 | P: 0.4500 | R: 0.4975\n",
      "  Early stopping at epoch 10 (patience=3)\n",
      "\n",
      "  Final Dev Metrics (config_C_focal_oversample):\n",
      "    F1: 0.5000 | P: 0.5025 | R: 0.4975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9473    0.9483    0.9478      1895\n",
      "         PCL     0.5025    0.4975    0.5000       199\n",
      "\n",
      "    accuracy                         0.9054      2094\n",
      "   macro avg     0.7249    0.7229    0.7239      2094\n",
      "weighted avg     0.9050    0.9054    0.9052      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Config C: Focal Loss + Oversampling\n",
    "model_c, metrics_c, history_c = train_model(\n",
    "    config_name='config_C_focal_oversample',\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    use_focal_loss=True,\n",
    "    use_oversampling=True,\n",
    "    oversample_factor=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m results = pd.DataFrame({\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mConfig\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mA: Focal Loss\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mB: Oversampling\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mC: Focal + Oversample\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBaseline (RoBERTa-base)\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mF1\u001b[39m\u001b[33m'\u001b[39m: [\u001b[43mmetrics_a\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m], metrics_b[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m], metrics_c[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m], \u001b[32m0.48\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPrecision\u001b[39m\u001b[33m'\u001b[39m: [metrics_a[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m], metrics_b[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m], metrics_c[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRecall\u001b[39m\u001b[33m'\u001b[39m: [metrics_a[\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m], metrics_b[\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m], metrics_c[\u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m      6\u001b[39m })\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mRESULTS COMPARISON\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'metrics_a' is not defined"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Config': ['A: Focal Loss', 'B: Oversampling', 'C: Focal + Oversample', 'Baseline (RoBERTa-base)'],\n",
    "    'F1': [metrics_a['f1'], metrics_b['f1'], metrics_c['f1'], 0.48],\n",
    "    'Precision': [metrics_a['precision'], metrics_b['precision'], metrics_c['precision'], None],\n",
    "    'Recall': [metrics_a['recall'], metrics_b['recall'], metrics_c['recall'], None],\n",
    "})\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('RESULTS COMPARISON')\n",
    "print('='*60)\n",
    "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "config_metrics = {'A': metrics_a, 'B': metrics_b, 'C': metrics_c}\n",
    "best_config = max(config_metrics, key=lambda k: config_metrics[k]['f1'])\n",
    "print(f'\\nBest config: {best_config} (F1={config_metrics[best_config][\"f1\"]:.4f})')\n",
    "print(f'Beats baseline: {config_metrics[best_config][\"f1\"] > 0.48}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 3832 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 390/390 [00:00<00:00, 1126.19it/s, Materializing param=encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m config_name_map = {\u001b[33m'\u001b[39m\u001b[33mA\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mconfig_A_focal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mB\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mconfig_B_oversample\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mconfig_C_focal_oversample\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m      9\u001b[39m best_model = PCLMultiTaskModel(MODEL_NAME).to(DEVICE)\n\u001b[32m     10\u001b[39m best_model.load_state_dict(torch.load(\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name_map[\u001b[43mbest_config\u001b[49m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_best.pt\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     13\u001b[39m     map_location=DEVICE\n\u001b[32m     14\u001b[39m ))\n\u001b[32m     15\u001b[39m best_model.eval()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Create test dataset (dummy labels)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'best_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/task4_test.tsv', sep='\\t', header=None,\n",
    "                       names=['par_id', 'art_id', 'keyword', 'country_code', 'text'])\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "print(f'Test set: {len(test_df)} samples')\n",
    "\n",
    "# Load best model\n",
    "config_name_map = {'A': 'config_A_focal', 'B': 'config_B_oversample', 'C': 'config_C_focal_oversample'}\n",
    "best_model = PCLMultiTaskModel(MODEL_NAME).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(\n",
    "    f'{CHECKPOINT_DIR}/{config_name_map[best_config]}_best.pt',\n",
    "    weights_only=True,\n",
    "    map_location=DEVICE\n",
    "))\n",
    "best_model.eval()\n",
    "\n",
    "# Create test dataset (dummy labels)\n",
    "test_dataset = PCLDataset(\n",
    "    texts=test_df['text'].tolist(),\n",
    "    binary_labels=[0] * len(test_df),\n",
    "    category_labels=[[0]*7] * len(test_df),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Generate predictions\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        binary_logits, _ = best_model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(binary_logits, dim=1).cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "test_df['prediction'] = all_preds\n",
    "\n",
    "# Save predictions\n",
    "output_path = f'{BASE_DIR}/test_predictions.tsv'\n",
    "test_df[['par_id', 'prediction']].to_csv(output_path, sep='\\t', index=False, header=False)\n",
    "print(f'\\nPredictions saved to {output_path}')\n",
    "print(f'Prediction distribution: {pd.Series(all_preds).value_counts().sort_index().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
