{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCL Detection — Binary Classification Pipeline\n",
    "\n",
    "Systematic approach to PCL binary classification:\n",
    "1. True baseline (RoBERTa-base, unweighted CE, t=0.5)\n",
    "2. Incremental improvements: weighted CE, threshold optimisation, multi-task learning\n",
    "3. Ablation studies showing contribution of each component\n",
    "4. Error analysis and custom metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/akc123/PCL_Detection/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Running on GPU — batch_size=4, grad_accum=8\n",
      "Effective batch size: 32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Auto-detect environment and set batch sizes accordingly\n",
    "ON_GPUDOJO = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ or DEVICE.type == 'cuda'\n",
    "\n",
    "if ON_GPUDOJO:\n",
    "    BASE_DIR = '/vol/bitbucket/akc123/PCL_Detection'\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUM = 8\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    print('Running on GPU — batch_size=4, grad_accum=8')\n",
    "else:\n",
    "    BASE_DIR = '/Users/alexanderchow/Documents/Y3/60035_NLP/PCL_Detection'\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM = 16\n",
    "    EVAL_BATCH_SIZE = 4\n",
    "    print('Running locally (MPS/CPU) — batch_size=2, grad_accum=16')\n",
    "\n",
    "print(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM}')\n",
    "\n",
    "DATA_DIR = f'{BASE_DIR}/data'\n",
    "SPLITS_DIR = f'{BASE_DIR}/practice splits'\n",
    "CHECKPOINT_DIR = f'{BASE_DIR}/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Upsampled \"women\": 38 -> 38 PCL samples\n",
      "  Upsampled \"immigrant\": 23 -> 69 PCL samples\n",
      "  Upsampled \"migrant\": 31 -> 62 PCL samples\n",
      "  Upsampled \"disabled\": 67 -> 67 PCL samples\n",
      "Train: 8452 samples (871 PCL)\n",
      "Dev:   2094 samples (199 PCL)\n",
      "\n",
      "Train PCL per keyword (after upsampling):\n",
      "keyword\n",
      "women             38\n",
      "vulnerable        60\n",
      "migrant           62\n",
      "disabled          67\n",
      "immigrant         69\n",
      "refugee           73\n",
      "hopeless          98\n",
      "poor-families    112\n",
      "in-need          143\n",
      "homeless         149\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train class distribution:\n",
      "binary_label\n",
      "0    7581\n",
      "1     871\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example text: \"hopeless: We 're living in times of absolute insanity , as I 'm pretty sure most people are aware . ...\"\n"
     ]
    }
   ],
   "source": [
    "# Load main PCL dataset (skip 4 header lines)\n",
    "pcl_df = pd.read_csv(\n",
    "    f'{DATA_DIR}/dontpatronizeme_pcl.tsv',\n",
    "    sep='\\t', skiprows=4, header=None,\n",
    "    names=['par_id', 'art_id', 'keyword', 'country_code', 'text', 'label'],\n",
    "    quoting=3\n",
    ")\n",
    "pcl_df['par_id'] = pcl_df['par_id'].astype(int)\n",
    "pcl_df['label'] = pcl_df['label'].astype(int)\n",
    "\n",
    "# Binary label: {0,1}->0, {2,3,4}->1\n",
    "pcl_df['binary_label'] = (pcl_df['label'] >= 2).astype(int)\n",
    "\n",
    "# Clean text: strip <h> tags and HTML artifacts\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)       # remove HTML tags\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)      # remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # normalise whitespace\n",
    "    return text\n",
    "pcl_df['text'] = pcl_df['text'].apply(clean_text)\n",
    "\n",
    "# Prepend keyword to text as explicit topic context for the model\n",
    "pcl_df['text'] = pcl_df['keyword'] + ': ' + pcl_df['text']\n",
    "\n",
    "# Load train/dev splits\n",
    "train_splits = pd.read_csv(f'{SPLITS_DIR}/train_semeval_parids-labels.csv')\n",
    "dev_splits = pd.read_csv(f'{SPLITS_DIR}/dev_semeval_parids-labels.csv')\n",
    "train_splits['par_id'] = train_splits['par_id'].astype(int)\n",
    "dev_splits['par_id'] = dev_splits['par_id'].astype(int)\n",
    "\n",
    "# Parse category labels from split files (7-dim multi-label vectors)\n",
    "def parse_category_label(label_str):\n",
    "    try:\n",
    "        return ast.literal_eval(label_str)\n",
    "    except:\n",
    "        return [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "train_splits['category_labels'] = train_splits['label'].apply(parse_category_label)\n",
    "dev_splits['category_labels'] = dev_splits['label'].apply(parse_category_label)\n",
    "\n",
    "# Merge with main data\n",
    "train_ids = set(train_splits['par_id'].values)\n",
    "dev_ids = set(dev_splits['par_id'].values)\n",
    "\n",
    "train_df = pcl_df[pcl_df['par_id'].isin(train_ids)].copy()\n",
    "dev_df = pcl_df[pcl_df['par_id'].isin(dev_ids)].copy()\n",
    "\n",
    "# Merge category labels\n",
    "cat_train = train_splits[['par_id', 'category_labels']].copy()\n",
    "cat_dev = dev_splits[['par_id', 'category_labels']].copy()\n",
    "\n",
    "train_df = train_df.merge(cat_train, on='par_id', how='left')\n",
    "dev_df = dev_df.merge(cat_dev, on='par_id', how='left')\n",
    "\n",
    "# Fill missing category labels with zeros\n",
    "train_df['category_labels'] = train_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "dev_df['category_labels'] = dev_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "\n",
    "# Upsample PCL samples for underperforming keywords\n",
    "underperforming_kws = ['women', 'immigrant', 'migrant', 'disabled']\n",
    "pcl_train = train_df[train_df['binary_label'] == 1]\n",
    "median_pcl_count = int(pcl_train['keyword'].value_counts().median())\n",
    "\n",
    "augmented_rows = []\n",
    "for kw in underperforming_kws:\n",
    "    kw_pcl = pcl_train[pcl_train['keyword'] == kw]\n",
    "    current_count = len(kw_pcl)\n",
    "    if current_count < median_pcl_count:\n",
    "        n_copies = (median_pcl_count // current_count) - 1\n",
    "        for _ in range(n_copies):\n",
    "            augmented_rows.append(kw_pcl)\n",
    "        print(f'  Upsampled \"{kw}\": {current_count} -> {current_count * (n_copies + 1)} PCL samples')\n",
    "\n",
    "if augmented_rows:\n",
    "    train_df = pd.concat([train_df] + augmented_rows, ignore_index=True)\n",
    "\n",
    "print(f'Train: {len(train_df)} samples ({train_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'Dev:   {len(dev_df)} samples ({dev_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'\\nTrain PCL per keyword (after upsampling):')\n",
    "print(train_df[train_df['binary_label'] == 1]['keyword'].value_counts().sort_values())\n",
    "print(f'\\nTrain class distribution:')\n",
    "print(train_df['binary_label'].value_counts().sort_index())\n",
    "print(f'\\nExample text: \"{train_df[\"text\"].iloc[0][:100]}...\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: roberta-base\n",
      "Max length: 256\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, binary_labels, category_labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.binary_labels = binary_labels\n",
    "        self.category_labels = category_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'binary_label': torch.tensor(self.binary_labels[idx], dtype=torch.long),\n",
    "            'category_labels': torch.tensor(self.category_labels[idx], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def create_datasets(train_df, dev_df, tokenizer, max_length):\n",
    "    train_dataset = PCLDataset(\n",
    "        texts=train_df['text'].tolist(),\n",
    "        binary_labels=train_df['binary_label'].tolist(),\n",
    "        category_labels=train_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    dev_dataset = PCLDataset(\n",
    "        texts=dev_df['text'].tolist(),\n",
    "        binary_labels=dev_df['binary_label'].tolist(),\n",
    "        category_labels=dev_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return train_dataset, dev_dataset\n",
    "\n",
    "print(f'Tokenizer loaded: {MODEL_NAME}')\n",
    "print(f'Max length: {MAX_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes defined: PCLMultiTaskModel, BaselineModel\n"
     ]
    }
   ],
   "source": [
    "class PCLMultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name, num_categories=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        self.binary_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "        self.category_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_categories)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        binary_logits = self.binary_head(cls_output)\n",
    "        category_logits = self.category_head(cls_output)\n",
    "\n",
    "        return binary_logits, category_logits\n",
    "\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"Simple RoBERTa-base binary classifier (baseline).\"\"\"\n",
    "    def __init__(self, model_name='roberta-base', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits, None  # None for compatibility with evaluate()\n",
    "\n",
    "print('Model classes defined: PCLMultiTaskModel, BaselineModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "print_every_updates = 20\n",
    "\n",
    "def free_gpu():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def evaluate(model, dataloader, device, threshold=0.5):\n",
    "    \"\"\"Evaluate model on a dataset, return metrics and probabilities.\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['binary_label']\n",
    "\n",
    "            binary_logits, _ = model(input_ids, attention_mask)\n",
    "            probs = F.softmax(binary_logits, dim=1)[:, 1].cpu()\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    all_preds = [1 if p >= threshold else 0 for p in all_probs]\n",
    "    f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
    "    precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'f1': f1, 'precision': precision, 'recall': recall,\n",
    "        'preds': all_preds, 'labels': all_labels, 'probs': all_probs,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_threshold(probs, labels):\n",
    "    \"\"\"Sweep thresholds on probability outputs to maximise F1.\"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_threshold = 0.5\n",
    "    for t in np.arange(0.05, 0.95, 0.01):\n",
    "        preds = [1 if p >= t else 0 for p in probs]\n",
    "        f1 = f1_score(labels, preds, pos_label=1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "def train_model(config_name, train_df, dev_df, tokenizer,\n",
    "                use_weighted_ce=True, use_threshold_opt=True, use_multitask=False,\n",
    "                num_epochs=10, batch_size=BATCH_SIZE, grad_accum_steps=GRAD_ACCUM,\n",
    "                lr=2e-5, weight_decay=0.01, patience=3, category_weight=0.3,\n",
    "                model_class=PCLMultiTaskModel, model_name='roberta-base'):\n",
    "    \"\"\"Train a model with the given configuration.\"\"\"\n",
    "    free_gpu()\n",
    "\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training Config: {config_name}')\n",
    "    print(f'  Model: {model_name}')\n",
    "    print(f'  Weighted CE: {use_weighted_ce} | Multi-task: {use_multitask}')\n",
    "    print(f'  Threshold Opt: {use_threshold_opt}')\n",
    "    print(f'  Epochs: {num_epochs} | LR: {lr} | Patience: {patience}')\n",
    "    print(f'  Batch: {batch_size} x {grad_accum_steps} = {batch_size * grad_accum_steps} effective')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    effective_train_df = train_df.copy()\n",
    "\n",
    "    # Create tokenizer for this model\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = PCLDataset(\n",
    "        texts=effective_train_df['text'].tolist(),\n",
    "        binary_labels=effective_train_df['binary_label'].tolist(),\n",
    "        category_labels=effective_train_df['category_labels'].tolist(),\n",
    "        tokenizer=tok, max_length=MAX_LENGTH\n",
    "    )\n",
    "    dev_dataset = PCLDataset(\n",
    "        texts=dev_df['text'].tolist(),\n",
    "        binary_labels=dev_df['binary_label'].tolist(),\n",
    "        category_labels=dev_df['category_labels'].tolist(),\n",
    "        tokenizer=tok, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    if model_class == BaselineModel:\n",
    "        model = BaselineModel(model_name=model_name).to(DEVICE).float()\n",
    "    else:\n",
    "        model = PCLMultiTaskModel(model_name=model_name).to(DEVICE).float()\n",
    "\n",
    "    # Loss function\n",
    "    if use_weighted_ce:\n",
    "        n_neg = (effective_train_df['binary_label'] == 0).sum()\n",
    "        n_pos = (effective_train_df['binary_label'] == 1).sum()\n",
    "        weight = torch.tensor([1.0, n_neg / n_pos], dtype=torch.float).to(DEVICE)\n",
    "        binary_criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        print(f'  CE class weights: [{weight[0]:.3f}, {weight[1]:.3f}]')\n",
    "    else:\n",
    "        binary_criterion = nn.CrossEntropyLoss()\n",
    "        print(f'  Unweighted CE')\n",
    "\n",
    "    category_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * num_epochs // grad_accum_steps\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            binary_labels = batch['binary_label'].to(DEVICE)\n",
    "            category_labels = batch['category_labels'].to(DEVICE)\n",
    "\n",
    "            binary_logits, category_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_binary = binary_criterion(binary_logits, binary_labels)\n",
    "            if use_multitask and category_logits is not None:\n",
    "                loss_category = category_criterion(category_logits, category_labels)\n",
    "                loss = loss_binary + category_weight * loss_category\n",
    "            else:\n",
    "                loss = loss_binary\n",
    "            loss = loss / grad_accum_steps\n",
    "\n",
    "            loss.backward()\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            if (step + 1) % grad_accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                update = (step + 1) // grad_accum_steps\n",
    "                if update % print_every_updates == 0:\n",
    "                    avg_recent = total_loss / (step + 1)\n",
    "                    print(f\"    step {step+1}/{len(train_loader)} \"\n",
    "                          f\"(update {update}) | avg loss so far: {avg_recent:.4f}\")\n",
    "\n",
    "        # Handle remaining gradients\n",
    "        if (step + 1) % grad_accum_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on dev at t=0.5\n",
    "        metrics = evaluate(model, dev_loader, DEVICE, threshold=0.5)\n",
    "        history.append({\n",
    "            'epoch': epoch + 1, 'loss': avg_loss,\n",
    "            'f1': metrics['f1'], 'precision': metrics['precision'], 'recall': metrics['recall']\n",
    "        })\n",
    "\n",
    "        print(f'  Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f} | '\n",
    "              f'F1: {metrics[\"f1\"]:.4f} | P: {metrics[\"precision\"]:.4f} | R: {metrics[\"recall\"]:.4f}')\n",
    "\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/{config_name}_best.pt')\n",
    "            print(f'  -> New best F1! Saved.')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'  Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{config_name}_best.pt', weights_only=True))\n",
    "\n",
    "    # Evaluate at t=0.5\n",
    "    final_metrics = evaluate(model, dev_loader, DEVICE, threshold=0.5)\n",
    "\n",
    "    # Threshold optimisation (if enabled)\n",
    "    if use_threshold_opt:\n",
    "        best_thresh, _ = find_best_threshold(final_metrics['probs'], final_metrics['labels'])\n",
    "        thresh_metrics = evaluate(model, dev_loader, DEVICE, threshold=best_thresh)\n",
    "    else:\n",
    "        best_thresh = 0.5\n",
    "        thresh_metrics = final_metrics\n",
    "\n",
    "    print(f'\\n  Dev F1 @ t=0.50: {final_metrics[\"f1\"]:.4f}')\n",
    "    if use_threshold_opt:\n",
    "        print(f'  Dev F1 @ t={best_thresh:.2f} (optimised): {thresh_metrics[\"f1\"]:.4f}')\n",
    "    print(classification_report(\n",
    "        thresh_metrics['labels'], thresh_metrics['preds'],\n",
    "        target_names=['No PCL', 'PCL'], digits=4\n",
    "    ))\n",
    "\n",
    "    # Move to CPU and free GPU\n",
    "    model = model.cpu()\n",
    "    del model; free_gpu()\n",
    "\n",
    "    return final_metrics, thresh_metrics, history, best_thresh, tok\n",
    "\n",
    "print('Training function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. True Baseline: RoBERTa-base + Unweighted CE + t=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: baseline\n",
      "  Model: roberta-base\n",
      "  Weighted CE: False | Multi-task: False\n",
      "  Threshold Opt: False\n",
      "  Epochs: 30 | LR: 1e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1553.12it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unweighted CE\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.5745\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.5671\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.5550\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.5389\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.5180\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.4920\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.4714\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.4593\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.4410\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.4290\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.4175\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.4130\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.4040\n",
      "  Epoch 1/30 — Loss: 0.4024 | F1: 0.0000 | P: 0.0000 | R: 0.0000\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.3337\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.3185\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.2990\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.2882\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.2779\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.2686\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.2606\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.2563\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.2578\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2539\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2513\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2506\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2503\n",
      "  Epoch 2/30 — Loss: 0.2505 | F1: 0.5042 | P: 0.5779 | R: 0.4472\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1762\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1882\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.2164\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.2137\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.2137\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.2135\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.2088\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.2089\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.2058\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2046\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2017\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2020\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2020\n",
      "  Epoch 3/30 — Loss: 0.2030 | F1: 0.5056 | P: 0.5732 | R: 0.4523\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1694\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1649\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1597\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1610\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1665\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1654\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1638\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1586\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1554\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1567\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1518\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1535\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.1554\n",
      "  Epoch 4/30 — Loss: 0.1555 | F1: 0.5348 | P: 0.6000 | R: 0.4824\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0785\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1012\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1093\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1124\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1196\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1166\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1124\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1101\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1055\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1028\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1022\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1001\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0988\n",
      "  Epoch 5/30 — Loss: 0.0992 | F1: 0.5139 | P: 0.6694 | R: 0.4171\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0591\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0514\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0434\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0449\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0537\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0586\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0574\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0571\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0605\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0600\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0639\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0655\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0683\n",
      "  Epoch 6/30 — Loss: 0.0676 | F1: 0.5244 | P: 0.6667 | R: 0.4322\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0598\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0408\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0392\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0342\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0404\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0413\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0435\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0487\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0501\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0517\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0491\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0471\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0459\n",
      "  Epoch 7/30 — Loss: 0.0458 | F1: 0.5357 | P: 0.6569 | R: 0.4523\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0036\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0089\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0098\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0113\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0150\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0162\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0169\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0170\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0198\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0234\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0222\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0214\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0217\n",
      "  Epoch 8/30 — Loss: 0.0222 | F1: 0.5859 | P: 0.5216 | R: 0.6683\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0084\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0143\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0157\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0181\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0195\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0214\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0191\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0167\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0168\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0163\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0154\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0161\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0169\n",
      "  Epoch 9/30 — Loss: 0.0169 | F1: 0.5694 | P: 0.5434 | R: 0.5980\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0223\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0246\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0177\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0165\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0192\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0170\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0173\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0155\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0198\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0203\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0214\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0207\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0209\n",
      "  Epoch 10/30 — Loss: 0.0211 | F1: 0.6120 | P: 0.6707 | R: 0.5628\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0010\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0041\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0062\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0050\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0040\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0034\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0043\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0069\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0072\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0076\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0070\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0073\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0071\n",
      "  Epoch 11/30 — Loss: 0.0070 | F1: 0.5586 | P: 0.6940 | R: 0.4673\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0001\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0001\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0040\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0106\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0094\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0078\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0067\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0059\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0065\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0058\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0054\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0055\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0053\n",
      "  Epoch 12/30 — Loss: 0.0052 | F1: 0.6026 | P: 0.6237 | R: 0.5829\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0008\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0004\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0024\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0022\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0019\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0039\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0034\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0035\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0031\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0032\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0048\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0044\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0051\n",
      "  Epoch 13/30 — Loss: 0.0051 | F1: 0.6000 | P: 0.6126 | R: 0.5879\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0014\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0008\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0005\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0004\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0005\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0004\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0014\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0053\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0047\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0042\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0039\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0047\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0072\n",
      "  Epoch 14/30 — Loss: 0.0071 | F1: 0.5666 | P: 0.6494 | R: 0.5025\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0020\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0118\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0256\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0192\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0167\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0158\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0148\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0159\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0142\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0130\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0136\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0142\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0132\n",
      "  Epoch 15/30 — Loss: 0.0133 | F1: 0.5902 | P: 0.6467 | R: 0.5427\n",
      "  Early stopping at epoch 15\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6120\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9549    0.9710    0.9628      1895\n",
      "         PCL     0.6707    0.5628    0.6120       199\n",
      "\n",
      "    accuracy                         0.9322      2094\n",
      "   macro avg     0.8128    0.7669    0.7874      2094\n",
      "weighted avg     0.9278    0.9322    0.9295      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# True baseline: RoBERTa-base, unweighted CE, no tricks, fixed threshold=0.5\n",
    "BASELINE_MODEL = 'roberta-base'\n",
    "\n",
    "metrics_bl, thresh_metrics_bl, history_bl, thresh_bl, tok_bl = train_model(\n",
    "    config_name='baseline',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=BaselineModel, model_name=BASELINE_MODEL,\n",
    "    use_weighted_ce=False, use_multitask=False, use_threshold_opt=False,\n",
    "    num_epochs=30, lr=1e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Incremental Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: config_A_weighted_ce_thresh_mt\n",
      "  Model: roberta-base\n",
      "  Weighted CE: True | Multi-task: True\n",
      "  Threshold Opt: True\n",
      "  Epochs: 30 | LR: 1e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1749.37it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CE class weights: [1.000, 8.704]\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.9541\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.9481\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.9360\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.9279\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.9154\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.9024\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.8878\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.8789\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.8728\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.8644\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.8554\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.8454\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.8237\n",
      "  Epoch 1/30 — Loss: 0.8181 | F1: 0.0481 | P: 0.5556 | R: 0.0251\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.5613\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.5163\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.5086\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.5053\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.5015\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.4946\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.4942\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.4982\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.4937\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.4863\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.4774\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.4712\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.4698\n",
      "  Epoch 2/30 — Loss: 0.4724 | F1: 0.4830 | P: 0.6290 | R: 0.3920\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.3314\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.3507\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.3496\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.3711\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.3725\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.3711\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.3693\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.3621\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.3672\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.3710\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.3711\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.3681\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.3708\n",
      "  Epoch 3/30 — Loss: 0.3708 | F1: 0.5485 | P: 0.4362 | R: 0.7387\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.2821\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.3235\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.3142\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.3162\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.3133\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.3124\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.3123\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.2996\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.2981\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2906\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2889\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2873\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2835\n",
      "  Epoch 4/30 — Loss: 0.2878 | F1: 0.6010 | P: 0.6041 | R: 0.5980\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1866\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1680\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1760\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1667\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1824\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1845\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1892\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1957\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1971\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2133\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2125\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2119\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2103\n",
      "  Epoch 5/30 — Loss: 0.2112 | F1: 0.6181 | P: 0.6181 | R: 0.6181\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1642\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1494\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1428\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1283\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1216\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1295\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1452\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1442\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1413\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1352\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1421\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1409\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.1470\n",
      "  Epoch 6/30 — Loss: 0.1460 | F1: 0.5706 | P: 0.6689 | R: 0.4975\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0581\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0590\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0703\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0816\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0764\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0770\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0765\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0968\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0966\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0964\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0970\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0951\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0925\n",
      "  Epoch 7/30 — Loss: 0.0917 | F1: 0.6028 | P: 0.5633 | R: 0.6482\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0521\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0570\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0590\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0604\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0534\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0538\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0581\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0585\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0569\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0560\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0610\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0631\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0660\n",
      "  Epoch 8/30 — Loss: 0.0662 | F1: 0.6253 | P: 0.6744 | R: 0.5829\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0360\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0493\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0432\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0383\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0416\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0442\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0412\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0403\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0396\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0451\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0434\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0426\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0487\n",
      "  Epoch 9/30 — Loss: 0.0484 | F1: 0.6015 | P: 0.6158 | R: 0.5879\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0199\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0291\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0365\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0326\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0288\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0293\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0279\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0286\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0275\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0263\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0267\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0295\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0294\n",
      "  Epoch 10/30 — Loss: 0.0296 | F1: 0.6099 | P: 0.5759 | R: 0.6482\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0106\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0174\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0192\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0187\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0282\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0289\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0321\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0350\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0323\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0303\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0287\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0270\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0283\n",
      "  Epoch 11/30 — Loss: 0.0281 | F1: 0.6393 | P: 0.5858 | R: 0.7035\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0099\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0363\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0283\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0263\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0268\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0240\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0240\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0240\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0225\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0261\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0246\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0231\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0221\n",
      "  Epoch 12/30 — Loss: 0.0229 | F1: 0.5912 | P: 0.6564 | R: 0.5377\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0304\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0285\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0220\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0200\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0181\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0180\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0187\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0187\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0177\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0208\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0200\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0191\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0192\n",
      "  Epoch 13/30 — Loss: 0.0190 | F1: 0.5760 | P: 0.6136 | R: 0.5427\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0115\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0347\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0294\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0238\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0208\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0189\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0193\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0228\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0321\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0322\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0302\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0284\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0268\n",
      "  Epoch 14/30 — Loss: 0.0265 | F1: 0.5882 | P: 0.5990 | R: 0.5779\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0131\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0102\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0098\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0101\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0095\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0088\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0123\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0121\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0115\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0111\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0109\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0144\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0142\n",
      "  Epoch 15/30 — Loss: 0.0142 | F1: 0.5863 | P: 0.6446 | R: 0.5377\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0186\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0136\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0112\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0102\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0102\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0097\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0113\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0104\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0100\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0096\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0094\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0091\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0087\n",
      "  Epoch 16/30 — Loss: 0.0088 | F1: 0.6025 | P: 0.5922 | R: 0.6131\n",
      "  Early stopping at epoch 16\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6393\n",
      "  Dev F1 @ t=0.60 (optimised): 0.6467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9683    0.9504    0.9593      1895\n",
      "         PCL     0.5983    0.7035    0.6467       199\n",
      "\n",
      "    accuracy                         0.9269      2094\n",
      "   macro avg     0.7833    0.8270    0.8030      2094\n",
      "weighted avg     0.9331    0.9269    0.9295      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Config A: Weighted CE + Threshold Optimisation + Multi-task Learning\n",
    "metrics_a, thresh_metrics_a, history_a, thresh_a, tok_a = train_model(\n",
    "    config_name='config_A_weighted_ce_thresh_mt',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=PCLMultiTaskModel, model_name='roberta-base',\n",
    "    use_weighted_ce=True, use_multitask=True, use_threshold_opt=True,\n",
    "    num_epochs=30, lr=1e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: ablation_no_multitask\n",
      "  Model: roberta-base\n",
      "  Weighted CE: True | Multi-task: False\n",
      "  Threshold Opt: True\n",
      "  Epochs: 30 | LR: 1e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1505.22it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CE class weights: [1.000, 8.704]\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.6475\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.6331\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.6326\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.6300\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.6328\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.6367\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.6307\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.6238\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.6199\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.6183\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.6176\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.6157\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.6098\n",
      "  Epoch 1/30 — Loss: 0.6091 | F1: 0.0000 | P: 0.0000 | R: 0.0000\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.5140\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.5082\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.4880\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.4619\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.4464\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.4427\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.4316\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.4304\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.4214\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.4214\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.4159\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.4131\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.4115\n",
      "  Epoch 2/30 — Loss: 0.4100 | F1: 0.5597 | P: 0.4451 | R: 0.7538\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.2612\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.3062\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.3252\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.3167\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.3052\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.3005\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.3066\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.3128\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.3194\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.3145\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.3124\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.3107\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.3073\n",
      "  Epoch 3/30 — Loss: 0.3063 | F1: 0.5407 | P: 0.5659 | R: 0.5176\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.2357\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.2628\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.2558\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.2504\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.2424\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.2417\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.2432\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.2440\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.2471\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2476\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2501\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2482\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2463\n",
      "  Epoch 4/30 — Loss: 0.2438 | F1: 0.5763 | P: 0.4982 | R: 0.6834\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1354\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1350\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1342\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1250\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1379\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1425\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1457\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1459\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1454\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1420\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1601\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1604\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.1615\n",
      "  Epoch 5/30 — Loss: 0.1624 | F1: 0.6197 | P: 0.5815 | R: 0.6633\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0864\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1240\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1144\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1097\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1006\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0987\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1018\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0988\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1029\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1047\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1027\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1073\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.1099\n",
      "  Epoch 6/30 — Loss: 0.1085 | F1: 0.5955 | P: 0.5436 | R: 0.6583\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0523\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0454\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0522\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0484\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0644\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0656\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0675\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0735\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0749\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0738\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0725\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0724\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0737\n",
      "  Epoch 7/30 — Loss: 0.0729 | F1: 0.5429 | P: 0.6291 | R: 0.4774\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0649\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0368\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0363\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0355\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0372\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0368\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0425\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0388\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0392\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0388\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0422\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0423\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0404\n",
      "  Epoch 8/30 — Loss: 0.0430 | F1: 0.5309 | P: 0.6880 | R: 0.4322\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0406\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0207\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0308\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0288\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0272\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0250\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0253\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0226\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0214\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0204\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0185\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0211\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0216\n",
      "  Epoch 9/30 — Loss: 0.0226 | F1: 0.5345 | P: 0.6642 | R: 0.4472\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0074\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0103\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0149\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0117\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0142\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0136\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0130\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0160\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0162\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0158\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0159\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0158\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0147\n",
      "  Epoch 10/30 — Loss: 0.0145 | F1: 0.5764 | P: 0.5750 | R: 0.5779\n",
      "  Early stopping at epoch 10\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6197\n",
      "  Dev F1 @ t=0.49 (optimised): 0.6215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9646    0.9493    0.9569      1895\n",
      "         PCL     0.5808    0.6683    0.6215       199\n",
      "\n",
      "    accuracy                         0.9226      2094\n",
      "   macro avg     0.7727    0.8088    0.7892      2094\n",
      "weighted avg     0.9281    0.9226    0.9250      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ablation 1: Config A without multi-task (isolate multi-task contribution)\n",
    "metrics_abl_nomt, thresh_metrics_abl_nomt, history_abl_nomt, thresh_abl_nomt, tok_abl_nomt = train_model(\n",
    "    config_name='ablation_no_multitask',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=BaselineModel, model_name='roberta-base',\n",
    "    use_weighted_ce=True, use_multitask=False, use_threshold_opt=True,\n",
    "    num_epochs=30, lr=1e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: ablation_no_thresh\n",
      "  Model: roberta-base\n",
      "  Weighted CE: True | Multi-task: True\n",
      "  Threshold Opt: False\n",
      "  Epochs: 30 | LR: 1e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1609.30it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CE class weights: [1.000, 8.704]\n",
      "    step 160/2113 (update 20) | avg loss so far: 1.0346\n",
      "    step 320/2113 (update 40) | avg loss so far: 1.0281\n",
      "    step 480/2113 (update 60) | avg loss so far: 1.0093\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.9920\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.9735\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.9553\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.9363\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.9246\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.9086\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.8961\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.8760\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.8529\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.8289\n",
      "  Epoch 1/30 — Loss: 0.8248 | F1: 0.4473 | P: 0.3301 | R: 0.6935\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.4864\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.4938\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.5016\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.4987\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.4949\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.5068\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.5014\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.4934\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.4848\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.4784\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.4812\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.4739\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.4650\n",
      "  Epoch 2/30 — Loss: 0.4645 | F1: 0.4789 | P: 0.3477 | R: 0.7688\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.3905\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.3829\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.3759\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.3710\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.3786\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.3672\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.3642\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.3710\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.3725\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.3732\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.3704\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.3719\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.3726\n",
      "  Epoch 3/30 — Loss: 0.3725 | F1: 0.5591 | P: 0.4887 | R: 0.6533\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.2916\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.3086\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.3159\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.3212\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.3092\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.3031\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.2954\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.2983\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.2868\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2898\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2935\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2901\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2884\n",
      "  Epoch 4/30 — Loss: 0.2866 | F1: 0.5655 | P: 0.4823 | R: 0.6834\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1661\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1694\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1974\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1849\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1965\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1909\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1930\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.2104\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.2076\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2107\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2104\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2122\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2120\n",
      "  Epoch 5/30 — Loss: 0.2127 | F1: 0.5771 | P: 0.4756 | R: 0.7337\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1024\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1202\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1421\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1462\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1538\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1537\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1441\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1424\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1529\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1536\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1496\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1521\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.1492\n",
      "  Epoch 6/30 — Loss: 0.1486 | F1: 0.6259 | P: 0.6095 | R: 0.6432\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0533\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0787\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1026\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1031\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1016\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0930\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1058\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1053\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1059\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1129\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1139\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1120\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.1137\n",
      "  Epoch 7/30 — Loss: 0.1121 | F1: 0.5816 | P: 0.5907 | R: 0.5729\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0207\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0365\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0357\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0373\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0418\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0540\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0525\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0633\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0607\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0627\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0643\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0666\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0643\n",
      "  Epoch 8/30 — Loss: 0.0642 | F1: 0.6131 | P: 0.6131 | R: 0.6131\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0957\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0864\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0754\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0803\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1059\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0903\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0815\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0762\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0695\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0732\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0702\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0664\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0641\n",
      "  Epoch 9/30 — Loss: 0.0639 | F1: 0.5862 | P: 0.6846 | R: 0.5126\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0664\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0503\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0540\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0512\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0488\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0441\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0463\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0476\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0449\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0464\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0468\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0467\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0488\n",
      "  Epoch 10/30 — Loss: 0.0482 | F1: 0.6085 | P: 0.5733 | R: 0.6482\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0490\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0313\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0240\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0204\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0264\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0244\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0228\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0247\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0253\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0281\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0293\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0280\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0297\n",
      "  Epoch 11/30 — Loss: 0.0301 | F1: 0.6284 | P: 0.6238 | R: 0.6332\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0151\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0394\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0325\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0322\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0292\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0283\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0280\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0270\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0262\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0274\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0266\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0261\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0257\n",
      "  Epoch 12/30 — Loss: 0.0254 | F1: 0.6009 | P: 0.5425 | R: 0.6734\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0114\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0115\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0102\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0118\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0109\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0111\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0107\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0114\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0192\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0189\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0188\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0179\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0171\n",
      "  Epoch 13/30 — Loss: 0.0175 | F1: 0.6146 | P: 0.5972 | R: 0.6332\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0146\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0279\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0360\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0316\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0298\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0270\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0266\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0248\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0251\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0233\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0218\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0220\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0215\n",
      "  Epoch 14/30 — Loss: 0.0212 | F1: 0.5945 | P: 0.5960 | R: 0.5930\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0283\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0191\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0156\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0199\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0176\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0178\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0177\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0175\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0168\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0178\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0170\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0185\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0206\n",
      "  Epoch 15/30 — Loss: 0.0204 | F1: 0.5934 | P: 0.6042 | R: 0.5829\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0061\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0058\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0094\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0093\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0086\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0089\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0083\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0097\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0100\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0098\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0100\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0131\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0125\n",
      "  Epoch 16/30 — Loss: 0.0124 | F1: 0.5855 | P: 0.6043 | R: 0.5678\n",
      "  Early stopping at epoch 16\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9614    0.9599    0.9607      1895\n",
      "         PCL     0.6238    0.6332    0.6284       199\n",
      "\n",
      "    accuracy                         0.9288      2094\n",
      "   macro avg     0.7926    0.7965    0.7945      2094\n",
      "weighted avg     0.9293    0.9288    0.9291      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ablation 2: Config A without threshold opt (isolate threshold contribution)\n",
    "metrics_abl_nothresh, thresh_metrics_abl_nothresh, history_abl_nothresh, thresh_abl_nothresh, tok_abl_nothresh = train_model(\n",
    "    config_name='ablation_no_thresh',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=PCLMultiTaskModel, model_name='roberta-base',\n",
    "    use_weighted_ce=True, use_multitask=True, use_threshold_opt=False,\n",
    "    num_epochs=30, lr=1e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: ablation_no_weighted_ce\n",
      "  Model: roberta-base\n",
      "  Weighted CE: False | Multi-task: True\n",
      "  Threshold Opt: True\n",
      "  Epochs: 30 | LR: 1e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1587.47it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unweighted CE\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.7287\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.7294\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.7227\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.7104\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.6973\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.6832\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.6697\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.6490\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.6279\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.6048\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.5847\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.5655\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.5483\n",
      "  Epoch 1/30 — Loss: 0.5459 | F1: 0.1888 | P: 0.6471 | R: 0.1106\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.2954\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.2941\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.2912\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.2964\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.2967\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.2907\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.2919\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.2912\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.2885\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2897\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2880\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2828\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2801\n",
      "  Epoch 2/30 — Loss: 0.2810 | F1: 0.4642 | P: 0.7234 | R: 0.3417\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.2017\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.2064\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.2085\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.2202\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.2243\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.2213\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.2237\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.2202\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.2229\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.2241\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.2237\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.2222\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.2215\n",
      "  Epoch 3/30 — Loss: 0.2207 | F1: 0.5673 | P: 0.6600 | R: 0.4975\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1634\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1636\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1738\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1708\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1749\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1731\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1714\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1735\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1742\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1753\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1739\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1716\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.1704\n",
      "  Epoch 4/30 — Loss: 0.1698 | F1: 0.5000 | P: 0.6903 | R: 0.3920\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1213\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.1225\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.1312\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.1223\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.1177\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.1211\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.1215\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.1213\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.1232\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.1183\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.1227\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.1229\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.1204\n",
      "  Epoch 5/30 — Loss: 0.1199 | F1: 0.6098 | P: 0.5924 | R: 0.6281\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0714\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0740\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0757\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0801\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0818\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0779\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0795\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0814\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0819\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0809\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0787\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0823\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0809\n",
      "  Epoch 6/30 — Loss: 0.0809 | F1: 0.5753 | P: 0.4671 | R: 0.7487\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.1117\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0801\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0692\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0607\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0626\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0605\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0604\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0599\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0596\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0619\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0611\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0587\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0580\n",
      "  Epoch 7/30 — Loss: 0.0598 | F1: 0.5486 | P: 0.6358 | R: 0.4824\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0232\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0280\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0291\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0289\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0278\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0263\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0332\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0321\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0308\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0302\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0300\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0341\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0367\n",
      "  Epoch 8/30 — Loss: 0.0369 | F1: 0.5621 | P: 0.4962 | R: 0.6482\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0371\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0342\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0347\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0336\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0303\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0283\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0310\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0332\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0307\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0332\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0323\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0345\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0339\n",
      "  Epoch 9/30 — Loss: 0.0336 | F1: 0.5602 | P: 0.5847 | R: 0.5377\n",
      "    step 160/2113 (update 20) | avg loss so far: 0.0295\n",
      "    step 320/2113 (update 40) | avg loss so far: 0.0256\n",
      "    step 480/2113 (update 60) | avg loss so far: 0.0232\n",
      "    step 640/2113 (update 80) | avg loss so far: 0.0229\n",
      "    step 800/2113 (update 100) | avg loss so far: 0.0217\n",
      "    step 960/2113 (update 120) | avg loss so far: 0.0201\n",
      "    step 1120/2113 (update 140) | avg loss so far: 0.0208\n",
      "    step 1280/2113 (update 160) | avg loss so far: 0.0208\n",
      "    step 1440/2113 (update 180) | avg loss so far: 0.0201\n",
      "    step 1600/2113 (update 200) | avg loss so far: 0.0201\n",
      "    step 1760/2113 (update 220) | avg loss so far: 0.0204\n",
      "    step 1920/2113 (update 240) | avg loss so far: 0.0203\n",
      "    step 2080/2113 (update 260) | avg loss so far: 0.0208\n",
      "  Epoch 10/30 — Loss: 0.0207 | F1: 0.5562 | P: 0.6306 | R: 0.4975\n",
      "  Early stopping at epoch 10\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6098\n",
      "  Dev F1 @ t=0.75 (optimised): 0.6194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9576    0.9662    0.9619      1895\n",
      "         PCL     0.6484    0.5930    0.6194       199\n",
      "\n",
      "    accuracy                         0.9308      2094\n",
      "   macro avg     0.8030    0.7796    0.7907      2094\n",
      "weighted avg     0.9282    0.9308    0.9294      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ablation 3: Config A without weighted CE (isolate weighted CE contribution)\n",
    "metrics_abl_nowe, thresh_metrics_abl_nowe, history_abl_nowe, thresh_abl_nowe, tok_abl_nowe = train_model(\n",
    "    config_name='ablation_no_weighted_ce',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=PCLMultiTaskModel, model_name='roberta-base',\n",
    "    use_weighted_ce=False, use_multitask=True, use_threshold_opt=True,\n",
    "    num_epochs=30, lr=1e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Comparison & Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS COMPARISON (all models evaluated on dev set)\n",
      "================================================================================\n",
      "                                    Config Threshold     F1  Precision  Recall\n",
      "           Baseline (unweighted CE, t=0.5)      0.50 0.6120     0.6707  0.5628\n",
      "A: + Weighted CE + Thresh Opt + Multi-task      0.60 0.6467     0.5983  0.7035\n",
      "                Ablation: A w/o Multi-task      0.49 0.6215     0.5808  0.6683\n",
      "             Ablation: A w/o Threshold Opt      0.50 0.6284     0.6238  0.6332\n",
      "               Ablation: A w/o Weighted CE      0.75 0.6194     0.6484  0.5930\n",
      "\n",
      "** Best model: Config A (F1=0.6467 @ t=0.60) **\n",
      "   Improvement over baseline: +0.0346 F1\n"
     ]
    }
   ],
   "source": [
    "# ---- Results comparison table ----\n",
    "results = pd.DataFrame([\n",
    "    {\n",
    "        'Config': 'Baseline (unweighted CE, t=0.5)',\n",
    "        'Threshold': '0.50',\n",
    "        'F1': thresh_metrics_bl['f1'],\n",
    "        'Precision': thresh_metrics_bl['precision'],\n",
    "        'Recall': thresh_metrics_bl['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'A: + Weighted CE + Thresh Opt + Multi-task',\n",
    "        'Threshold': f'{thresh_a:.2f}',\n",
    "        'F1': thresh_metrics_a['f1'],\n",
    "        'Precision': thresh_metrics_a['precision'],\n",
    "        'Recall': thresh_metrics_a['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'Ablation: A w/o Multi-task',\n",
    "        'Threshold': f'{thresh_abl_nomt:.2f}',\n",
    "        'F1': thresh_metrics_abl_nomt['f1'],\n",
    "        'Precision': thresh_metrics_abl_nomt['precision'],\n",
    "        'Recall': thresh_metrics_abl_nomt['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'Ablation: A w/o Threshold Opt',\n",
    "        'Threshold': '0.50',\n",
    "        'F1': thresh_metrics_abl_nothresh['f1'],\n",
    "        'Precision': thresh_metrics_abl_nothresh['precision'],\n",
    "        'Recall': thresh_metrics_abl_nothresh['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'Ablation: A w/o Weighted CE',\n",
    "        'Threshold': f'{thresh_abl_nowe:.2f}',\n",
    "        'F1': thresh_metrics_abl_nowe['f1'],\n",
    "        'Precision': thresh_metrics_abl_nowe['precision'],\n",
    "        'Recall': thresh_metrics_abl_nowe['recall'],\n",
    "    },\n",
    "])\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('RESULTS COMPARISON (all models evaluated on dev set)')\n",
    "print('='*80)\n",
    "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "# Best model is Config A\n",
    "best_metrics = thresh_metrics_a\n",
    "best_threshold = thresh_a\n",
    "best_tok = tok_a\n",
    "best_ckpt_name = 'config_A_weighted_ce_thresh_mt'\n",
    "best_model_class = PCLMultiTaskModel\n",
    "best_model_name = 'roberta-base'\n",
    "best_key = 'A'\n",
    "\n",
    "improvement = best_metrics['f1'] - thresh_metrics_bl['f1']\n",
    "print(f'\\n** Best model: Config A (F1={best_metrics[\"f1\"]:.4f} @ t={best_threshold:.2f}) **')\n",
    "print(f'   Improvement over baseline: +{improvement:.4f} F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate dev.txt and test.txt Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev predictions saved to /vol/bitbucket/akc123/PCL_Detection/dev.txt\n",
      "  2094 predictions, 234 predicted PCL\n",
      "\n",
      "Test set: 3832 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1663.47it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to /vol/bitbucket/akc123/PCL_Detection/test.txt\n",
      "  3832 predictions, 373 predicted PCL\n",
      "  Using threshold: 0.60\n"
     ]
    }
   ],
   "source": [
    "# ---- Dev predictions ----\n",
    "dev_preds = best_metrics['preds']\n",
    "dev_pred_path = f'{BASE_DIR}/dev.txt'\n",
    "with open(dev_pred_path, 'w') as f:\n",
    "    for p in dev_preds:\n",
    "        f.write(f'{p}\\n')\n",
    "print(f'Dev predictions saved to {dev_pred_path}')\n",
    "print(f'  {len(dev_preds)} predictions, {sum(dev_preds)} predicted PCL')\n",
    "\n",
    "# ---- Test predictions ----\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/task4_test.tsv', sep='\\t', header=None,\n",
    "                       names=['par_id', 'art_id', 'keyword', 'country_code', 'text'])\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "# Prepend keyword to match training format\n",
    "test_df['text'] = test_df['keyword'] + ': ' + test_df['text']\n",
    "print(f'\\nTest set: {len(test_df)} samples')\n",
    "\n",
    "test_dataset = PCLDataset(\n",
    "    texts=test_df['text'].tolist(),\n",
    "    binary_labels=[0] * len(test_df),\n",
    "    category_labels=[[0]*7] * len(test_df),\n",
    "    tokenizer=best_tok, max_length=MAX_LENGTH\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Reload best model from checkpoint\n",
    "best_model = PCLMultiTaskModel(model_name=best_model_name).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{best_ckpt_name}_best.pt', weights_only=True, map_location=DEVICE))\n",
    "best_model.eval()\n",
    "\n",
    "test_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        binary_logits, _ = best_model(input_ids, attention_mask)\n",
    "        probs = F.softmax(binary_logits, dim=1)[:, 1].cpu().tolist()\n",
    "        test_probs.extend(probs)\n",
    "\n",
    "del best_model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "test_preds = [1 if p >= best_threshold else 0 for p in test_probs]\n",
    "\n",
    "test_pred_path = f'{BASE_DIR}/test.txt'\n",
    "with open(test_pred_path, 'w') as f:\n",
    "    for p in test_preds:\n",
    "        f.write(f'{p}\\n')\n",
    "print(f'Test predictions saved to {test_pred_path}')\n",
    "print(f'  {len(test_preds)} predictions, {sum(test_preds)} predicted PCL')\n",
    "print(f'  Using threshold: {best_threshold:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1581.24it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ERROR ANALYSIS: Baseline vs Best Model on Dev Set\n",
      "======================================================================\n",
      "\n",
      "1. PREDICTION AGREEMENT QUADRANTS (n=2094)\n",
      "   a) Both correct:                     1904 (90.9%)\n",
      "   b) Both wrong:                        105 (5.0%)\n",
      "   c) Baseline correct, best wrong:       48 (2.3%)\n",
      "   d) Baseline wrong, best correct:       37 (1.8%)\n",
      "\n",
      "2. CONFUSION MATRIX COMPARISON\n",
      "   Metric                      Baseline   Best Model\n",
      "   --------------------------------------------------\n",
      "   True Positives                   112          140\n",
      "   True Negatives                  1840         1801\n",
      "   False Positives                   55           94\n",
      "   False Negatives                   87           59\n",
      "\n",
      "======================================================================\n",
      "3. BOTH MODELS CORRECT — \"easy\" examples (1904 total)\n",
      "======================================================================\n",
      "   Both correctly identified as PCL:     111\n",
      "   Both correctly identified as non-PCL: 1793\n",
      "\n",
      "   --- PCL correctly caught by BOTH models (5/111 shown) ---\n",
      "   (These show what \"obvious\" PCL looks like to the models)\n",
      "   [3] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"disabled\"\n",
      "         \"disabled: When some people feel causing problem for some others by breaking into their homes to steal is n't too good , they just result to begging . You now see people without deformities begging , w\"\n",
      "\n",
      "   [4] (bl_prob=0.998, best_prob=0.998)\n",
      "         keyword=\"poor-families\"\n",
      "         \"poor-families: We are alarmed to learn of your recently circulated proposals that would eviscerate the Lifeline program and leave many of the most vulnerable people in the country without access to af\"\n",
      "\n",
      "   [5] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"in-need\"\n",
      "         \"in-need: \"\"\" We share a global responsibility to respond to this crisis . We commend others who have acted in a compassionate and generous way , especially the Government of Bangladesh and host commun\"\n",
      "\n",
      "   [6] (bl_prob=0.997, best_prob=1.000)\n",
      "         keyword=\"in-need\"\n",
      "         \"in-need: The former Chelsea star through his foundation gave out toys , bags and clothes to kids in need of a brighter holiday\"\n",
      "\n",
      "   [7] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"homeless: It can not be right to allow homes to sit empty while many struggle to find somewhere to live , others having to sleep rough on pavements during Christmas , hoping against hope , for some ch\"\n",
      "\n",
      "   Confidence on correctly detected PCL:\n",
      "     Baseline:   mean=0.993  median=1.000\n",
      "     Best model: mean=0.991  median=1.000\n",
      "   Confidence on correctly rejected non-PCL:\n",
      "     Baseline:   mean=0.001  median=0.000\n",
      "     Best model: mean=0.004  median=0.000\n",
      "\n",
      "======================================================================\n",
      "4. BOTH MODELS WRONG — hardest examples (105 total)\n",
      "======================================================================\n",
      "   Shared false negatives (PCL missed by both): 58\n",
      "   Shared false positives (non-PCL flagged by both): 47\n",
      "\n",
      "   --- Shared FN: PCL that NEITHER model detects (8/58 shown) ---\n",
      "   [0] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"homeless: \"His present \"\" chambers \"\" may be quite humble , but Shiyani has the tiny space very neatly organized and clean . Many people pass him by but do not manage to see him , because the space is\"\n",
      "\n",
      "   [1] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"disabled\"\n",
      "         \"disabled: Krueger recently harnessed that creativity to self-publish a book featuring the poems , artwork , photography and short stories of 16 ill or disabled artists from around the world . She hope\"\n",
      "\n",
      "   [9] (bl_prob=0.000, best_prob=0.190)\n",
      "         keyword=\"refugee\"\n",
      "         \"refugee: He depicts demonstrations by refugees at the border post , their catastrophic living conditions and the desperate attempt of several hundred to cross a river a few kilometres from the camp to\"\n",
      "\n",
      "   [12] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"homeless: It 's calculated that over 204,000 days of purpose-built residential accommodation to otherwise potentially homeless elderly men and women have been delivered by this personally driven altru\"\n",
      "\n",
      "   [13] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"disabled\"\n",
      "         \"disabled: \"\"\" I and my daughter Monica are excited about providing a space for disabled people to be able to get together and earn fair prices for their work , \"\" Mr. Rogers said .\"\"\n",
      "\n",
      "   [19] (bl_prob=0.010, best_prob=0.002)\n",
      "         keyword=\"women\"\n",
      "         \"women: \"She continued , \"\" I stepped away from hiding behind a fabricated version of myself . I no longer put actions behind my fears and insecurities . I made a choice to redirect my energy to be a c\"\n",
      "\n",
      "   [20] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"homeless: Bombarded by schizophrenia , addiction and homelessness , you might say that Eoghan O'Driscoll has been to hell and back . But he is finding a new balance through painting . Interview : Mich\"\n",
      "\n",
      "   [22] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"hopeless\"\n",
      "         \"hopeless: \"\"\" So many of us see the state of our home as some kind of reflection of our self-worth ( even though it 's not ! ) , and a mess can so easily make you feel like a failure . And for those w\"\n",
      "\n",
      "   --- Shared FP: non-PCL flagged by BOTH models (5/47 shown) ---\n",
      "   [169] (bl_prob=0.995, best_prob=1.000)\n",
      "         keyword=\"poor-families\"\n",
      "         \"poor-families: Marcos said the government should help poor families that try every possible means to survive . With Joel Zurbano More from this Category :\"\n",
      "\n",
      "   [254] (bl_prob=1.000, best_prob=0.999)\n",
      "         keyword=\"hopeless\"\n",
      "         \"hopeless: \"\"\" So we do need to heal ourselves as an Aboriginal Torres Strait islander community , but also as a nation . \"\" A life of hope , not hopelessness\"\"\n",
      "\n",
      "   [343] (bl_prob=0.993, best_prob=1.000)\n",
      "         keyword=\"refugee\"\n",
      "         \"refugee: \"\"\" The fact is that ruthless smugglers are preying on desperate migrants and refugees with no regard at all for human dignity , \"\" said Michele Sison , the US Deputy Ambassador to the UN .\"\"\n",
      "\n",
      "   [356] (bl_prob=0.998, best_prob=0.998)\n",
      "         keyword=\"homeless\"\n",
      "         \"homeless: His friends at the Chevron want people to know he was n't just a faceless homeless person . He was their friend and their family .\"\n",
      "\n",
      "   [388] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"in-need\"\n",
      "         \"in-need: \"\"\" Your personal leadership has been critical to addressing the plight of the Rohingya who fled to safety in your country . I thank you for all you have done to assist these men , women and \"\n",
      "\n",
      "======================================================================\n",
      "5. REGRESSIONS — baseline correct, best model wrong (48 total)\n",
      "======================================================================\n",
      "   New false negatives (best misses PCL baseline caught): 1\n",
      "   New false positives (best flags non-PCL baseline ignored): 47\n",
      "\n",
      "   --- New FN: PCL missed by best but not baseline (1/1 shown) ---\n",
      "   [80] (bl_prob=0.917, best_prob=0.000)\n",
      "         keyword=\"poor-families\"\n",
      "         \"poor-families: The school authorities have taken the perfect procedure , not to obstruct studies . The 300 odd athletes who engage in sports are given the freedom to practice before and after school h\"\n",
      "\n",
      "\n",
      "   --- New FP: non-PCL flagged by best but not baseline (5/47 shown) ---\n",
      "   [228] (bl_prob=0.052, best_prob=0.974)\n",
      "         keyword=\"in-need\"\n",
      "         \"in-need: \"\"\" This incident will not tear us down but rather strengthen us as an organization . We will continue our mission of helping Veterans in need . It is through your generous donations and the \"\n",
      "\n",
      "   [247] (bl_prob=0.000, best_prob=0.948)\n",
      "         keyword=\"disabled\"\n",
      "         \"disabled: The Jali family in Brown 's Farm was elated as the City of Cape Town donated wheelchairs to their disabled members .\"\n",
      "\n",
      "   [326] (bl_prob=0.001, best_prob=0.998)\n",
      "         keyword=\"homeless\"\n",
      "         \"homeless: As leaders , we will personally support victims but we ask the government to also help . We are going to clear all victims ' hospital bills and we want to ensure that those left homeless rec\"\n",
      "\n",
      "   [379] (bl_prob=0.496, best_prob=0.997)\n",
      "         keyword=\"poor-families\"\n",
      "         \"poor-families: \"\"\" We want all poor families to earn enough and save enough . The rural savings banks were meant to encourage that . \"\"\"\"\n",
      "\n",
      "   [384] (bl_prob=0.058, best_prob=0.985)\n",
      "         keyword=\"women\"\n",
      "         \"women: Khushi said women 's participation in various sectors of the society should be increased and this will help change the attitude of the people towards them .\"\n",
      "\n",
      "======================================================================\n",
      "6. IMPROVEMENTS — best model correct, baseline wrong (37 total)\n",
      "======================================================================\n",
      "   Fixed false negatives (best catches PCL baseline missed): 29\n",
      "   Fixed false positives (best correctly ignores non-PCL): 8\n",
      "\n",
      "   --- Fixed FN: PCL caught by best but not baseline (5/29 shown) ---\n",
      "   [2] (bl_prob=0.000, best_prob=0.996)\n",
      "         keyword=\"poor-families\"\n",
      "         \"poor-families: 10:41am - Parents of children who died must get compensation , free medicine must be provided to poor families across UP : Ram Gopal Yadav\"\n",
      "\n",
      "   [16] (bl_prob=0.000, best_prob=0.687)\n",
      "         keyword=\"immigrant\"\n",
      "         \"immigrant: Sheepherding in America has always been an immigrant 's job , too dirty , too cold and too lonely for anyone with options .\"\n",
      "\n",
      "   [18] (bl_prob=0.003, best_prob=1.000)\n",
      "         keyword=\"in-need\"\n",
      "         \"in-need: \"He said : \"\" We need improved security for civilians and aid workers and access to all those in need , but we must also build a bigger humanitarian muscle to provide for the suffering millio\"\n",
      "\n",
      "   [31] (bl_prob=0.000, best_prob=1.000)\n",
      "         keyword=\"refugee\"\n",
      "         \"refugee: Departing from his prepared remarks , Francis shared his experiences of the day earlier with thousands of people gathered for his blessing . He says among the 300 refugees he greeted Saturday\"\n",
      "\n",
      "   [37] (bl_prob=0.000, best_prob=0.992)\n",
      "         keyword=\"hopeless\"\n",
      "         \"hopeless: \"\"\" They have made them to become hopeless and now , in \"\" going to God \"\" , they have ended up being deceived further and their situation is exploited and the society becomes even worse .\"\"\n",
      "\n",
      "\n",
      "   --- Fixed FP: non-PCL correctly ignored by best but not baseline (5/8 shown) ---\n",
      "   [188] (bl_prob=0.994, best_prob=0.381)\n",
      "         keyword=\"disabled\"\n",
      "         \"disabled: A crew of disabled athletes will be tackling this weekend 's Chattanooga Waterfront Triathlon to show others with disabilities they , too , can participate in a healthy , active lifestyle .\"\n",
      "\n",
      "   [272] (bl_prob=0.995, best_prob=0.001)\n",
      "         keyword=\"women\"\n",
      "         \"women: \"\"\" The government needs to come forward , and it needs to give resources to legal aid , so that those women and those families have their basic human rights and safety met , around representat\"\n",
      "\n",
      "   [287] (bl_prob=0.983, best_prob=0.001)\n",
      "         keyword=\"women\"\n",
      "         \"women: \"She added : \"\" I would also like to carefully point out that the issue was not her religious beliefs , but rather it is about choosing to treat men and women differently by shaking the hands o\"\n",
      "\n",
      "   [562] (bl_prob=0.974, best_prob=0.000)\n",
      "         keyword=\"hopeless\"\n",
      "         \"hopeless: @Good Governance Forum : The worst part is that Govt does not seem serious in tackling the issue . Both provicial and Federal governments have been passing the blame on each other which is i\"\n",
      "\n",
      "   [1420] (bl_prob=0.815, best_prob=0.000)\n",
      "         keyword=\"in-need\"\n",
      "         \"in-need: That one person who always needs help - it 's nice to be there for people and having someone there for you when you need them . However , there are a few people who are constantly in need of \"\n",
      "\n",
      "======================================================================\n",
      "7. KEYWORD / TOPIC ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "   Best model performance by keyword (sorted by PCL count):\n",
      "      Keyword  Total  PCL  TP  FP  FN  Recall  Precision\n",
      "poor-families    190   38  26  19  12    0.68       0.58\n",
      "      in-need    226   33  30  18   3    0.91       0.62\n",
      "     homeless    212   29  22  10   7    0.76       0.69\n",
      "     hopeless    217   26  19  13   7    0.73       0.59\n",
      "   vulnerable    209   20  14  12   6    0.70       0.54\n",
      "     disabled    194   14   7   3   7    0.50       0.70\n",
      "        women    233   14   7   7   7    0.50       0.50\n",
      "      refugee    188   13  10   9   3    0.77       0.53\n",
      "    immigrant    218    7   3   1   4    0.43       0.75\n",
      "      migrant    207    5   2   2   3    0.40       0.50\n",
      "\n",
      "   Keywords with lowest recall (most missed PCL):\n",
      "     \"migrant\": recall=0.40 (2/5 caught, 3 missed)\n",
      "     \"immigrant\": recall=0.43 (3/7 caught, 4 missed)\n",
      "     \"disabled\": recall=0.50 (7/14 caught, 7 missed)\n",
      "     \"women\": recall=0.50 (7/14 caught, 7 missed)\n",
      "     \"poor-families\": recall=0.68 (26/38 caught, 12 missed)\n",
      "\n",
      "   Keywords with most false positives:\n",
      "     \"poor-families\": 19 FP out of 190 samples\n",
      "     \"in-need\": 18 FP out of 226 samples\n",
      "     \"hopeless\": 13 FP out of 217 samples\n",
      "     \"vulnerable\": 12 FP out of 209 samples\n",
      "     \"homeless\": 10 FP out of 212 samples\n",
      "\n",
      "======================================================================\n",
      "8. PCL CATEGORY ANALYSIS (best model errors)\n",
      "======================================================================\n",
      "\n",
      "   PCL samples: 199 total, 140 caught (TP), 59 missed (FN)\n",
      "\n",
      "   Category                         Caught   Missed   Recall\n",
      "   --------------------------------------------------------\n",
      "   Unbalanced power relations          109       33     0.77\n",
      "   Shallow solution                     25       11     0.69\n",
      "   Presupposition                       42       20     0.68\n",
      "   Authority voice                      29        9     0.76\n",
      "   Metaphor                             38       14     0.73\n",
      "   Compassion                           81       25     0.76\n",
      "   The poorer the merrier                7        4     0.64\n",
      "\n",
      "======================================================================\n",
      "9. CONFIDENCE ANALYSIS\n",
      "======================================================================\n",
      "   True Positives       n= 140  mean_prob=0.989  median=1.000  std=0.048\n",
      "   True Negatives       n=1801  mean_prob=0.004  median=0.000  std=0.039\n",
      "   False Positives      n=  94  mean_prob=0.983  median=1.000  std=0.053\n",
      "   False Negatives      n=  59  mean_prob=0.017  median=0.000  std=0.061\n",
      "\n",
      "   False negatives are PCL samples where the model assigned LOW probability.\n",
      "   If FN mean_prob is close to the threshold, the model is uncertain — threshold tuning may help.\n",
      "   If FN mean_prob is very low, the model genuinely cannot recognise these as PCL.\n",
      "\n",
      "======================================================================\n",
      "10. TEXT LENGTH ANALYSIS\n",
      "======================================================================\n",
      "   All samples          n=2094  mean=48.4  median=42.0  max=273\n",
      "   True Positives       n= 140  mean=54.1  median=46.0  max=166\n",
      "   False Positives      n=  94  mean=54.2  median=47.0  max=178\n",
      "   False Negatives      n=  59  mean=57.1  median=53.0  max=176\n",
      "   True Negatives       n=1801  mean=47.3  median=42.0  max=273\n",
      "\n",
      "======================================================================\n",
      "11. LEXICAL PATTERNS: words over-represented in false negatives\n",
      "======================================================================\n",
      "\n",
      "   Words more frequent in false negatives than true positives:\n",
      "   Word                   FN count    FN rate    TP rate    Ratio\n",
      "   ------------------------------------------------------------\n",
      "   women                        15     0.0044     0.0029      1.5x\n",
      "   disabled                     12     0.0036     0.0021      1.7x\n",
      "   see                           7     0.0021     0.0009      2.2x\n",
      "   disabled:                     7     0.0021     0.0009      2.2x\n",
      "   women:                        7     0.0021     0.0009      2.2x\n",
      "   day                           6     0.0018     0.0011      1.7x\n",
      "   school                        6     0.0018     0.0007      2.7x\n",
      "   last                          6     0.0018     0.0000      infx\n",
      "   year                          6     0.0018     0.0009      1.9x\n",
      "   conditions                    5     0.0015     0.0003      5.6x\n",
      "   health                        5     0.0015     0.0005      2.8x\n",
      "   often                         5     0.0015     0.0005      2.8x\n",
      "   focus                         5     0.0015     0.0003      5.6x\n",
      "   immigrants                    5     0.0015     0.0003      5.6x\n",
      "   years                         5     0.0015     0.0009      1.6x\n",
      "\n",
      "   Interpretation: these words appear in PCL text the model fails to detect.\n",
      "   High-ratio words suggest the model may associate them with non-PCL contexts.\n"
     ]
    }
   ],
   "source": [
    "# ---- Error analysis: compare baseline vs best model ----\n",
    "from collections import Counter\n",
    "\n",
    "# Reload baseline from checkpoint\n",
    "model_baseline = BaselineModel(model_name=BASELINE_MODEL).to(DEVICE)\n",
    "model_baseline.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/baseline_best.pt', weights_only=True, map_location=DEVICE))\n",
    "\n",
    "baseline_dev_metrics = evaluate(model_baseline,\n",
    "    DataLoader(PCLDataset(dev_df['text'].tolist(), dev_df['binary_label'].tolist(),\n",
    "                          dev_df['category_labels'].tolist(), tok_bl, MAX_LENGTH),\n",
    "               batch_size=EVAL_BATCH_SIZE, shuffle=False),\n",
    "    DEVICE, threshold=0.5)\n",
    "\n",
    "del model_baseline; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "baseline_preds = baseline_dev_metrics['preds']\n",
    "baseline_probs = baseline_dev_metrics['probs']\n",
    "best_preds = best_metrics['preds']\n",
    "best_probs = best_metrics['probs']\n",
    "true_labels = best_metrics['labels']\n",
    "dev_texts = dev_df['text'].tolist()\n",
    "dev_keywords = dev_df['keyword'].tolist()\n",
    "dev_categories = dev_df['category_labels'].tolist()\n",
    "\n",
    "# ============================================================\n",
    "# 1. Four-quadrant classification: baseline vs best model\n",
    "# ============================================================\n",
    "both_correct = []    # a) both correct\n",
    "both_wrong = []      # b) both wrong\n",
    "base_right_best_wrong = []  # c) baseline correct, best wrong\n",
    "base_wrong_best_right = []  # d) baseline wrong, best correct\n",
    "\n",
    "for i, (true, bp, mp) in enumerate(zip(true_labels, baseline_preds, best_preds)):\n",
    "    b_correct = (bp == true)\n",
    "    m_correct = (mp == true)\n",
    "    if b_correct and m_correct:\n",
    "        both_correct.append(i)\n",
    "    elif not b_correct and not m_correct:\n",
    "        both_wrong.append(i)\n",
    "    elif b_correct and not m_correct:\n",
    "        base_right_best_wrong.append(i)\n",
    "    else:\n",
    "        base_wrong_best_right.append(i)\n",
    "\n",
    "print('=' * 70)\n",
    "print('ERROR ANALYSIS: Baseline vs Best Model on Dev Set')\n",
    "print('=' * 70)\n",
    "\n",
    "n = len(true_labels)\n",
    "print(f'\\n1. PREDICTION AGREEMENT QUADRANTS (n={n})')\n",
    "print(f'   a) Both correct:                    {len(both_correct):>5} ({100*len(both_correct)/n:.1f}%)')\n",
    "print(f'   b) Both wrong:                      {len(both_wrong):>5} ({100*len(both_wrong)/n:.1f}%)')\n",
    "print(f'   c) Baseline correct, best wrong:    {len(base_right_best_wrong):>5} ({100*len(base_right_best_wrong)/n:.1f}%)')\n",
    "print(f'   d) Baseline wrong, best correct:    {len(base_wrong_best_right):>5} ({100*len(base_wrong_best_right)/n:.1f}%)')\n",
    "\n",
    "# ============================================================\n",
    "# 2. Breakdown by error type within each quadrant\n",
    "# ============================================================\n",
    "def error_type_breakdown(indices, labels, bl_preds, best_preds):\n",
    "    \"\"\"Classify errors as FP or FN for each model.\"\"\"\n",
    "    stats = {'bl_fp': 0, 'bl_fn': 0, 'best_fp': 0, 'best_fn': 0}\n",
    "    for i in indices:\n",
    "        if labels[i] == 0 and bl_preds[i] == 1: stats['bl_fp'] += 1\n",
    "        if labels[i] == 1 and bl_preds[i] == 0: stats['bl_fn'] += 1\n",
    "        if labels[i] == 0 and best_preds[i] == 1: stats['best_fp'] += 1\n",
    "        if labels[i] == 1 and best_preds[i] == 0: stats['best_fn'] += 1\n",
    "    return stats\n",
    "\n",
    "print(f'\\n2. CONFUSION MATRIX COMPARISON')\n",
    "print(f'   {\"Metric\":<25} {\"Baseline\":>10} {\"Best Model\":>12}')\n",
    "print(f'   {\"-\"*50}')\n",
    "for label_name, true_val, pred_val in [('True Positives', 1, 1), ('True Negatives', 0, 0),\n",
    "                                         ('False Positives', 0, 1), ('False Negatives', 1, 0)]:\n",
    "    bl_count = sum(1 for t, p in zip(true_labels, baseline_preds) if t == true_val and p == pred_val)\n",
    "    best_count = sum(1 for t, p in zip(true_labels, best_preds) if t == true_val and p == pred_val)\n",
    "    print(f'   {label_name:<25} {bl_count:>10} {best_count:>12}')\n",
    "\n",
    "# ============================================================\n",
    "# 3. Quadrant a) deep dive — both models correct (easy cases)\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'3. BOTH MODELS CORRECT — \"easy\" examples ({len(both_correct)} total)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "both_correct_pcl = [i for i in both_correct if true_labels[i] == 1]\n",
    "both_correct_nopcl = [i for i in both_correct if true_labels[i] == 0]\n",
    "print(f'   Both correctly identified as PCL:     {len(both_correct_pcl)}')\n",
    "print(f'   Both correctly identified as non-PCL: {len(both_correct_nopcl)}')\n",
    "\n",
    "print(f'\\n   --- PCL correctly caught by BOTH models ({min(5, len(both_correct_pcl))}/{len(both_correct_pcl)} shown) ---')\n",
    "print(f'   (These show what \"obvious\" PCL looks like to the models)')\n",
    "for idx in both_correct_pcl[:5]:\n",
    "    print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "    print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "    print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "    print()\n",
    "\n",
    "# Confidence comparison: are both models equally confident on easy cases?\n",
    "if both_correct_pcl:\n",
    "    bl_probs_easy_pcl = [baseline_probs[i] for i in both_correct_pcl]\n",
    "    best_probs_easy_pcl = [best_probs[i] for i in both_correct_pcl]\n",
    "    print(f'   Confidence on correctly detected PCL:')\n",
    "    print(f'     Baseline:   mean={np.mean(bl_probs_easy_pcl):.3f}  median={np.median(bl_probs_easy_pcl):.3f}')\n",
    "    print(f'     Best model: mean={np.mean(best_probs_easy_pcl):.3f}  median={np.median(best_probs_easy_pcl):.3f}')\n",
    "\n",
    "if both_correct_nopcl:\n",
    "    bl_probs_easy_nopcl = [baseline_probs[i] for i in both_correct_nopcl]\n",
    "    best_probs_easy_nopcl = [best_probs[i] for i in both_correct_nopcl]\n",
    "    print(f'   Confidence on correctly rejected non-PCL:')\n",
    "    print(f'     Baseline:   mean={np.mean(bl_probs_easy_nopcl):.3f}  median={np.median(bl_probs_easy_nopcl):.3f}')\n",
    "    print(f'     Best model: mean={np.mean(best_probs_easy_nopcl):.3f}  median={np.median(best_probs_easy_nopcl):.3f}')\n",
    "\n",
    "# ============================================================\n",
    "# 4. Quadrant b) deep dive — both models wrong (hardest cases)\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'4. BOTH MODELS WRONG — hardest examples ({len(both_wrong)} total)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "# Split into shared FN vs shared FP\n",
    "shared_fn = [i for i in both_wrong if true_labels[i] == 1]\n",
    "shared_fp = [i for i in both_wrong if true_labels[i] == 0]\n",
    "print(f'   Shared false negatives (PCL missed by both): {len(shared_fn)}')\n",
    "print(f'   Shared false positives (non-PCL flagged by both): {len(shared_fp)}')\n",
    "\n",
    "print(f'\\n   --- Shared FN: PCL that NEITHER model detects ({min(8, len(shared_fn))}/{len(shared_fn)} shown) ---')\n",
    "for idx in shared_fn[:8]:\n",
    "    print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "    print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "    print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "    print()\n",
    "\n",
    "print(f'   --- Shared FP: non-PCL flagged by BOTH models ({min(5, len(shared_fp))}/{len(shared_fp)} shown) ---')\n",
    "for idx in shared_fp[:5]:\n",
    "    print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "    print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "    print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "    print()\n",
    "\n",
    "# ============================================================\n",
    "# 5. Quadrant c) — regressions (baseline got right, best got wrong)\n",
    "# ============================================================\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'5. REGRESSIONS — baseline correct, best model wrong ({len(base_right_best_wrong)} total)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "regressions_fn = [i for i in base_right_best_wrong if true_labels[i] == 1]\n",
    "regressions_fp = [i for i in base_right_best_wrong if true_labels[i] == 0]\n",
    "print(f'   New false negatives (best misses PCL baseline caught): {len(regressions_fn)}')\n",
    "print(f'   New false positives (best flags non-PCL baseline ignored): {len(regressions_fp)}')\n",
    "\n",
    "for label, indices, tag in [('New FN', regressions_fn, 'PCL missed'), ('New FP', regressions_fp, 'non-PCL flagged')]:\n",
    "    print(f'\\n   --- {label}: {tag} by best but not baseline ({min(5, len(indices))}/{len(indices)} shown) ---')\n",
    "    for idx in indices[:5]:\n",
    "        print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "        print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "        print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "        print()\n",
    "\n",
    "# ============================================================\n",
    "# 6. Quadrant d) — improvements (best got right, baseline got wrong)\n",
    "# ============================================================\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'6. IMPROVEMENTS — best model correct, baseline wrong ({len(base_wrong_best_right)} total)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "improvements_fn_fixed = [i for i in base_wrong_best_right if true_labels[i] == 1]\n",
    "improvements_fp_fixed = [i for i in base_wrong_best_right if true_labels[i] == 0]\n",
    "print(f'   Fixed false negatives (best catches PCL baseline missed): {len(improvements_fn_fixed)}')\n",
    "print(f'   Fixed false positives (best correctly ignores non-PCL): {len(improvements_fp_fixed)}')\n",
    "\n",
    "for label, indices, tag in [('Fixed FN', improvements_fn_fixed, 'PCL caught'),\n",
    "                             ('Fixed FP', improvements_fp_fixed, 'non-PCL correctly ignored')]:\n",
    "    print(f'\\n   --- {label}: {tag} by best but not baseline ({min(5, len(indices))}/{len(indices)} shown) ---')\n",
    "    for idx in indices[:5]:\n",
    "        print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "        print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "        print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "        print()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Keyword analysis — which topics are hardest?\n",
    "# ============================================================\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'7. KEYWORD / TOPIC ANALYSIS')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "# Per-keyword error rates for the best model\n",
    "keyword_stats = {}\n",
    "for i, (true, pred, kw) in enumerate(zip(true_labels, best_preds, dev_keywords)):\n",
    "    kw = str(kw).strip()\n",
    "    if kw not in keyword_stats:\n",
    "        keyword_stats[kw] = {'total': 0, 'pcl': 0, 'tp': 0, 'fp': 0, 'fn': 0}\n",
    "    keyword_stats[kw]['total'] += 1\n",
    "    if true == 1:\n",
    "        keyword_stats[kw]['pcl'] += 1\n",
    "        if pred == 1:\n",
    "            keyword_stats[kw]['tp'] += 1\n",
    "        else:\n",
    "            keyword_stats[kw]['fn'] += 1\n",
    "    elif pred == 1:\n",
    "        keyword_stats[kw]['fp'] += 1\n",
    "\n",
    "kw_df = pd.DataFrame([\n",
    "    {\n",
    "        'Keyword': kw,\n",
    "        'Total': s['total'],\n",
    "        'PCL': s['pcl'],\n",
    "        'TP': s['tp'],\n",
    "        'FP': s['fp'],\n",
    "        'FN': s['fn'],\n",
    "        'Recall': s['tp'] / s['pcl'] if s['pcl'] > 0 else None,\n",
    "        'Precision': s['tp'] / (s['tp'] + s['fp']) if (s['tp'] + s['fp']) > 0 else None,\n",
    "    }\n",
    "    for kw, s in keyword_stats.items()\n",
    "]).sort_values('PCL', ascending=False)\n",
    "\n",
    "print('\\n   Best model performance by keyword (sorted by PCL count):')\n",
    "print(kw_df.to_string(index=False, float_format='{:.2f}'.format))\n",
    "\n",
    "# Keywords most associated with errors\n",
    "print(f'\\n   Keywords with lowest recall (most missed PCL):')\n",
    "low_recall = kw_df[kw_df['PCL'] >= 5].nsmallest(5, 'Recall')\n",
    "for _, row in low_recall.iterrows():\n",
    "    print(f'     \"{row[\"Keyword\"]}\": recall={row[\"Recall\"]:.2f} ({row[\"TP\"]}/{row[\"PCL\"]} caught, {row[\"FN\"]} missed)')\n",
    "\n",
    "print(f'\\n   Keywords with most false positives:')\n",
    "high_fp = kw_df.nlargest(5, 'FP')\n",
    "for _, row in high_fp.iterrows():\n",
    "    print(f'     \"{row[\"Keyword\"]}\": {row[\"FP\"]} FP out of {row[\"Total\"]} samples')\n",
    "\n",
    "# ============================================================\n",
    "# 8. PCL category analysis — which PCL types are hardest?\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'8. PCL CATEGORY ANALYSIS (best model errors)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "CATEGORY_NAMES = [\n",
    "    'Unbalanced power relations', 'Shallow solution', 'Presupposition',\n",
    "    'Authority voice', 'Metaphor', 'Compassion', 'The poorer the merrier'\n",
    "]\n",
    "\n",
    "# For PCL samples only, check which categories are missed vs caught\n",
    "pcl_indices = [i for i, t in enumerate(true_labels) if t == 1]\n",
    "caught = [i for i in pcl_indices if best_preds[i] == 1]\n",
    "missed = [i for i in pcl_indices if best_preds[i] == 0]\n",
    "\n",
    "cat_caught = np.array([dev_categories[i] for i in caught]) if caught else np.zeros((0, 7))\n",
    "cat_missed = np.array([dev_categories[i] for i in missed]) if missed else np.zeros((0, 7))\n",
    "\n",
    "print(f'\\n   PCL samples: {len(pcl_indices)} total, {len(caught)} caught (TP), {len(missed)} missed (FN)')\n",
    "print(f'\\n   {\"Category\":<30} {\"Caught\":>8} {\"Missed\":>8} {\"Recall\":>8}')\n",
    "print(f'   {\"-\"*56}')\n",
    "for j, cat_name in enumerate(CATEGORY_NAMES):\n",
    "    n_caught = int(cat_caught[:, j].sum()) if len(cat_caught) > 0 else 0\n",
    "    n_missed = int(cat_missed[:, j].sum()) if len(cat_missed) > 0 else 0\n",
    "    total = n_caught + n_missed\n",
    "    recall = n_caught / total if total > 0 else 0\n",
    "    print(f'   {cat_name:<30} {n_caught:>8} {n_missed:>8} {recall:>8.2f}')\n",
    "\n",
    "# ============================================================\n",
    "# 9. Confidence analysis — model certainty on errors\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'9. CONFIDENCE ANALYSIS')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "best_fn_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 1 and p == 0]\n",
    "best_fp_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 0 and p == 1]\n",
    "best_tp_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 1 and p == 1]\n",
    "best_tn_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 0 and p == 0]\n",
    "\n",
    "for name, indices in [('True Positives', best_tp_indices), ('True Negatives', best_tn_indices),\n",
    "                       ('False Positives', best_fp_indices), ('False Negatives', best_fn_indices)]:\n",
    "    if indices:\n",
    "        probs_subset = [best_probs[i] for i in indices]\n",
    "        print(f'   {name:<20} n={len(indices):>4}  mean_prob={np.mean(probs_subset):.3f}  '\n",
    "              f'median={np.median(probs_subset):.3f}  std={np.std(probs_subset):.3f}')\n",
    "\n",
    "print(f'\\n   False negatives are PCL samples where the model assigned LOW probability.')\n",
    "print(f'   If FN mean_prob is close to the threshold, the model is uncertain — threshold tuning may help.')\n",
    "print(f'   If FN mean_prob is very low, the model genuinely cannot recognise these as PCL.')\n",
    "\n",
    "# ============================================================\n",
    "# 10. Text length analysis\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'10. TEXT LENGTH ANALYSIS')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "for name, indices in [('All samples', list(range(n))),\n",
    "                       ('True Positives', best_tp_indices), ('False Positives', best_fp_indices),\n",
    "                       ('False Negatives', best_fn_indices), ('True Negatives', best_tn_indices)]:\n",
    "    if indices:\n",
    "        lengths = [len(dev_texts[i].split()) for i in indices]\n",
    "        print(f'   {name:<20} n={len(indices):>4}  mean={np.mean(lengths):.1f}  '\n",
    "              f'median={np.median(lengths):.1f}  max={max(lengths)}')\n",
    "\n",
    "# ============================================================\n",
    "# 11. Lexical patterns in errors — frequent words in FN vs TP\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'11. LEXICAL PATTERNS: words over-represented in false negatives')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "def get_word_freq(indices, texts):\n",
    "    words = []\n",
    "    for i in indices:\n",
    "        words.extend(texts[i].lower().split())\n",
    "    return Counter(words)\n",
    "\n",
    "stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "             'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "             'should', 'may', 'might', 'shall', 'can', 'to', 'of', 'in', 'for',\n",
    "             'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "             'and', 'but', 'or', 'nor', 'not', 'so', 'yet', 'both', 'either',\n",
    "             'neither', 'each', 'every', 'all', 'any', 'few', 'more', 'most',\n",
    "             'other', 'some', 'such', 'no', 'than', 'too', 'very', 'just',\n",
    "             'that', 'this', 'these', 'those', 'it', 'its', 'they', 'them',\n",
    "             'their', 'we', 'our', 'he', 'she', 'his', 'her', 'who', 'which',\n",
    "             'what', 'when', 'where', 'how', 'if', 'then', 'about', 'up', 'out',\n",
    "             'also', 'only', 'over', 'after', 'before', 'between', 'under', 'again',\n",
    "             'there', 'here', 'because', 'while', 'i', 'you', 'my', 'your', 'me',\n",
    "             'him', 'us', 'said', 'one', 'two', 'even', 'get', 'make', 'like',\n",
    "             'much', 'many', 'well', 'back', 'made', 'still', 'going', 'way'}\n",
    "\n",
    "fn_freq = get_word_freq(best_fn_indices, dev_texts)\n",
    "tp_freq = get_word_freq(best_tp_indices, dev_texts)\n",
    "\n",
    "# Normalise to rates and find words distinctive to FN\n",
    "fn_total = sum(fn_freq.values()) or 1\n",
    "tp_total = sum(tp_freq.values()) or 1\n",
    "\n",
    "distinctive_fn = []\n",
    "for word, count in fn_freq.most_common(500):\n",
    "    if word in stopwords or len(word) <= 2:\n",
    "        continue\n",
    "    fn_rate = count / fn_total\n",
    "    tp_rate = tp_freq.get(word, 0) / tp_total\n",
    "    if fn_rate > tp_rate * 1.5 and count >= 3:\n",
    "        distinctive_fn.append((word, count, fn_rate, tp_rate))\n",
    "\n",
    "print(f'\\n   Words more frequent in false negatives than true positives:')\n",
    "print(f'   {\"Word\":<20} {\"FN count\":>10} {\"FN rate\":>10} {\"TP rate\":>10} {\"Ratio\":>8}')\n",
    "print(f'   {\"-\"*60}')\n",
    "for word, count, fn_rate, tp_rate in sorted(distinctive_fn, key=lambda x: -x[2])[:15]:\n",
    "    ratio = fn_rate / tp_rate if tp_rate > 0 else float('inf')\n",
    "    print(f'   {word:<20} {count:>10} {fn_rate:>10.4f} {tp_rate:>10.4f} {ratio:>8.1f}x')\n",
    "\n",
    "print(f'\\n   Interpretation: these words appear in PCL text the model fails to detect.')\n",
    "print(f'   High-ratio words suggest the model may associate them with non-PCL contexts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved to /vol/bitbucket/akc123/PCL_Detection/custom_metrics.png\n"
     ]
    }
   ],
   "source": [
    "# ---- Precision-Recall Curve: Best Model vs Baseline ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Left: Precision-Recall Curves ---\n",
    "ax = axes[0]\n",
    "\n",
    "# Best model PR curve\n",
    "prec_best, rec_best, thresholds_best = precision_recall_curve(\n",
    "    best_metrics['labels'], best_metrics['probs'], pos_label=1)\n",
    "ax.plot(rec_best, prec_best, label=f'Best model ({best_key})', color='tab:blue', linewidth=2)\n",
    "\n",
    "# Baseline PR curve\n",
    "prec_base, rec_base, thresholds_base = precision_recall_curve(\n",
    "    baseline_dev_metrics['labels'], baseline_dev_metrics['probs'], pos_label=1)\n",
    "ax.plot(rec_base, prec_base, label='RoBERTa-base baseline', color='tab:orange', linewidth=2, linestyle='--')\n",
    "\n",
    "# Mark the operating points\n",
    "ax.scatter([best_metrics['recall']], [best_metrics['precision']],\n",
    "           marker='*', s=200, color='tab:blue', zorder=5, label=f'Best @ t={best_threshold:.2f}')\n",
    "ax.scatter([baseline_dev_metrics['recall']], [baseline_dev_metrics['precision']],\n",
    "           marker='*', s=200, color='tab:orange', zorder=5, label=f'Baseline @ t={thresh_bl:.2f}')\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curve', fontsize=13)\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_xlim([0, 1.02])\n",
    "ax.set_ylim([0, 1.02])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Right: Confusion Matrix Heatmap (Best Model) ---\n",
    "ax = axes[1]\n",
    "\n",
    "cm = confusion_matrix(best_metrics['labels'], best_metrics['preds'], labels=[0, 1])\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "classes = ['No PCL (0)', 'PCL (1)']\n",
    "tick_marks = [0, 1]\n",
    "ax.set_xticks(tick_marks)\n",
    "ax.set_xticklabels(classes, fontsize=11)\n",
    "ax.set_yticks(tick_marks)\n",
    "ax.set_yticklabels(classes, fontsize=11)\n",
    "\n",
    "# Annotate each cell with count\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        ax.text(j, i, f'{cm[i, j]}', ha='center', va='center', fontsize=16, fontweight='bold', color=color)\n",
    "\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix — Best Model ({best_key}, t={best_threshold:.2f})', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{BASE_DIR}/custom_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure saved to {BASE_DIR}/custom_metrics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Ablation Study Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ABLATION STUDY\n",
      "================================================================================\n",
      "                         Config Weighted CE Multi-task Thresh Opt Dev F1\n",
      "Baseline (unweighted CE, t=0.5)          No         No         No 0.6120\n",
      "                Config A (full)         Yes        Yes        Yes 0.6467\n",
      "               A w/o Multi-task         Yes         No        Yes 0.6215\n",
      "            A w/o Threshold Opt         Yes        Yes         No 0.6284\n",
      "              A w/o Weighted CE          No        Yes        Yes 0.6194\n",
      "\n",
      "Component contributions (F1 drop when removed from Config A):\n",
      "  Multi-task learning:    +0.0252\n",
      "  Threshold optimisation: +0.0182\n",
      "  Weighted CE:            +0.0272\n",
      "\n",
      "Total improvement over baseline: +0.0346 F1\n"
     ]
    }
   ],
   "source": [
    "# ---- Ablation Study Summary ----\n",
    "ablation_table = pd.DataFrame([\n",
    "    {'Config': 'Baseline (unweighted CE, t=0.5)', 'Weighted CE': 'No', 'Multi-task': 'No', 'Thresh Opt': 'No',\n",
    "     'Dev F1': f'{thresh_metrics_bl[\"f1\"]:.4f}'},\n",
    "    {'Config': 'Config A (full)', 'Weighted CE': 'Yes', 'Multi-task': 'Yes', 'Thresh Opt': 'Yes',\n",
    "     'Dev F1': f'{thresh_metrics_a[\"f1\"]:.4f}'},\n",
    "    {'Config': 'A w/o Multi-task', 'Weighted CE': 'Yes', 'Multi-task': 'No', 'Thresh Opt': 'Yes',\n",
    "     'Dev F1': f'{thresh_metrics_abl_nomt[\"f1\"]:.4f}'},\n",
    "    {'Config': 'A w/o Threshold Opt', 'Weighted CE': 'Yes', 'Multi-task': 'Yes', 'Thresh Opt': 'No',\n",
    "     'Dev F1': f'{thresh_metrics_abl_nothresh[\"f1\"]:.4f}'},\n",
    "    {'Config': 'A w/o Weighted CE', 'Weighted CE': 'No', 'Multi-task': 'Yes', 'Thresh Opt': 'Yes',\n",
    "     'Dev F1': f'{thresh_metrics_abl_nowe[\"f1\"]:.4f}'},\n",
    "])\n",
    "\n",
    "print('='*80)\n",
    "print('ABLATION STUDY')\n",
    "print('='*80)\n",
    "print(ablation_table.to_string(index=False))\n",
    "\n",
    "# Component contributions\n",
    "full_f1 = thresh_metrics_a['f1']\n",
    "print(f'\\nComponent contributions (F1 drop when removed from Config A):')\n",
    "print(f'  Multi-task learning:    {full_f1 - thresh_metrics_abl_nomt[\"f1\"]:+.4f}')\n",
    "print(f'  Threshold optimisation: {full_f1 - thresh_metrics_abl_nothresh[\"f1\"]:+.4f}')\n",
    "print(f'  Weighted CE:            {full_f1 - thresh_metrics_abl_nowe[\"f1\"]:+.4f}')\n",
    "print(f'\\nTotal improvement over baseline: {full_f1 - thresh_metrics_bl[\"f1\"]:+.4f} F1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
