{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PCL Detection — Binary Classification Pipeline\n\nSystematic approach to PCL binary classification:\n1. True baseline (RoBERTa-base, unweighted CE, t=0.5)\n2. Incremental improvements: weighted CE, threshold optimisation, multi-task learning\n3. Ablation studies showing contribution of each component\n4. Error analysis and custom metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Auto-detect environment and set batch sizes accordingly\n",
    "ON_GPUDOJO = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ or DEVICE.type == 'cuda'\n",
    "\n",
    "if ON_GPUDOJO:\n",
    "    BASE_DIR = '/home/azureuser/PCL_Detection'\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM = 16\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    print('Running on GPUDOJO (CUDA) — batch_size=2, grad_accum=16')\n",
    "else:\n",
    "    BASE_DIR = '/Users/alexanderchow/Documents/Y3/60035_NLP/PCL_Detection'\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM = 16\n",
    "    EVAL_BATCH_SIZE = 4\n",
    "    print('Running locally (MPS/CPU) — batch_size=2, grad_accum=16')\n",
    "\n",
    "print(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM}')\n",
    "\n",
    "DATA_DIR = f'{BASE_DIR}/data'\n",
    "SPLITS_DIR = f'{BASE_DIR}/practice splits'\n",
    "CHECKPOINT_DIR = f'{BASE_DIR}/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8375 samples (794 PCL)\n",
      "Dev:   2094 samples (199 PCL)\n",
      "\n",
      "Train class distribution:\n",
      "binary_label\n",
      "0    7581\n",
      "1     794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load main PCL dataset (skip 4 header lines)\n",
    "pcl_df = pd.read_csv(\n",
    "    f'{DATA_DIR}/dontpatronizeme_pcl.tsv',\n",
    "    sep='\\t', skiprows=4, header=None,\n",
    "    names=['par_id', 'art_id', 'keyword', 'country_code', 'text', 'label'],\n",
    "    quoting=3\n",
    ")\n",
    "pcl_df['par_id'] = pcl_df['par_id'].astype(int)\n",
    "pcl_df['label'] = pcl_df['label'].astype(int)\n",
    "\n",
    "# Binary label: {0,1}->0, {2,3,4}->1\n",
    "pcl_df['binary_label'] = (pcl_df['label'] >= 2).astype(int)\n",
    "\n",
    "# Clean text: strip <h> tags and HTML artifacts\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)       # remove HTML tags\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)      # remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # normalise whitespace\n",
    "    return text\n",
    "\n",
    "pcl_df['text'] = pcl_df['text'].apply(clean_text)\n",
    "\n",
    "# Load train/dev splits\n",
    "train_splits = pd.read_csv(f'{SPLITS_DIR}/train_semeval_parids-labels.csv')\n",
    "dev_splits = pd.read_csv(f'{SPLITS_DIR}/dev_semeval_parids-labels.csv')\n",
    "train_splits['par_id'] = train_splits['par_id'].astype(int)\n",
    "dev_splits['par_id'] = dev_splits['par_id'].astype(int)\n",
    "\n",
    "# Parse category labels from split files (7-dim multi-label vectors)\n",
    "def parse_category_label(label_str):\n",
    "    try:\n",
    "        return ast.literal_eval(label_str)\n",
    "    except:\n",
    "        return [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "train_splits['category_labels'] = train_splits['label'].apply(parse_category_label)\n",
    "dev_splits['category_labels'] = dev_splits['label'].apply(parse_category_label)\n",
    "\n",
    "# Merge with main data\n",
    "train_ids = set(train_splits['par_id'].values)\n",
    "dev_ids = set(dev_splits['par_id'].values)\n",
    "\n",
    "train_df = pcl_df[pcl_df['par_id'].isin(train_ids)].copy()\n",
    "dev_df = pcl_df[pcl_df['par_id'].isin(dev_ids)].copy()\n",
    "\n",
    "# Merge category labels\n",
    "cat_train = train_splits[['par_id', 'category_labels']].copy()\n",
    "cat_dev = dev_splits[['par_id', 'category_labels']].copy()\n",
    "\n",
    "train_df = train_df.merge(cat_train, on='par_id', how='left')\n",
    "dev_df = dev_df.merge(cat_dev, on='par_id', how='left')\n",
    "\n",
    "# Fill missing category labels with zeros\n",
    "train_df['category_labels'] = train_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "dev_df['category_labels'] = dev_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_df)} samples ({train_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'Dev:   {len(dev_df)} samples ({dev_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'\\nTrain class distribution:')\n",
    "print(train_df['binary_label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODEL_NAME = 'roberta-base'\nMAX_LENGTH = 256\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nclass PCLDataset(Dataset):\n    def __init__(self, texts, binary_labels, category_labels, tokenizer, max_length):\n        self.texts = texts\n        self.binary_labels = binary_labels\n        self.category_labels = category_labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'binary_label': torch.tensor(self.binary_labels[idx], dtype=torch.long),\n            'category_labels': torch.tensor(self.category_labels[idx], dtype=torch.float),\n        }\n\ndef create_datasets(train_df, dev_df, tokenizer, max_length):\n    train_dataset = PCLDataset(\n        texts=train_df['text'].tolist(),\n        binary_labels=train_df['binary_label'].tolist(),\n        category_labels=train_df['category_labels'].tolist(),\n        tokenizer=tokenizer,\n        max_length=max_length\n    )\n    dev_dataset = PCLDataset(\n        texts=dev_df['text'].tolist(),\n        binary_labels=dev_df['binary_label'].tolist(),\n        category_labels=dev_df['category_labels'].tolist(),\n        tokenizer=tokenizer,\n        max_length=max_length\n    )\n    return train_dataset, dev_dataset\n\nprint(f'Tokenizer loaded: {MODEL_NAME}')\nprint(f'Max length: {MAX_LENGTH}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PCLMultiTaskModel(nn.Module):\n    def __init__(self, model_name, num_categories=7, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        hidden_size = self.encoder.config.hidden_size\n\n        self.binary_head = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, 2)\n        )\n\n        self.category_head = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, num_categories)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n\n        binary_logits = self.binary_head(cls_output)\n        category_logits = self.category_head(cls_output)\n\n        return binary_logits, category_logits\n\n\nclass BaselineModel(nn.Module):\n    \"\"\"Simple RoBERTa-base binary classifier (baseline).\"\"\"\n    def __init__(self, model_name='roberta-base', dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        hidden_size = self.encoder.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, 2)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        logits = self.classifier(cls_output)\n        return logits, None  # None for compatibility with evaluate()\n\nprint('Model classes defined: PCLMultiTaskModel, BaselineModel')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\nprint_every_updates = 20\n\ndef free_gpu():\n    \"\"\"Clear GPU memory.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\ndef evaluate(model, dataloader, device, threshold=0.5):\n    \"\"\"Evaluate model on a dataset, return metrics and probabilities.\"\"\"\n    model.eval()\n    all_probs = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['binary_label']\n\n            binary_logits, _ = model(input_ids, attention_mask)\n            probs = F.softmax(binary_logits, dim=1)[:, 1].cpu()\n\n            all_probs.extend(probs.tolist())\n            all_labels.extend(labels.tolist())\n\n    all_preds = [1 if p >= threshold else 0 for p in all_probs]\n    f1 = f1_score(all_labels, all_preds, pos_label=1)\n    precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n    recall = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n\n    return {\n        'f1': f1, 'precision': precision, 'recall': recall,\n        'preds': all_preds, 'labels': all_labels, 'probs': all_probs,\n        'threshold': threshold\n    }\n\n\ndef find_best_threshold(probs, labels):\n    \"\"\"Sweep thresholds on probability outputs to maximise F1.\"\"\"\n    best_f1 = 0.0\n    best_threshold = 0.5\n    for t in np.arange(0.05, 0.95, 0.01):\n        preds = [1 if p >= t else 0 for p in probs]\n        f1 = f1_score(labels, preds, pos_label=1)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_threshold = t\n    return best_threshold, best_f1\n\n\ndef train_model(config_name, train_df, dev_df, tokenizer,\n                use_weighted_ce=True, use_threshold_opt=True, use_multitask=False,\n                num_epochs=10, batch_size=BATCH_SIZE, grad_accum_steps=GRAD_ACCUM,\n                lr=2e-5, weight_decay=0.01, patience=3, category_weight=0.3,\n                model_class=PCLMultiTaskModel, model_name='roberta-base'):\n    \"\"\"Train a model with the given configuration.\"\"\"\n    free_gpu()\n\n    print(f'\\n{\"=\"*60}')\n    print(f'Training Config: {config_name}')\n    print(f'  Model: {model_name}')\n    print(f'  Weighted CE: {use_weighted_ce} | Multi-task: {use_multitask}')\n    print(f'  Threshold Opt: {use_threshold_opt}')\n    print(f'  Epochs: {num_epochs} | LR: {lr} | Patience: {patience}')\n    print(f'  Batch: {batch_size} x {grad_accum_steps} = {batch_size * grad_accum_steps} effective')\n    print(f'{\"=\"*60}')\n\n    effective_train_df = train_df.copy()\n\n    # Create tokenizer for this model\n    tok = AutoTokenizer.from_pretrained(model_name)\n\n    train_dataset = PCLDataset(\n        texts=effective_train_df['text'].tolist(),\n        binary_labels=effective_train_df['binary_label'].tolist(),\n        category_labels=effective_train_df['category_labels'].tolist(),\n        tokenizer=tok, max_length=MAX_LENGTH\n    )\n    dev_dataset = PCLDataset(\n        texts=dev_df['text'].tolist(),\n        binary_labels=dev_df['binary_label'].tolist(),\n        category_labels=dev_df['category_labels'].tolist(),\n        tokenizer=tok, max_length=MAX_LENGTH\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    dev_loader = DataLoader(dev_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n\n    # Model\n    if model_class == BaselineModel:\n        model = BaselineModel(model_name=model_name).to(DEVICE).float()\n    else:\n        model = PCLMultiTaskModel(model_name=model_name).to(DEVICE).float()\n\n    # Loss function\n    if use_weighted_ce:\n        n_neg = (effective_train_df['binary_label'] == 0).sum()\n        n_pos = (effective_train_df['binary_label'] == 1).sum()\n        weight = torch.tensor([1.0, n_neg / n_pos], dtype=torch.float).to(DEVICE)\n        binary_criterion = nn.CrossEntropyLoss(weight=weight)\n        print(f'  CE class weights: [{weight[0]:.3f}, {weight[1]:.3f}]')\n    else:\n        binary_criterion = nn.CrossEntropyLoss()\n        print(f'  Unweighted CE')\n\n    category_criterion = nn.BCEWithLogitsLoss()\n\n    # Optimizer & scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    total_steps = len(train_loader) * num_epochs // grad_accum_steps\n    warmup_steps = int(0.1 * total_steps)\n    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\n    best_f1 = 0.0\n    patience_counter = 0\n    history = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n        optimizer.zero_grad()\n\n        for step, batch in enumerate(train_loader):\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            binary_labels = batch['binary_label'].to(DEVICE)\n            category_labels = batch['category_labels'].to(DEVICE)\n\n            binary_logits, category_logits = model(input_ids, attention_mask)\n\n            loss_binary = binary_criterion(binary_logits, binary_labels)\n            if use_multitask and category_logits is not None:\n                loss_category = category_criterion(category_logits, category_labels)\n                loss = loss_binary + category_weight * loss_category\n            else:\n                loss = loss_binary\n            loss = loss / grad_accum_steps\n\n            loss.backward()\n            total_loss += loss.item() * grad_accum_steps\n\n            if (step + 1) % grad_accum_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n\n                update = (step + 1) // grad_accum_steps\n                if update % print_every_updates == 0:\n                    avg_recent = total_loss / (step + 1)\n                    print(f\"    step {step+1}/{len(train_loader)} \"\n                          f\"(update {update}) | avg loss so far: {avg_recent:.4f}\")\n\n        # Handle remaining gradients\n        if (step + 1) % grad_accum_steps != 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        avg_loss = total_loss / len(train_loader)\n\n        # Evaluate on dev at t=0.5\n        metrics = evaluate(model, dev_loader, DEVICE, threshold=0.5)\n        history.append({\n            'epoch': epoch + 1, 'loss': avg_loss,\n            'f1': metrics['f1'], 'precision': metrics['precision'], 'recall': metrics['recall']\n        })\n\n        print(f'  Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f} | '\n              f'F1: {metrics[\"f1\"]:.4f} | P: {metrics[\"precision\"]:.4f} | R: {metrics[\"recall\"]:.4f}')\n\n        if metrics['f1'] > best_f1:\n            best_f1 = metrics['f1']\n            patience_counter = 0\n            torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/{config_name}_best.pt')\n            print(f'  -> New best F1! Saved.')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f'  Early stopping at epoch {epoch+1}')\n                break\n\n    # Load best model\n    model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{config_name}_best.pt', weights_only=True))\n\n    # Evaluate at t=0.5\n    final_metrics = evaluate(model, dev_loader, DEVICE, threshold=0.5)\n\n    # Threshold optimisation (if enabled)\n    if use_threshold_opt:\n        best_thresh, _ = find_best_threshold(final_metrics['probs'], final_metrics['labels'])\n        thresh_metrics = evaluate(model, dev_loader, DEVICE, threshold=best_thresh)\n    else:\n        best_thresh = 0.5\n        thresh_metrics = final_metrics\n\n    print(f'\\n  Dev F1 @ t=0.50: {final_metrics[\"f1\"]:.4f}')\n    if use_threshold_opt:\n        print(f'  Dev F1 @ t={best_thresh:.2f} (optimised): {thresh_metrics[\"f1\"]:.4f}')\n    print(classification_report(\n        thresh_metrics['labels'], thresh_metrics['preds'],\n        target_names=['No PCL', 'PCL'], digits=4\n    ))\n\n    # Move to CPU and free GPU\n    model = model.cpu()\n    del model; free_gpu()\n\n    return final_metrics, thresh_metrics, history, best_thresh, tok\n\nprint('Training function defined.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. True Baseline: RoBERTa-base + Unweighted CE + t=0.5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# True baseline: RoBERTa-base, unweighted CE, no tricks, fixed threshold=0.5\nBASELINE_MODEL = 'roberta-base'\n\nmetrics_bl, thresh_metrics_bl, history_bl, thresh_bl, tok_bl = train_model(\n    config_name='baseline',\n    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n    model_class=BaselineModel, model_name=BASELINE_MODEL,\n    use_weighted_ce=False, use_multitask=False, use_threshold_opt=False,\n    lr=2e-5, patience=3\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Incremental Improvements"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Config A: Weighted CE + Threshold Optimisation + Multi-task Learning\nmetrics_a, thresh_metrics_a, history_a, thresh_a, tok_a = train_model(\n    config_name='config_A_weighted_ce_thresh_mt',\n    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n    model_class=PCLMultiTaskModel, model_name='roberta-base',\n    use_weighted_ce=True, use_multitask=True, use_threshold_opt=True,\n    lr=2e-5, patience=5\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Ablation Studies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ablation 1: Config A without multi-task (isolate multi-task contribution)\nmetrics_abl_nomt, thresh_metrics_abl_nomt, history_abl_nomt, thresh_abl_nomt, tok_abl_nomt = train_model(\n    config_name='ablation_no_multitask',\n    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n    model_class=BaselineModel, model_name='roberta-base',\n    use_weighted_ce=True, use_multitask=False, use_threshold_opt=True,\n    lr=2e-5, patience=5\n)\n\n# Ablation 2: Config A without threshold opt (isolate threshold contribution)\nmetrics_abl_nothresh, thresh_metrics_abl_nothresh, history_abl_nothresh, thresh_abl_nothresh, tok_abl_nothresh = train_model(\n    config_name='ablation_no_thresh',\n    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n    model_class=PCLMultiTaskModel, model_name='roberta-base',\n    use_weighted_ce=True, use_multitask=True, use_threshold_opt=False,\n    lr=2e-5, patience=5\n)\n\n# Ablation 3: Config A without weighted CE (isolate weighted CE contribution)\nmetrics_abl_nowe, thresh_metrics_abl_nowe, history_abl_nowe, thresh_abl_nowe, tok_abl_nowe = train_model(\n    config_name='ablation_no_weighted_ce',\n    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n    model_class=PCLMultiTaskModel, model_name='roberta-base',\n    use_weighted_ce=False, use_multitask=True, use_threshold_opt=True,\n    lr=2e-5, patience=5\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Comparison & Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Results comparison table ----\nresults = pd.DataFrame([\n    {\n        'Config': 'Baseline (unweighted CE, t=0.5)',\n        'Threshold': '0.50',\n        'F1': thresh_metrics_bl['f1'],\n        'Precision': thresh_metrics_bl['precision'],\n        'Recall': thresh_metrics_bl['recall'],\n    },\n    {\n        'Config': 'A: + Weighted CE + Thresh Opt + Multi-task',\n        'Threshold': f'{thresh_a:.2f}',\n        'F1': thresh_metrics_a['f1'],\n        'Precision': thresh_metrics_a['precision'],\n        'Recall': thresh_metrics_a['recall'],\n    },\n    {\n        'Config': 'Ablation: A w/o Multi-task',\n        'Threshold': f'{thresh_abl_nomt:.2f}',\n        'F1': thresh_metrics_abl_nomt['f1'],\n        'Precision': thresh_metrics_abl_nomt['precision'],\n        'Recall': thresh_metrics_abl_nomt['recall'],\n    },\n    {\n        'Config': 'Ablation: A w/o Threshold Opt',\n        'Threshold': '0.50',\n        'F1': thresh_metrics_abl_nothresh['f1'],\n        'Precision': thresh_metrics_abl_nothresh['precision'],\n        'Recall': thresh_metrics_abl_nothresh['recall'],\n    },\n    {\n        'Config': 'Ablation: A w/o Weighted CE',\n        'Threshold': f'{thresh_abl_nowe:.2f}',\n        'F1': thresh_metrics_abl_nowe['f1'],\n        'Precision': thresh_metrics_abl_nowe['precision'],\n        'Recall': thresh_metrics_abl_nowe['recall'],\n    },\n])\n\nprint('\\n' + '='*80)\nprint('RESULTS COMPARISON (all models evaluated on dev set)')\nprint('='*80)\nprint(results.to_string(index=False, float_format='{:.4f}'.format))\n\n# Best model is Config A\nbest_metrics = thresh_metrics_a\nbest_threshold = thresh_a\nbest_tok = tok_a\nbest_ckpt_name = 'config_A_weighted_ce_thresh_mt'\nbest_model_class = PCLMultiTaskModel\nbest_model_name = 'roberta-base'\nbest_key = 'A'\n\nimprovement = best_metrics['f1'] - thresh_metrics_bl['f1']\nprint(f'\\n** Best model: Config A (F1={best_metrics[\"f1\"]:.4f} @ t={best_threshold:.2f}) **')\nprint(f'   Improvement over baseline: +{improvement:.4f} F1')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate dev.txt and test.txt Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Dev predictions ----\ndev_preds = best_metrics['preds']\ndev_pred_path = f'{BASE_DIR}/dev.txt'\nwith open(dev_pred_path, 'w') as f:\n    for p in dev_preds:\n        f.write(f'{p}\\n')\nprint(f'Dev predictions saved to {dev_pred_path}')\nprint(f'  {len(dev_preds)} predictions, {sum(dev_preds)} predicted PCL')\n\n# ---- Test predictions ----\ntest_df = pd.read_csv(f'{DATA_DIR}/task4_test.tsv', sep='\\t', header=None,\n                       names=['par_id', 'art_id', 'keyword', 'country_code', 'text'])\ntest_df['text'] = test_df['text'].apply(clean_text)\nprint(f'\\nTest set: {len(test_df)} samples')\n\ntest_dataset = PCLDataset(\n    texts=test_df['text'].tolist(),\n    binary_labels=[0] * len(test_df),\n    category_labels=[[0]*7] * len(test_df),\n    tokenizer=best_tok, max_length=MAX_LENGTH\n)\ntest_loader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n\n# Reload best model from checkpoint\nbest_model = PCLMultiTaskModel(model_name=best_model_name).to(DEVICE)\nbest_model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{best_ckpt_name}_best.pt', weights_only=True, map_location=DEVICE))\nbest_model.eval()\n\ntest_probs = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        binary_logits, _ = best_model(input_ids, attention_mask)\n        probs = F.softmax(binary_logits, dim=1)[:, 1].cpu().tolist()\n        test_probs.extend(probs)\n\ndel best_model; gc.collect(); torch.cuda.empty_cache()\n\ntest_preds = [1 if p >= best_threshold else 0 for p in test_probs]\n\ntest_pred_path = f'{BASE_DIR}/test.txt'\nwith open(test_pred_path, 'w') as f:\n    for p in test_preds:\n        f.write(f'{p}\\n')\nprint(f'Test predictions saved to {test_pred_path}')\nprint(f'  {len(test_preds)} predictions, {sum(test_preds)} predicted PCL')\nprint(f'  Using threshold: {best_threshold:.2f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Error analysis: compare baseline vs best model ----\n# Reload baseline from checkpoint\nmodel_baseline = BaselineModel(model_name=BASELINE_MODEL).to(DEVICE)\nmodel_baseline.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/baseline_best.pt', weights_only=True, map_location=DEVICE))\n\nbaseline_dev_metrics = evaluate(model_baseline,\n    DataLoader(PCLDataset(dev_df['text'].tolist(), dev_df['binary_label'].tolist(),\n                          dev_df['category_labels'].tolist(), tok_bl, MAX_LENGTH),\n               batch_size=EVAL_BATCH_SIZE, shuffle=False),\n    DEVICE, threshold=0.5)\n\ndel model_baseline; gc.collect(); torch.cuda.empty_cache()\n\nbaseline_preds = baseline_dev_metrics['preds']\nbest_preds = best_metrics['preds']\ntrue_labels = best_metrics['labels']\ndev_texts = dev_df['text'].tolist()\n\nprint('='*60)\nprint('ERROR ANALYSIS: Baseline vs Best Model on Dev Set')\nprint('='*60)\n\nbaseline_fp = sum(1 for t, p in zip(true_labels, baseline_preds) if t == 0 and p == 1)\nbaseline_fn = sum(1 for t, p in zip(true_labels, baseline_preds) if t == 1 and p == 0)\nbaseline_tp = sum(1 for t, p in zip(true_labels, baseline_preds) if t == 1 and p == 1)\nbest_fp = sum(1 for t, p in zip(true_labels, best_preds) if t == 0 and p == 1)\nbest_fn = sum(1 for t, p in zip(true_labels, best_preds) if t == 1 and p == 0)\nbest_tp = sum(1 for t, p in zip(true_labels, best_preds) if t == 1 and p == 1)\n\nprint(f'\\n{\"Metric\":<25} {\"Baseline\":>10} {\"Best Model\":>12}')\nprint('-' * 50)\nprint(f'{\"True Positives\":<25} {baseline_tp:>10} {best_tp:>12}')\nprint(f'{\"False Positives\":<25} {baseline_fp:>10} {best_fp:>12}')\nprint(f'{\"False Negatives\":<25} {baseline_fn:>10} {best_fn:>12}')\n\nfixed_fn = []\nfixed_fp = []\nfor i, (t, bp, mp) in enumerate(zip(true_labels, baseline_preds, best_preds)):\n    if t == 1 and bp == 0 and mp == 1:\n        fixed_fn.append(i)\n    if t == 0 and bp == 1 and mp == 0:\n        fixed_fp.append(i)\n\nprint(f'\\nBest model fixes {len(fixed_fn)} FN and {len(fixed_fp)} FP from baseline')\n\nprint(f'\\n--- PCL missed by baseline but caught by best model ({min(5, len(fixed_fn))} shown) ---')\nfor idx in fixed_fn[:5]:\n    print(f'  [{idx}] {dev_texts[idx][:150]}...')\n\nremaining_fn = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 1 and p == 0]\nprint(f'\\n--- Remaining false negatives ({min(5, len(remaining_fn))}/{len(remaining_fn)} shown) ---')\nfor idx in remaining_fn[:5]:\n    print(f'  [{idx}] {dev_texts[idx][:150]}...')\n\nfn_lengths = [len(dev_texts[i].split()) for i in remaining_fn]\nfp_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 0 and p == 1]\nfp_lengths = [len(dev_texts[i].split()) for i in fp_indices]\nall_lengths = [len(t.split()) for t in dev_texts]\n\nprint(f'\\n--- Text length analysis ---')\nprint(f'  Overall mean length:     {np.mean(all_lengths):.1f} words')\nprint(f'  False negative mean:     {np.mean(fn_lengths):.1f} words' if fn_lengths else '  No false negatives')\nprint(f'  False positive mean:     {np.mean(fp_lengths):.1f} words' if fp_lengths else '  No false positives')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Precision-Recall Curve: Best Model vs Baseline ----\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# --- Left: Precision-Recall Curves ---\nax = axes[0]\n\n# Best model PR curve\nprec_best, rec_best, thresholds_best = precision_recall_curve(\n    best_metrics['labels'], best_metrics['probs'], pos_label=1)\nax.plot(rec_best, prec_best, label=f'Best model ({best_key})', color='tab:blue', linewidth=2)\n\n# Baseline PR curve\nprec_base, rec_base, thresholds_base = precision_recall_curve(\n    baseline_dev_metrics['labels'], baseline_dev_metrics['probs'], pos_label=1)\nax.plot(rec_base, prec_base, label='RoBERTa-base baseline', color='tab:orange', linewidth=2, linestyle='--')\n\n# Mark the operating points\nax.scatter([best_metrics['recall']], [best_metrics['precision']],\n           marker='*', s=200, color='tab:blue', zorder=5, label=f'Best @ t={best_threshold:.2f}')\nax.scatter([baseline_dev_metrics['recall']], [baseline_dev_metrics['precision']],\n           marker='*', s=200, color='tab:orange', zorder=5, label=f'Baseline @ t={thresh_bl:.2f}')\n\nax.set_xlabel('Recall', fontsize=12)\nax.set_ylabel('Precision', fontsize=12)\nax.set_title('Precision-Recall Curve', fontsize=13)\nax.legend(fontsize=9)\nax.set_xlim([0, 1.02])\nax.set_ylim([0, 1.02])\nax.grid(True, alpha=0.3)\n\n# --- Right: Confusion Matrix Heatmap (Best Model) ---\nax = axes[1]\n\ncm = confusion_matrix(best_metrics['labels'], best_metrics['preds'], labels=[0, 1])\nim = ax.imshow(cm, interpolation='nearest', cmap='Blues')\nfig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\nclasses = ['No PCL (0)', 'PCL (1)']\ntick_marks = [0, 1]\nax.set_xticks(tick_marks)\nax.set_xticklabels(classes, fontsize=11)\nax.set_yticks(tick_marks)\nax.set_yticklabels(classes, fontsize=11)\n\n# Annotate each cell with count\nfor i in range(2):\n    for j in range(2):\n        color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n        ax.text(j, i, f'{cm[i, j]}', ha='center', va='center', fontsize=16, fontweight='bold', color=color)\n\nax.set_xlabel('Predicted', fontsize=12)\nax.set_ylabel('Actual', fontsize=12)\nax.set_title(f'Confusion Matrix — Best Model ({best_key}, t={best_threshold:.2f})', fontsize=13)\n\nplt.tight_layout()\nplt.savefig(f'{BASE_DIR}/custom_metrics.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(f'Figure saved to {BASE_DIR}/custom_metrics.png')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Ablation Study Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Ablation Study Summary ----\nablation_table = pd.DataFrame([\n    {'Config': 'Baseline (unweighted CE, t=0.5)', 'Weighted CE': 'No', 'Multi-task': 'No', 'Thresh Opt': 'No',\n     'Dev F1': f'{thresh_metrics_bl[\"f1\"]:.4f}'},\n    {'Config': 'Config A (full)', 'Weighted CE': 'Yes', 'Multi-task': 'Yes', 'Thresh Opt': 'Yes',\n     'Dev F1': f'{thresh_metrics_a[\"f1\"]:.4f}'},\n    {'Config': 'A w/o Multi-task', 'Weighted CE': 'Yes', 'Multi-task': 'No', 'Thresh Opt': 'Yes',\n     'Dev F1': f'{thresh_metrics_abl_nomt[\"f1\"]:.4f}'},\n    {'Config': 'A w/o Threshold Opt', 'Weighted CE': 'Yes', 'Multi-task': 'Yes', 'Thresh Opt': 'No',\n     'Dev F1': f'{thresh_metrics_abl_nothresh[\"f1\"]:.4f}'},\n    {'Config': 'A w/o Weighted CE', 'Weighted CE': 'No', 'Multi-task': 'Yes', 'Thresh Opt': 'Yes',\n     'Dev F1': f'{thresh_metrics_abl_nowe[\"f1\"]:.4f}'},\n])\n\nprint('='*80)\nprint('ABLATION STUDY')\nprint('='*80)\nprint(ablation_table.to_string(index=False))\n\n# Component contributions\nfull_f1 = thresh_metrics_a['f1']\nprint(f'\\nComponent contributions (F1 drop when removed from Config A):')\nprint(f'  Multi-task learning:    {full_f1 - thresh_metrics_abl_nomt[\"f1\"]:+.4f}')\nprint(f'  Threshold optimisation: {full_f1 - thresh_metrics_abl_nothresh[\"f1\"]:+.4f}')\nprint(f'  Weighted CE:            {full_f1 - thresh_metrics_abl_nowe[\"f1\"]:+.4f}')\nprint(f'\\nTotal improvement over baseline: {full_f1 - thresh_metrics_bl[\"f1\"]:+.4f} F1')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}