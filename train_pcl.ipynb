{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCL Detection — Binary Classification Pipeline\n",
    "\n",
    "Systematic approach to PCL binary classification:\n",
    "1. True baseline (RoBERTa-base, unweighted CE, t=0.5)\n",
    "2. Incremental improvements: weighted CE, threshold optimisation, multi-task learning\n",
    "3. Ablation studies showing contribution of each component\n",
    "4. Error analysis and custom metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/akc123/PCL_Detection/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Running on GPU — batch_size=4, grad_accum=8\n",
      "Effective batch size: 32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Auto-detect environment and set batch sizes accordingly\n",
    "ON_GPUDOJO = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ or DEVICE.type == 'cuda'\n",
    "\n",
    "if ON_GPUDOJO:\n",
    "    BASE_DIR = '/vol/bitbucket/akc123/PCL_Detection'\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUM = 8\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    print('Running on GPU — batch_size=4, grad_accum=8')\n",
    "else:\n",
    "    BASE_DIR = '/Users/alexanderchow/Documents/Y3/60035_NLP/PCL_Detection'\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM = 16\n",
    "    EVAL_BATCH_SIZE = 4\n",
    "    print('Running locally (MPS/CPU) — batch_size=2, grad_accum=16')\n",
    "\n",
    "print(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM}')\n",
    "\n",
    "DATA_DIR = f'{BASE_DIR}/data'\n",
    "SPLITS_DIR = f'{BASE_DIR}/practice splits'\n",
    "CHECKPOINT_DIR = f'{BASE_DIR}/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8375 samples (794 PCL)\n",
      "Dev:   2094 samples (199 PCL)\n",
      "\n",
      "Train class distribution:\n",
      "binary_label\n",
      "0    7581\n",
      "1     794\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load main PCL dataset (skip 4 header lines)\n",
    "pcl_df = pd.read_csv(\n",
    "    f'{DATA_DIR}/dontpatronizeme_pcl.tsv',\n",
    "    sep='\\t', skiprows=4, header=None,\n",
    "    names=['par_id', 'art_id', 'keyword', 'country_code', 'text', 'label'],\n",
    "    quoting=3\n",
    ")\n",
    "pcl_df['par_id'] = pcl_df['par_id'].astype(int)\n",
    "pcl_df['label'] = pcl_df['label'].astype(int)\n",
    "\n",
    "# Binary label: {0,1}->0, {2,3,4}->1\n",
    "pcl_df['binary_label'] = (pcl_df['label'] >= 2).astype(int)\n",
    "\n",
    "# Clean text: strip <h> tags and HTML artifacts\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)       # remove HTML tags\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)      # remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # normalise whitespace\n",
    "    return text\n",
    "pcl_df['text'] = pcl_df['text'].apply(clean_text)\n",
    "\n",
    "# Load train/dev splits\n",
    "train_splits = pd.read_csv(f'{SPLITS_DIR}/train_semeval_parids-labels.csv')\n",
    "dev_splits = pd.read_csv(f'{SPLITS_DIR}/dev_semeval_parids-labels.csv')\n",
    "train_splits['par_id'] = train_splits['par_id'].astype(int)\n",
    "dev_splits['par_id'] = dev_splits['par_id'].astype(int)\n",
    "\n",
    "# Parse category labels from split files (7-dim multi-label vectors)\n",
    "def parse_category_label(label_str):\n",
    "    try:\n",
    "        return ast.literal_eval(label_str)\n",
    "    except:\n",
    "        return [0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "train_splits['category_labels'] = train_splits['label'].apply(parse_category_label)\n",
    "dev_splits['category_labels'] = dev_splits['label'].apply(parse_category_label)\n",
    "\n",
    "# Merge with main data\n",
    "train_ids = set(train_splits['par_id'].values)\n",
    "dev_ids = set(dev_splits['par_id'].values)\n",
    "\n",
    "train_df = pcl_df[pcl_df['par_id'].isin(train_ids)].copy()\n",
    "dev_df = pcl_df[pcl_df['par_id'].isin(dev_ids)].copy()\n",
    "\n",
    "# Merge category labels\n",
    "cat_train = train_splits[['par_id', 'category_labels']].copy()\n",
    "cat_dev = dev_splits[['par_id', 'category_labels']].copy()\n",
    "\n",
    "train_df = train_df.merge(cat_train, on='par_id', how='left')\n",
    "dev_df = dev_df.merge(cat_dev, on='par_id', how='left')\n",
    "\n",
    "# Fill missing category labels with zeros\n",
    "train_df['category_labels'] = train_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "dev_df['category_labels'] = dev_df['category_labels'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0]*7\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_df)} samples ({train_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'Dev:   {len(dev_df)} samples ({dev_df[\"binary_label\"].sum()} PCL)')\n",
    "print(f'\\nTrain class distribution:')\n",
    "print(train_df['binary_label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: roberta-base\n",
      "Max length: 256\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, texts, binary_labels, category_labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.binary_labels = binary_labels\n",
    "        self.category_labels = category_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'binary_label': torch.tensor(self.binary_labels[idx], dtype=torch.long),\n",
    "            'category_labels': torch.tensor(self.category_labels[idx], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def create_datasets(train_df, dev_df, tokenizer, max_length):\n",
    "    train_dataset = PCLDataset(\n",
    "        texts=train_df['text'].tolist(),\n",
    "        binary_labels=train_df['binary_label'].tolist(),\n",
    "        category_labels=train_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    dev_dataset = PCLDataset(\n",
    "        texts=dev_df['text'].tolist(),\n",
    "        binary_labels=dev_df['binary_label'].tolist(),\n",
    "        category_labels=dev_df['category_labels'].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return train_dataset, dev_dataset\n",
    "\n",
    "print(f'Tokenizer loaded: {MODEL_NAME}')\n",
    "print(f'Max length: {MAX_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes defined: PCLMultiTaskModel, BaselineModel\n"
     ]
    }
   ],
   "source": [
    "class PCLMultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name, num_categories=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        self.binary_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "        self.category_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_categories)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        binary_logits = self.binary_head(cls_output)\n",
    "        category_logits = self.category_head(cls_output)\n",
    "\n",
    "        return binary_logits, category_logits\n",
    "\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"Simple RoBERTa-base binary classifier (baseline).\"\"\"\n",
    "    def __init__(self, model_name='roberta-base', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits, None  # None for compatibility with evaluate()\n",
    "\n",
    "print('Model classes defined: PCLMultiTaskModel, BaselineModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "print_every_updates = 20\n",
    "\n",
    "def free_gpu():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def evaluate(model, dataloader, device, threshold=0.5):\n",
    "    \"\"\"Evaluate model on a dataset, return metrics and probabilities.\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['binary_label']\n",
    "\n",
    "            binary_logits, _ = model(input_ids, attention_mask)\n",
    "            probs = F.softmax(binary_logits, dim=1)[:, 1].cpu()\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    all_preds = [1 if p >= threshold else 0 for p in all_probs]\n",
    "    f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
    "    precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'f1': f1, 'precision': precision, 'recall': recall,\n",
    "        'preds': all_preds, 'labels': all_labels, 'probs': all_probs,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_threshold(probs, labels):\n",
    "    \"\"\"Sweep thresholds on probability outputs to maximise F1.\"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_threshold = 0.5\n",
    "    for t in np.arange(0.05, 0.95, 0.01):\n",
    "        preds = [1 if p >= t else 0 for p in probs]\n",
    "        f1 = f1_score(labels, preds, pos_label=1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "def train_model(config_name, train_df, dev_df, tokenizer,\n",
    "                use_weighted_ce=True, use_threshold_opt=True, use_multitask=False,\n",
    "                num_epochs=10, batch_size=BATCH_SIZE, grad_accum_steps=GRAD_ACCUM,\n",
    "                lr=2e-5, weight_decay=0.01, patience=3, category_weight=0.3,\n",
    "                model_class=PCLMultiTaskModel, model_name='roberta-base'):\n",
    "    \"\"\"Train a model with the given configuration.\"\"\"\n",
    "    free_gpu()\n",
    "\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training Config: {config_name}')\n",
    "    print(f'  Model: {model_name}')\n",
    "    print(f'  Weighted CE: {use_weighted_ce} | Multi-task: {use_multitask}')\n",
    "    print(f'  Threshold Opt: {use_threshold_opt}')\n",
    "    print(f'  Epochs: {num_epochs} | LR: {lr} | Patience: {patience}')\n",
    "    print(f'  Batch: {batch_size} x {grad_accum_steps} = {batch_size * grad_accum_steps} effective')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    effective_train_df = train_df.copy()\n",
    "\n",
    "    # Create tokenizer for this model\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = PCLDataset(\n",
    "        texts=effective_train_df['text'].tolist(),\n",
    "        binary_labels=effective_train_df['binary_label'].tolist(),\n",
    "        category_labels=effective_train_df['category_labels'].tolist(),\n",
    "        tokenizer=tok, max_length=MAX_LENGTH\n",
    "    )\n",
    "    dev_dataset = PCLDataset(\n",
    "        texts=dev_df['text'].tolist(),\n",
    "        binary_labels=dev_df['binary_label'].tolist(),\n",
    "        category_labels=dev_df['category_labels'].tolist(),\n",
    "        tokenizer=tok, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    if model_class == BaselineModel:\n",
    "        model = BaselineModel(model_name=model_name).to(DEVICE).float()\n",
    "    else:\n",
    "        model = PCLMultiTaskModel(model_name=model_name).to(DEVICE).float()\n",
    "\n",
    "    # Loss function\n",
    "    if use_weighted_ce:\n",
    "        n_neg = (effective_train_df['binary_label'] == 0).sum()\n",
    "        n_pos = (effective_train_df['binary_label'] == 1).sum()\n",
    "        weight = torch.tensor([1.0, n_neg / n_pos], dtype=torch.float).to(DEVICE)\n",
    "        binary_criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        print(f'  CE class weights: [{weight[0]:.3f}, {weight[1]:.3f}]')\n",
    "    else:\n",
    "        binary_criterion = nn.CrossEntropyLoss()\n",
    "        print(f'  Unweighted CE')\n",
    "\n",
    "    category_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * num_epochs // grad_accum_steps\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            binary_labels = batch['binary_label'].to(DEVICE)\n",
    "            category_labels = batch['category_labels'].to(DEVICE)\n",
    "\n",
    "            binary_logits, category_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_binary = binary_criterion(binary_logits, binary_labels)\n",
    "            if use_multitask and category_logits is not None:\n",
    "                loss_category = category_criterion(category_logits, category_labels)\n",
    "                loss = loss_binary + category_weight * loss_category\n",
    "            else:\n",
    "                loss = loss_binary\n",
    "            loss = loss / grad_accum_steps\n",
    "\n",
    "            loss.backward()\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "            if (step + 1) % grad_accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                update = (step + 1) // grad_accum_steps\n",
    "                if update % print_every_updates == 0:\n",
    "                    avg_recent = total_loss / (step + 1)\n",
    "                    print(f\"    step {step+1}/{len(train_loader)} \"\n",
    "                          f\"(update {update}) | avg loss so far: {avg_recent:.4f}\")\n",
    "\n",
    "        # Handle remaining gradients\n",
    "        if (step + 1) % grad_accum_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on dev at t=0.5\n",
    "        metrics = evaluate(model, dev_loader, DEVICE, threshold=0.5)\n",
    "        history.append({\n",
    "            'epoch': epoch + 1, 'loss': avg_loss,\n",
    "            'f1': metrics['f1'], 'precision': metrics['precision'], 'recall': metrics['recall']\n",
    "        })\n",
    "\n",
    "        print(f'  Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f} | '\n",
    "              f'F1: {metrics[\"f1\"]:.4f} | P: {metrics[\"precision\"]:.4f} | R: {metrics[\"recall\"]:.4f}')\n",
    "\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/{config_name}_best.pt')\n",
    "            print(f'  -> New best F1! Saved.')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'  Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{config_name}_best.pt', weights_only=True))\n",
    "\n",
    "    # Evaluate at t=0.5\n",
    "    final_metrics = evaluate(model, dev_loader, DEVICE, threshold=0.5)\n",
    "\n",
    "    # Threshold optimisation (if enabled)\n",
    "    if use_threshold_opt:\n",
    "        best_thresh, _ = find_best_threshold(final_metrics['probs'], final_metrics['labels'])\n",
    "        thresh_metrics = evaluate(model, dev_loader, DEVICE, threshold=best_thresh)\n",
    "    else:\n",
    "        best_thresh = 0.5\n",
    "        thresh_metrics = final_metrics\n",
    "\n",
    "    print(f'\\n  Dev F1 @ t=0.50: {final_metrics[\"f1\"]:.4f}')\n",
    "    if use_threshold_opt:\n",
    "        print(f'  Dev F1 @ t={best_thresh:.2f} (optimised): {thresh_metrics[\"f1\"]:.4f}')\n",
    "    print(classification_report(\n",
    "        thresh_metrics['labels'], thresh_metrics['preds'],\n",
    "        target_names=['No PCL', 'PCL'], digits=4\n",
    "    ))\n",
    "\n",
    "    # Move to CPU and free GPU\n",
    "    model = model.cpu()\n",
    "    del model; free_gpu()\n",
    "\n",
    "    return final_metrics, thresh_metrics, history, best_thresh, tok\n",
    "\n",
    "print('Training function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. True Baseline: RoBERTa-base + Unweighted CE + t=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: baseline\n",
      "  Model: roberta-base\n",
      "  Weighted CE: False | Multi-task: False\n",
      "  Threshold Opt: False\n",
      "  Epochs: 20 | LR: 2e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1495.35it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unweighted CE\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.6716\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.6459\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.5967\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.5369\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.4966\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.4734\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.4523\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.4350\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.4170\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.4009\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.3882\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.3754\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.3642\n",
      "  Epoch 1/20 — Loss: 0.3626 | F1: 0.4068 | P: 0.6250 | R: 0.3015\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.2469\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.2354\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.2279\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.2216\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.2161\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.2206\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.2147\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.2136\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.2097\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.2060\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.2066\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.2045\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.2093\n",
      "  Epoch 2/20 — Loss: 0.2092 | F1: 0.3985 | P: 0.7500 | R: 0.2714\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.1575\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.1647\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.1725\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.1617\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.1545\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.1484\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.1493\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.1506\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1540\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.1549\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.1605\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1618\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1607\n",
      "  Epoch 3/20 — Loss: 0.1609 | F1: 0.4884 | P: 0.7115 | R: 0.3719\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0847\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0837\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0867\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0904\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0923\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0995\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.1014\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.1045\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1059\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.1031\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.1109\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1111\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1120\n",
      "  Epoch 4/20 — Loss: 0.1121 | F1: 0.4883 | P: 0.7300 | R: 0.3668\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0736\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0789\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0806\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0751\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0656\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0674\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0622\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0697\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0662\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0673\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0699\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0687\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0688\n",
      "  Epoch 5/20 — Loss: 0.0687 | F1: 0.5511 | P: 0.7177 | R: 0.4472\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0574\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0362\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0331\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0389\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0334\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0334\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0364\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0331\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0340\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0348\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0332\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0317\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0314\n",
      "  Epoch 6/20 — Loss: 0.0312 | F1: 0.5785 | P: 0.6402 | R: 0.5276\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0210\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0339\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0322\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0316\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0320\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0276\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0248\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0232\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0221\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0222\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0225\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0217\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0205\n",
      "  Epoch 7/20 — Loss: 0.0211 | F1: 0.5684 | P: 0.5967 | R: 0.5427\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0269\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0172\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0117\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0097\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0116\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0103\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0089\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0087\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0095\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0100\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0096\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0100\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0102\n",
      "  Epoch 8/20 — Loss: 0.0101 | F1: 0.6139 | P: 0.5872 | R: 0.6432\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0122\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0062\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0081\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0071\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0064\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0064\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0072\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0086\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0093\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0113\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0117\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0131\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0121\n",
      "  Epoch 9/20 — Loss: 0.0121 | F1: 0.5815 | P: 0.6331 | R: 0.5377\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0001\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0004\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0049\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0090\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0094\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0081\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0070\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0066\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0059\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0053\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0053\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0056\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0057\n",
      "  Epoch 10/20 — Loss: 0.0057 | F1: 0.5579 | P: 0.6812 | R: 0.4724\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0000\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0000\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0002\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0001\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0001\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0001\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0009\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0028\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0040\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0036\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0045\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0041\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0045\n",
      "  Epoch 11/20 — Loss: 0.0045 | F1: 0.5672 | P: 0.6985 | R: 0.4774\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0000\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0031\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0033\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0025\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0020\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0019\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0017\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0018\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0027\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0035\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0032\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0029\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0032\n",
      "  Epoch 12/20 — Loss: 0.0032 | F1: 0.5536 | P: 0.6788 | R: 0.4673\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0001\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0031\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0074\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0071\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0056\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0047\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0060\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0072\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0065\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0078\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0071\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0074\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0071\n",
      "  Epoch 13/20 — Loss: 0.0071 | F1: 0.5097 | P: 0.7117 | R: 0.3970\n",
      "  Early stopping at epoch 13\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9622    0.9525    0.9573      1895\n",
      "         PCL     0.5872    0.6432    0.6139       199\n",
      "\n",
      "    accuracy                         0.9231      2094\n",
      "   macro avg     0.7747    0.7979    0.7856      2094\n",
      "weighted avg     0.9265    0.9231    0.9247      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# True baseline: RoBERTa-base, unweighted CE, no tricks, fixed threshold=0.5\n",
    "BASELINE_MODEL = 'roberta-base'\n",
    "\n",
    "metrics_bl, thresh_metrics_bl, history_bl, thresh_bl, tok_bl = train_model(\n",
    "    config_name='baseline',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=BaselineModel, model_name=BASELINE_MODEL,\n",
    "    use_weighted_ce=False, use_multitask=False, use_threshold_opt=False,\n",
    "    num_epochs=20, lr=2e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Incremental Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: config_A_weighted_ce_thresh_mt\n",
      "  Model: roberta-base\n",
      "  Weighted CE: True | Multi-task: True\n",
      "  Threshold Opt: True\n",
      "  Epochs: 20 | LR: 2e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1468.59it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CE class weights: [1.000, 9.548]\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.8655\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.8539\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.8352\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.8156\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.8047\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.7962\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.7718\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.7569\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.7279\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.7014\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.6753\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.6679\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.6489\n",
      "  Epoch 1/20 — Loss: 0.6468 | F1: 0.4877 | P: 0.3519 | R: 0.7940\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.3913\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.4242\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.4111\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.4126\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.4215\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.4218\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.4220\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.4191\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.4134\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.4064\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.4058\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.4033\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.4014\n",
      "  Epoch 2/20 — Loss: 0.4000 | F1: 0.5378 | P: 0.4455 | R: 0.6784\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.2985\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.3005\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.2899\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.3042\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.3103\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.3113\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.3048\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.3025\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.3033\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.3032\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.2988\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.2934\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.2923\n",
      "  Epoch 3/20 — Loss: 0.2908 | F1: 0.5127 | P: 0.6923 | R: 0.4070\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.2561\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.2654\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.2341\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.2443\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.2407\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.2303\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.2212\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.2337\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.2273\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.2260\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.2179\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.2146\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.2206\n",
      "  Epoch 4/20 — Loss: 0.2194 | F1: 0.5248 | P: 0.6250 | R: 0.4523\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0706\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0941\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0919\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0908\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.1104\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.1068\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.1165\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.1210\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1238\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.1374\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.1335\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1345\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1368\n",
      "  Epoch 5/20 — Loss: 0.1373 | F1: 0.5923 | P: 0.5169 | R: 0.6935\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0782\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.1056\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.1020\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0859\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0748\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0798\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0770\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0737\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0737\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0750\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0785\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0779\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0868\n",
      "  Epoch 6/20 — Loss: 0.0863 | F1: 0.5698 | P: 0.6759 | R: 0.4925\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0280\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0351\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0399\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0405\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0411\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0363\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0374\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0449\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0472\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0457\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0489\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0475\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0457\n",
      "  Epoch 7/20 — Loss: 0.0455 | F1: 0.5967 | P: 0.5682 | R: 0.6281\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0224\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0449\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0347\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0366\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0457\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0473\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0434\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0488\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0459\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0443\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0447\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0443\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0417\n",
      "  Epoch 8/20 — Loss: 0.0415 | F1: 0.6339 | P: 0.6202 | R: 0.6482\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0107\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0145\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0314\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0352\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0322\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0402\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0364\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0382\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0353\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0330\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0320\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0301\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0302\n",
      "  Epoch 9/20 — Loss: 0.0301 | F1: 0.5513 | P: 0.7611 | R: 0.4322\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0117\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0108\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0098\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0183\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0234\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0217\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0204\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0190\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0187\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0176\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0167\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0171\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0165\n",
      "  Epoch 10/20 — Loss: 0.0164 | F1: 0.6117 | P: 0.5915 | R: 0.6332\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0101\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0091\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0110\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0127\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0129\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0219\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0197\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0181\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0176\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0173\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0164\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0158\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0153\n",
      "  Epoch 11/20 — Loss: 0.0153 | F1: 0.6027 | P: 0.6420 | R: 0.5678\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0140\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0102\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0147\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0125\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0123\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0115\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0108\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0114\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0108\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0106\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0109\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0116\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0114\n",
      "  Epoch 12/20 — Loss: 0.0113 | F1: 0.6236 | P: 0.7070 | R: 0.5578\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0043\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0051\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0054\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0066\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0067\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0063\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0062\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0061\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0059\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0059\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0061\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0061\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0061\n",
      "  Epoch 13/20 — Loss: 0.0061 | F1: 0.5941 | P: 0.7163 | R: 0.5075\n",
      "  Early stopping at epoch 13\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6339\n",
      "  Dev F1 @ t=0.45 (optimised): 0.6373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9634    0.9583    0.9608      1895\n",
      "         PCL     0.6220    0.6533    0.6373       199\n",
      "\n",
      "    accuracy                         0.9293      2094\n",
      "   macro avg     0.7927    0.8058    0.7991      2094\n",
      "weighted avg     0.9310    0.9293    0.9301      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Config A: Weighted CE + Threshold Optimisation + Multi-task Learning\n",
    "metrics_a, thresh_metrics_a, history_a, thresh_a, tok_a = train_model(\n",
    "    config_name='config_A_weighted_ce_thresh_mt',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=PCLMultiTaskModel, model_name='roberta-base',\n",
    "    use_weighted_ce=True, use_multitask=True, use_threshold_opt=True,\n",
    "    num_epochs=20, lr=2e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: config_B_weighted_ce_thresh_mt\n",
      "  Model: roberta-large\n",
      "  Weighted CE: True | Multi-task: True\n",
      "  Threshold Opt: True\n",
      "  Epochs: 20 | LR: 1e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 389/389 [00:01<00:00, 333.06it/s, Materializing param=encoder.layer.23.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CE class weights: [1.000, 9.548]\n",
      "    step 160/2094 (update 20) | avg loss so far: 1.2774\n",
      "    step 320/2094 (update 40) | avg loss so far: 1.2746\n",
      "    step 480/2094 (update 60) | avg loss so far: 1.2198\n",
      "    step 640/2094 (update 80) | avg loss so far: 1.1430\n",
      "    step 800/2094 (update 100) | avg loss so far: 1.1351\n",
      "    step 960/2094 (update 120) | avg loss so far: 1.0829\n",
      "    step 1120/2094 (update 140) | avg loss so far: 1.0470\n",
      "    step 1280/2094 (update 160) | avg loss so far: 1.0098\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.9827\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.9597\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.9337\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.9066\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.8817\n",
      "  Epoch 1/20 — Loss: 0.8800 | F1: 0.4033 | P: 0.2781 | R: 0.7337\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.5479\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.5447\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.5321\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.5036\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.4839\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.4850\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.4627\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.4508\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.4520\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.4512\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.4464\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.4429\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.4444\n",
      "  Epoch 2/20 — Loss: 0.4448 | F1: 0.0488 | P: 0.8333 | R: 0.0251\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.4051\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.3476\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.3647\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.3556\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.3401\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.3319\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.3262\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.3251\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.3271\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.3283\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.3318\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.3316\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.3312\n",
      "  Epoch 3/20 — Loss: 0.3303 | F1: 0.5996 | P: 0.4874 | R: 0.7789\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.2561\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.2523\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.2410\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.2504\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.2342\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.2325\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.2335\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.2357\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.2316\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.2286\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.2304\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.2333\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.2297\n",
      "  Epoch 4/20 — Loss: 0.2288 | F1: 0.5992 | P: 0.5164 | R: 0.7136\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0812\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.1266\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.1406\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.1651\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.1615\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.1488\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.1592\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.1524\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1530\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.1465\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.1474\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1587\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1605\n",
      "  Epoch 5/20 — Loss: 0.1595 | F1: 0.5917 | P: 0.5762 | R: 0.6080\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.1053\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0996\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0899\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0812\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.1084\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0949\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0999\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0913\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1070\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0993\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0981\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1123\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1148\n",
      "  Epoch 6/20 — Loss: 0.1141 | F1: 0.6004 | P: 0.5265 | R: 0.6985\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0187\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0191\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0233\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0366\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0315\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0292\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0352\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0335\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0361\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0401\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0447\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0430\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0414\n",
      "  Epoch 7/20 — Loss: 0.0413 | F1: 0.5528 | P: 0.7236 | R: 0.4472\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0651\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0414\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0322\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0266\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0237\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0250\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0234\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0230\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0215\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0314\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0329\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0324\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0312\n",
      "  Epoch 8/20 — Loss: 0.0311 | F1: 0.5855 | P: 0.6043 | R: 0.5678\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0089\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0087\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0096\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0106\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0110\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0138\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0314\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0288\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0266\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0271\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0255\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0248\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0250\n",
      "  Epoch 9/20 — Loss: 0.0249 | F1: 0.5975 | P: 0.6020 | R: 0.5930\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0098\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0185\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0421\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0361\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0488\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0417\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0373\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0352\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0329\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0305\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0296\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0288\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0275\n",
      "  Epoch 10/20 — Loss: 0.0273 | F1: 0.5820 | P: 0.6145 | R: 0.5528\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0094\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0206\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0194\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0168\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0150\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0227\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0203\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0189\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0177\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0186\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0187\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0177\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0168\n",
      "  Epoch 11/20 — Loss: 0.0171 | F1: 0.6034 | P: 0.6792 | R: 0.5427\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0389\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0224\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0267\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0233\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0200\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0241\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0213\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0234\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0214\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0203\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0194\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0195\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0185\n",
      "  Epoch 12/20 — Loss: 0.0184 | F1: 0.5698 | P: 0.6759 | R: 0.4925\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0047\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0052\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0081\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0073\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0066\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0064\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0066\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0063\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0061\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0060\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0057\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0058\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0057\n",
      "  Epoch 13/20 — Loss: 0.0057 | F1: 0.6067 | P: 0.6211 | R: 0.5930\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0427\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0236\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0170\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0140\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0119\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0153\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0135\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0122\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0112\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0104\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0099\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0095\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0090\n",
      "  Epoch 14/20 — Loss: 0.0090 | F1: 0.5913 | P: 0.6053 | R: 0.5779\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0020\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0020\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0024\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0033\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0033\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0034\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0032\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0031\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0030\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0030\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0031\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0030\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0031\n",
      "  Epoch 15/20 — Loss: 0.0031 | F1: 0.5831 | P: 0.6944 | R: 0.5025\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0028\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0026\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0023\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0025\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0024\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0024\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0024\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0023\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0023\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0022\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0022\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0022\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0022\n",
      "  Epoch 16/20 — Loss: 0.0022 | F1: 0.6101 | P: 0.6461 | R: 0.5779\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0019\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0015\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0019\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0018\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0017\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0018\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0018\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0018\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0020\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0020\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0020\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0019\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0019\n",
      "  Epoch 17/20 — Loss: 0.0019 | F1: 0.5984 | P: 0.6264 | R: 0.5729\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0026\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0018\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0016\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0015\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0015\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0015\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0015\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0015\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0015\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0015\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0015\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0015\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0015\n",
      "  Epoch 18/20 — Loss: 0.0015 | F1: 0.5895 | P: 0.6188 | R: 0.5628\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0013\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0014\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0013\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0013\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0013\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0013\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0029\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0027\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0025\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0024\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0023\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0022\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0021\n",
      "  Epoch 19/20 — Loss: 0.0021 | F1: 0.5973 | P: 0.6566 | R: 0.5477\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0015\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0012\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0013\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0012\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0012\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0012\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0011\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0012\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0011\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0012\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0012\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0012\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0012\n",
      "  Epoch 20/20 — Loss: 0.0012 | F1: 0.5989 | P: 0.6606 | R: 0.5477\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6101\n",
      "  Dev F1 @ t=0.61 (optimised): 0.6133\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9562    0.9678    0.9620      1895\n",
      "         PCL     0.6534    0.5779    0.6133       199\n",
      "\n",
      "    accuracy                         0.9308      2094\n",
      "   macro avg     0.8048    0.7728    0.7877      2094\n",
      "weighted avg     0.9274    0.9308    0.9288      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Config B: Weighted CE + Threshold Optimisation + Multi-task Learning\n",
    "metrics_b, thresh_metrics_b, history_b, thresh_b, tok_b = train_model(\n",
    "    config_name='config_B_weighted_ce_thresh_mt_large',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=PCLMultiTaskModel, model_name='roberta-large',\n",
    "    use_weighted_ce=True, use_multitask=True, use_threshold_opt=True,\n",
    "    num_epochs=20, lr=1e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: ablation_no_multitask\n",
      "  Model: roberta-base\n",
      "  Weighted CE: True | Multi-task: False\n",
      "  Threshold Opt: True\n",
      "  Epochs: 10 | LR: 2e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 404.35it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CE class weights: [1.000, 9.548]\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.7491\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.7253\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.6910\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.6616\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.6406\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.6140\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.5951\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.5708\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.5560\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.5431\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.5323\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.5175\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.5115\n",
      "  Epoch 1/10 — Loss: 0.5129 | F1: 0.4059 | P: 0.7639 | R: 0.2764\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.3215\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.3140\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.3454\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.3430\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.3487\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.3421\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.3452\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.3474\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.3380\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.3362\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.3325\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.3324\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.3294\n",
      "  Epoch 2/10 — Loss: 0.3283 | F1: 0.5950 | P: 0.5462 | R: 0.6533\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.1452\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.1830\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.2096\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.2147\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.2215\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.2232\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.2266\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.2275\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.2210\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.2133\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.2230\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.2211\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.2157\n",
      "  Epoch 3/10 — Loss: 0.2150 | F1: 0.5426 | P: 0.7288 | R: 0.4322\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.1532\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.1433\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.1295\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.1321\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.1512\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.1451\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.1420\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.1382\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1315\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.1268\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.1247\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1323\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1340\n",
      "  Epoch 4/10 — Loss: 0.1346 | F1: 0.5760 | P: 0.4784 | R: 0.7236\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.1295\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0857\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0708\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0776\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0725\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0807\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0729\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0735\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0737\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0682\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0723\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0704\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0682\n",
      "  Epoch 5/10 — Loss: 0.0699 | F1: 0.5727 | P: 0.5299 | R: 0.6231\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0141\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0513\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0690\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0733\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0718\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0676\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0612\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0591\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0539\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0515\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0508\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0479\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0489\n",
      "  Epoch 6/10 — Loss: 0.0489 | F1: 0.5730 | P: 0.6341 | R: 0.5226\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0332\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0300\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0209\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0186\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0230\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0203\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0180\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0194\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0174\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0163\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0149\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0140\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0163\n",
      "  Epoch 7/10 — Loss: 0.0164 | F1: 0.5817 | P: 0.6481 | R: 0.5276\n",
      "  Early stopping at epoch 7\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.5950\n",
      "  Dev F1 @ t=0.51 (optimised): 0.6005\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9629    0.9451    0.9539      1895\n",
      "         PCL     0.5556    0.6533    0.6005       199\n",
      "\n",
      "    accuracy                         0.9174      2094\n",
      "   macro avg     0.7592    0.7992    0.7772      2094\n",
      "weighted avg     0.9242    0.9174    0.9203      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ablation 1: Config A without multi-task (isolate multi-task contribution)\n",
    "metrics_abl_nomt, thresh_metrics_abl_nomt, history_abl_nomt, thresh_abl_nomt, tok_abl_nomt = train_model(\n",
    "    config_name='ablation_no_multitask',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=BaselineModel, model_name='roberta-base',\n",
    "    use_weighted_ce=True, use_multitask=False, use_threshold_opt=True,\n",
    "    lr=2e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: ablation_no_thresh\n",
      "  Model: roberta-base\n",
      "  Weighted CE: True | Multi-task: True\n",
      "  Threshold Opt: False\n",
      "  Epochs: 10 | LR: 2e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1562.07it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CE class weights: [1.000, 9.548]\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.9684\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.9358\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.8940\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.8625\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.8471\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.8118\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.7679\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.7338\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.7013\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.6762\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.6599\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.6468\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.6261\n",
      "  Epoch 1/10 — Loss: 0.6242 | F1: 0.5333 | P: 0.4555 | R: 0.6432\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.3855\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.3809\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.3570\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.3901\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.3925\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.3939\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.4065\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.4067\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.3946\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.3955\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.3924\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.3894\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.3938\n",
      "  Epoch 2/10 — Loss: 0.3946 | F1: 0.5685 | P: 0.5744 | R: 0.5628\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.2368\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.2753\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.2629\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.2550\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.2563\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.2481\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.2528\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.2504\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.2499\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.2484\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.2502\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.2531\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.2505\n",
      "  Epoch 3/10 — Loss: 0.2504 | F1: 0.5481 | P: 0.4290 | R: 0.7588\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.1311\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.1088\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.1329\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.1246\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.1421\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.1384\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.1638\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.1663\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1639\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.1618\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.1672\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1698\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1741\n",
      "  Epoch 4/10 — Loss: 0.1736 | F1: 0.5539 | P: 0.6597 | R: 0.4774\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0466\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.1185\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.1077\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.1289\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.1222\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.1193\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.1108\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.1177\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1153\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.1087\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.1106\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1226\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1181\n",
      "  Epoch 5/10 — Loss: 0.1182 | F1: 0.5538 | P: 0.7143 | R: 0.4523\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0325\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0321\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0344\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0313\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0369\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0383\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0368\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0399\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0456\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0429\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0409\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0435\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0433\n",
      "  Epoch 6/10 — Loss: 0.0431 | F1: 0.5494 | P: 0.7120 | R: 0.4472\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0416\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0359\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0287\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0245\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0237\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0239\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0280\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0308\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0329\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0308\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0342\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0324\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0341\n",
      "  Epoch 7/10 — Loss: 0.0340 | F1: 0.5980 | P: 0.5980 | R: 0.5980\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0116\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0113\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0166\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0227\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0217\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0202\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0207\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0217\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0257\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0265\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0250\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0241\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0235\n",
      "  Epoch 8/10 — Loss: 0.0235 | F1: 0.6061 | P: 0.6091 | R: 0.6030\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0273\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0187\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0162\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0145\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0137\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0136\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0157\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0169\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0175\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0193\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0185\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0178\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0186\n",
      "  Epoch 9/10 — Loss: 0.0185 | F1: 0.5954 | P: 0.6031 | R: 0.5879\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0082\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0091\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0107\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0161\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0151\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0145\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0137\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0130\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0128\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0124\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0122\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0119\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0117\n",
      "  Epoch 10/10 — Loss: 0.0117 | F1: 0.5920 | P: 0.6307 | R: 0.5578\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9584    0.9594    0.9589      1895\n",
      "         PCL     0.6091    0.6030    0.6061       199\n",
      "\n",
      "    accuracy                         0.9255      2094\n",
      "   macro avg     0.7837    0.7812    0.7825      2094\n",
      "weighted avg     0.9252    0.9255    0.9253      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ablation 2: Config A without threshold opt (isolate threshold contribution)\n",
    "metrics_abl_nothresh, thresh_metrics_abl_nothresh, history_abl_nothresh, thresh_abl_nothresh, tok_abl_nothresh = train_model(\n",
    "    config_name='ablation_no_thresh',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=PCLMultiTaskModel, model_name='roberta-base',\n",
    "    use_weighted_ce=True, use_multitask=True, use_threshold_opt=False,\n",
    "    lr=2e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Config: ablation_no_weighted_ce\n",
      "  Model: roberta-base\n",
      "  Weighted CE: False | Multi-task: True\n",
      "  Threshold Opt: True\n",
      "  Epochs: 10 | LR: 2e-05 | Patience: 5\n",
      "  Batch: 4 x 8 = 32 effective\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1505.61it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unweighted CE\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.7331\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.7018\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.6422\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.6121\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.5829\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.5466\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.5122\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.4821\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.4617\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.4428\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.4331\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.4213\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.4118\n",
      "  Epoch 1/10 — Loss: 0.4109 | F1: 0.4836 | P: 0.5956 | R: 0.4070\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.2089\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.2328\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.2336\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.2243\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.2275\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.2288\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.2306\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.2321\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.2360\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.2340\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.2340\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.2348\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.2349\n",
      "  Epoch 2/10 — Loss: 0.2353 | F1: 0.5296 | P: 0.6967 | R: 0.4271\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.1520\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.1621\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.1669\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.1727\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.1715\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.1718\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.1706\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.1702\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.1733\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.1661\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.1634\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.1655\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.1623\n",
      "  Epoch 3/10 — Loss: 0.1634 | F1: 0.5803 | P: 0.5989 | R: 0.5628\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0825\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0705\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0997\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0966\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0950\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0931\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0955\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0968\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0944\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0945\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0926\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0922\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0896\n",
      "  Epoch 4/10 — Loss: 0.0900 | F1: 0.5853 | P: 0.4763 | R: 0.7588\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.1091\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0768\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0628\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0571\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0603\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0618\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0579\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0573\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0577\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0564\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0533\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0531\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0540\n",
      "  Epoch 5/10 — Loss: 0.0539 | F1: 0.5876 | P: 0.6337 | R: 0.5477\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0412\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0436\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0473\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0467\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0414\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0404\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0379\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0398\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0391\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0368\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0360\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0354\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0343\n",
      "  Epoch 6/10 — Loss: 0.0342 | F1: 0.6169 | P: 0.6108 | R: 0.6231\n",
      "  -> New best F1! Saved.\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0179\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0236\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0275\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0277\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0248\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0249\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0241\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0226\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0254\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0258\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0264\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0271\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0261\n",
      "  Epoch 7/10 — Loss: 0.0260 | F1: 0.5886 | P: 0.7313 | R: 0.4925\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0315\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0294\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0245\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0211\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0191\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0188\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0192\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0182\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0178\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0169\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0173\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0173\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0170\n",
      "  Epoch 8/10 — Loss: 0.0170 | F1: 0.5780 | P: 0.6803 | R: 0.5025\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0096\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0091\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0110\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0134\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0147\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0138\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0158\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0166\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0162\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0154\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0153\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0149\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0147\n",
      "  Epoch 9/10 — Loss: 0.0146 | F1: 0.5856 | P: 0.6503 | R: 0.5327\n",
      "    step 160/2094 (update 20) | avg loss so far: 0.0081\n",
      "    step 320/2094 (update 40) | avg loss so far: 0.0077\n",
      "    step 480/2094 (update 60) | avg loss so far: 0.0082\n",
      "    step 640/2094 (update 80) | avg loss so far: 0.0093\n",
      "    step 800/2094 (update 100) | avg loss so far: 0.0090\n",
      "    step 960/2094 (update 120) | avg loss so far: 0.0092\n",
      "    step 1120/2094 (update 140) | avg loss so far: 0.0096\n",
      "    step 1280/2094 (update 160) | avg loss so far: 0.0097\n",
      "    step 1440/2094 (update 180) | avg loss so far: 0.0114\n",
      "    step 1600/2094 (update 200) | avg loss so far: 0.0116\n",
      "    step 1760/2094 (update 220) | avg loss so far: 0.0126\n",
      "    step 1920/2094 (update 240) | avg loss so far: 0.0125\n",
      "    step 2080/2094 (update 260) | avg loss so far: 0.0122\n",
      "  Epoch 10/10 — Loss: 0.0122 | F1: 0.5926 | P: 0.6257 | R: 0.5628\n",
      "\n",
      "  Dev F1 @ t=0.50: 0.6169\n",
      "  Dev F1 @ t=0.23 (optimised): 0.6210\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      No PCL     0.9618    0.9562    0.9590      1895\n",
      "         PCL     0.6048    0.6382    0.6210       199\n",
      "\n",
      "    accuracy                         0.9260      2094\n",
      "   macro avg     0.7833    0.7972    0.7900      2094\n",
      "weighted avg     0.9279    0.9260    0.9269      2094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ablation 3: Config A without weighted CE (isolate weighted CE contribution)\n",
    "metrics_abl_nowe, thresh_metrics_abl_nowe, history_abl_nowe, thresh_abl_nowe, tok_abl_nowe = train_model(\n",
    "    config_name='ablation_no_weighted_ce',\n",
    "    train_df=train_df, dev_df=dev_df, tokenizer=tokenizer,\n",
    "    model_class=PCLMultiTaskModel, model_name='roberta-base',\n",
    "    use_weighted_ce=False, use_multitask=True, use_threshold_opt=True,\n",
    "    lr=2e-5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Comparison & Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS COMPARISON (all models evaluated on dev set)\n",
      "================================================================================\n",
      "                                    Config Threshold     F1  Precision  Recall\n",
      "           Baseline (unweighted CE, t=0.5)      0.50 0.6139     0.5872  0.6432\n",
      "A: + Weighted CE + Thresh Opt + Multi-task      0.45 0.6373     0.6220  0.6533\n",
      "B: + Weighted CE + Thresh Opt + Multi-task      0.61 0.6133     0.6534  0.5779\n",
      "                Ablation: A w/o Multi-task      0.51 0.6005     0.5556  0.6533\n",
      "             Ablation: A w/o Threshold Opt      0.50 0.6061     0.6091  0.6030\n",
      "               Ablation: A w/o Weighted CE      0.23 0.6210     0.6048  0.6382\n",
      "\n",
      "** Best model: Config A (F1=0.6373 @ t=0.45) **\n",
      "   Improvement over baseline: +0.0233 F1\n"
     ]
    }
   ],
   "source": [
    "# ---- Results comparison table ----\n",
    "results = pd.DataFrame([\n",
    "    {\n",
    "        'Config': 'Baseline (unweighted CE, t=0.5)',\n",
    "        'Threshold': '0.50',\n",
    "        'F1': thresh_metrics_bl['f1'],\n",
    "        'Precision': thresh_metrics_bl['precision'],\n",
    "        'Recall': thresh_metrics_bl['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'A: + Weighted CE + Thresh Opt + Multi-task',\n",
    "        'Threshold': f'{thresh_a:.2f}',\n",
    "        'F1': thresh_metrics_a['f1'],\n",
    "        'Precision': thresh_metrics_a['precision'],\n",
    "        'Recall': thresh_metrics_a['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'B: + Weighted CE + Thresh Opt + Multi-task',\n",
    "        'Threshold': f'{thresh_b:.2f}',\n",
    "        'F1': thresh_metrics_b['f1'],\n",
    "        'Precision': thresh_metrics_b['precision'],\n",
    "        'Recall': thresh_metrics_b['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'Ablation: A w/o Multi-task',\n",
    "        'Threshold': f'{thresh_abl_nomt:.2f}',\n",
    "        'F1': thresh_metrics_abl_nomt['f1'],\n",
    "        'Precision': thresh_metrics_abl_nomt['precision'],\n",
    "        'Recall': thresh_metrics_abl_nomt['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'Ablation: A w/o Threshold Opt',\n",
    "        'Threshold': '0.50',\n",
    "        'F1': thresh_metrics_abl_nothresh['f1'],\n",
    "        'Precision': thresh_metrics_abl_nothresh['precision'],\n",
    "        'Recall': thresh_metrics_abl_nothresh['recall'],\n",
    "    },\n",
    "    {\n",
    "        'Config': 'Ablation: A w/o Weighted CE',\n",
    "        'Threshold': f'{thresh_abl_nowe:.2f}',\n",
    "        'F1': thresh_metrics_abl_nowe['f1'],\n",
    "        'Precision': thresh_metrics_abl_nowe['precision'],\n",
    "        'Recall': thresh_metrics_abl_nowe['recall'],\n",
    "    },\n",
    "])\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('RESULTS COMPARISON (all models evaluated on dev set)')\n",
    "print('='*80)\n",
    "print(results.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "# Best model is Config A\n",
    "best_metrics = thresh_metrics_a\n",
    "best_threshold = thresh_a\n",
    "best_tok = tok_a\n",
    "best_ckpt_name = 'config_A_weighted_ce_thresh_mt'\n",
    "best_model_class = PCLMultiTaskModel\n",
    "best_model_name = 'roberta-base'\n",
    "best_key = 'A'\n",
    "\n",
    "improvement = best_metrics['f1'] - thresh_metrics_bl['f1']\n",
    "print(f'\\n** Best model: Config A (F1={best_metrics[\"f1\"]:.4f} @ t={best_threshold:.2f}) **')\n",
    "print(f'   Improvement over baseline: +{improvement:.4f} F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate dev.txt and test.txt Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev predictions saved to /vol/bitbucket/akc123/PCL_Detection/dev.txt\n",
      "  2094 predictions, 209 predicted PCL\n",
      "\n",
      "Test set: 3832 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1408.53it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to /vol/bitbucket/akc123/PCL_Detection/test.txt\n",
      "  3832 predictions, 336 predicted PCL\n",
      "  Using threshold: 0.45\n"
     ]
    }
   ],
   "source": [
    "# ---- Dev predictions ----\n",
    "dev_preds = best_metrics['preds']\n",
    "dev_pred_path = f'{BASE_DIR}/dev.txt'\n",
    "with open(dev_pred_path, 'w') as f:\n",
    "    for p in dev_preds:\n",
    "        f.write(f'{p}\\n')\n",
    "print(f'Dev predictions saved to {dev_pred_path}')\n",
    "print(f'  {len(dev_preds)} predictions, {sum(dev_preds)} predicted PCL')\n",
    "\n",
    "# ---- Test predictions ----\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/task4_test.tsv', sep='\\t', header=None,\n",
    "                       names=['par_id', 'art_id', 'keyword', 'country_code', 'text'])\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "print(f'\\nTest set: {len(test_df)} samples')\n",
    "\n",
    "test_dataset = PCLDataset(\n",
    "    texts=test_df['text'].tolist(),\n",
    "    binary_labels=[0] * len(test_df),\n",
    "    category_labels=[[0]*7] * len(test_df),\n",
    "    tokenizer=best_tok, max_length=MAX_LENGTH\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Reload best model from checkpoint\n",
    "best_model = PCLMultiTaskModel(model_name=best_model_name).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/{best_ckpt_name}_best.pt', weights_only=True, map_location=DEVICE))\n",
    "best_model.eval()\n",
    "\n",
    "test_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        binary_logits, _ = best_model(input_ids, attention_mask)\n",
    "        probs = F.softmax(binary_logits, dim=1)[:, 1].cpu().tolist()\n",
    "        test_probs.extend(probs)\n",
    "\n",
    "del best_model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "test_preds = [1 if p >= best_threshold else 0 for p in test_probs]\n",
    "\n",
    "test_pred_path = f'{BASE_DIR}/test.txt'\n",
    "with open(test_pred_path, 'w') as f:\n",
    "    for p in test_preds:\n",
    "        f.write(f'{p}\\n')\n",
    "print(f'Test predictions saved to {test_pred_path}')\n",
    "print(f'  {len(test_preds)} predictions, {sum(test_preds)} predicted PCL')\n",
    "print(f'  Using threshold: {best_threshold:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1394.45it/s, Materializing param=encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ERROR ANALYSIS: Baseline vs Best Model on Dev Set\n",
      "======================================================================\n",
      "\n",
      "1. PREDICTION AGREEMENT QUADRANTS (n=2094)\n",
      "   a) Both correct:                     1894 (90.4%)\n",
      "   b) Both wrong:                        109 (5.2%)\n",
      "   c) Baseline correct, best wrong:       39 (1.9%)\n",
      "   d) Baseline wrong, best correct:       52 (2.5%)\n",
      "\n",
      "2. CONFUSION MATRIX COMPARISON\n",
      "   Metric                      Baseline   Best Model\n",
      "   --------------------------------------------------\n",
      "   True Positives                   128          130\n",
      "   True Negatives                  1805         1816\n",
      "   False Positives                   90           79\n",
      "   False Negatives                   71           69\n",
      "\n",
      "======================================================================\n",
      "3. BOTH MODELS CORRECT — \"easy\" examples (1894 total)\n",
      "======================================================================\n",
      "   Both correctly identified as PCL:     118\n",
      "   Both correctly identified as non-PCL: 1776\n",
      "\n",
      "   --- PCL correctly caught by BOTH models (5/118 shown) ---\n",
      "   (These show what \"obvious\" PCL looks like to the models)\n",
      "   [3] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"disabled\"\n",
      "         \"When some people feel causing problem for some others by breaking into their homes to steal is n't too good , they just result to begging . You now see people without deformities begging , when some p\"\n",
      "\n",
      "   [5] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"in-need\"\n",
      "         \"\"\"\" We share a global responsibility to respond to this crisis . We commend others who have acted in a compassionate and generous way , especially the Government of Bangladesh and host communities in \"\n",
      "\n",
      "   [6] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"in-need\"\n",
      "         \"The former Chelsea star through his foundation gave out toys , bags and clothes to kids in need of a brighter holiday\"\n",
      "\n",
      "   [7] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"It can not be right to allow homes to sit empty while many struggle to find somewhere to live , others having to sleep rough on pavements during Christmas , hoping against hope , for some charity to p\"\n",
      "\n",
      "   [10] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"migrant\"\n",
      "         \"18 December should serve as a time when we look with compassion at the fate of migrants , refugees and the internally displaced . It is especially a time when we must plan and increase resources for c\"\n",
      "\n",
      "   Confidence on correctly detected PCL:\n",
      "     Baseline:   mean=0.993  median=1.000\n",
      "     Best model: mean=0.987  median=1.000\n",
      "   Confidence on correctly rejected non-PCL:\n",
      "     Baseline:   mean=0.002  median=0.000\n",
      "     Best model: mean=0.001  median=0.000\n",
      "\n",
      "======================================================================\n",
      "4. BOTH MODELS WRONG — hardest examples (109 total)\n",
      "======================================================================\n",
      "   Shared false negatives (PCL missed by both): 59\n",
      "   Shared false positives (non-PCL flagged by both): 50\n",
      "\n",
      "   --- Shared FN: PCL that NEITHER model detects (8/59 shown) ---\n",
      "   [0] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"\"His present \"\" chambers \"\" may be quite humble , but Shiyani has the tiny space very neatly organized and clean . Many people pass him by but do not manage to see him , because the space is partially\"\n",
      "\n",
      "   [1] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"disabled\"\n",
      "         \"Krueger recently harnessed that creativity to self-publish a book featuring the poems , artwork , photography and short stories of 16 ill or disabled artists from around the world . She hopes the book\"\n",
      "\n",
      "   [2] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"poor-families\"\n",
      "         \"10:41am - Parents of children who died must get compensation , free medicine must be provided to poor families across UP : Ram Gopal Yadav\"\n",
      "\n",
      "   [9] (bl_prob=0.002, best_prob=0.014)\n",
      "         keyword=\"refugee\"\n",
      "         \"He depicts demonstrations by refugees at the border post , their catastrophic living conditions and the desperate attempt of several hundred to cross a river a few kilometres from the camp to get into\"\n",
      "\n",
      "   [12] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"It 's calculated that over 204,000 days of purpose-built residential accommodation to otherwise potentially homeless elderly men and women have been delivered by this personally driven altruistic act \"\n",
      "\n",
      "   [13] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"disabled\"\n",
      "         \"\"\"\" I and my daughter Monica are excited about providing a space for disabled people to be able to get together and earn fair prices for their work , \"\" Mr. Rogers said .\"\"\n",
      "\n",
      "   [16] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"immigrant\"\n",
      "         \"Sheepherding in America has always been an immigrant 's job , too dirty , too cold and too lonely for anyone with options .\"\n",
      "\n",
      "   [20] (bl_prob=0.000, best_prob=0.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"Bombarded by schizophrenia , addiction and homelessness , you might say that Eoghan O'Driscoll has been to hell and back . But he is finding a new balance through painting . Interview : Michael Laniga\"\n",
      "\n",
      "   --- Shared FP: non-PCL flagged by BOTH models (5/50 shown) ---\n",
      "   [188] (bl_prob=0.977, best_prob=0.995)\n",
      "         keyword=\"disabled\"\n",
      "         \"A crew of disabled athletes will be tackling this weekend 's Chattanooga Waterfront Triathlon to show others with disabilities they , too , can participate in a healthy , active lifestyle .\"\n",
      "\n",
      "   [254] (bl_prob=1.000, best_prob=0.999)\n",
      "         keyword=\"hopeless\"\n",
      "         \"\"\"\" So we do need to heal ourselves as an Aboriginal Torres Strait islander community , but also as a nation . \"\" A life of hope , not hopelessness\"\"\n",
      "\n",
      "   [343] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"refugee\"\n",
      "         \"\"\"\" The fact is that ruthless smugglers are preying on desperate migrants and refugees with no regard at all for human dignity , \"\" said Michele Sison , the US Deputy Ambassador to the UN .\"\"\n",
      "\n",
      "   [379] (bl_prob=1.000, best_prob=1.000)\n",
      "         keyword=\"poor-families\"\n",
      "         \"\"\"\" We want all poor families to earn enough and save enough . The rural savings banks were meant to encourage that . \"\"\"\"\n",
      "\n",
      "   [388] (bl_prob=0.999, best_prob=0.999)\n",
      "         keyword=\"in-need\"\n",
      "         \"\"\"\" Your personal leadership has been critical to addressing the plight of the Rohingya who fled to safety in your country . I thank you for all you have done to assist these men , women and children \"\n",
      "\n",
      "======================================================================\n",
      "5. REGRESSIONS — baseline correct, best model wrong (39 total)\n",
      "======================================================================\n",
      "   New false negatives (best misses PCL baseline caught): 10\n",
      "   New false positives (best flags non-PCL baseline ignored): 29\n",
      "\n",
      "   --- New FN: PCL missed by best but not baseline (5/10 shown) ---\n",
      "   [8] (bl_prob=1.000, best_prob=0.000)\n",
      "         keyword=\"women\"\n",
      "         \"\"\"\" People do n't understand the hurt , people do n't understand the pain . I 've read about women with their children sleeping in cars , sleeping in hotel rooms and it 's criminal . If they 're lucky\"\n",
      "\n",
      "   [19] (bl_prob=0.812, best_prob=0.206)\n",
      "         keyword=\"women\"\n",
      "         \"\"She continued , \"\" I stepped away from hiding behind a fabricated version of myself . I no longer put actions behind my fears and insecurities . I made a choice to redirect my energy to be a catalyst\"\n",
      "\n",
      "   [46] (bl_prob=0.978, best_prob=0.002)\n",
      "         keyword=\"refugee\"\n",
      "         \"\"\"\" It 's the largest humanitarian tragedy of our time , \"\" Ninette Kelley , the UN high commissioner for refugees ' representative to Lebanon , told the Guardian earlier this month in an interview . \"\n",
      "\n",
      "   [96] (bl_prob=0.940, best_prob=0.000)\n",
      "         keyword=\"vulnerable\"\n",
      "         \"\"He said that he also stopped in Maiduguri , Nigeria , where he met with people who were torched out of their homes 11 months ago . \"\" These are the stories we carry with us , \"\" he underlined . \"\" Th\"\n",
      "\n",
      "   [106] (bl_prob=0.984, best_prob=0.000)\n",
      "         keyword=\"homeless\"\n",
      "         \"He said the victims who are currently rendered homeless can now be relieved of troubles as the 5,000 iron sheets from Mwanza had arrived , with 1,200 already distributed to victims in Bukoba Municipal\"\n",
      "\n",
      "\n",
      "   --- New FP: non-PCL flagged by best but not baseline (5/29 shown) ---\n",
      "   [169] (bl_prob=0.077, best_prob=1.000)\n",
      "         keyword=\"poor-families\"\n",
      "         \"Marcos said the government should help poor families that try every possible means to survive . With Joel Zurbano More from this Category :\"\n",
      "\n",
      "   [198] (bl_prob=0.000, best_prob=0.993)\n",
      "         keyword=\"disabled\"\n",
      "         \"\"Guided by the philosophy of \"\" Building Communities \"\" , Swire Properties established its Community Ambassador Programme in 2001 . The programme seeks to help the elderly , the disabled , children an\"\n",
      "\n",
      "   [228] (bl_prob=0.001, best_prob=0.687)\n",
      "         keyword=\"in-need\"\n",
      "         \"\"\"\" This incident will not tear us down but rather strengthen us as an organization . We will continue our mission of helping Veterans in need . It is through your generous donations and the volunteer\"\n",
      "\n",
      "   [287] (bl_prob=0.001, best_prob=0.997)\n",
      "         keyword=\"women\"\n",
      "         \"\"She added : \"\" I would also like to carefully point out that the issue was not her religious beliefs , but rather it is about choosing to treat men and women differently by shaking the hands of women\"\n",
      "\n",
      "   [326] (bl_prob=0.297, best_prob=0.908)\n",
      "         keyword=\"homeless\"\n",
      "         \"As leaders , we will personally support victims but we ask the government to also help . We are going to clear all victims ' hospital bills and we want to ensure that those left homeless receive shelt\"\n",
      "\n",
      "======================================================================\n",
      "6. IMPROVEMENTS — best model correct, baseline wrong (52 total)\n",
      "======================================================================\n",
      "   Fixed false negatives (best catches PCL baseline missed): 12\n",
      "   Fixed false positives (best correctly ignores non-PCL): 40\n",
      "\n",
      "   --- Fixed FN: PCL caught by best but not baseline (5/12 shown) ---\n",
      "   [4] (bl_prob=0.004, best_prob=0.999)\n",
      "         keyword=\"poor-families\"\n",
      "         \"We are alarmed to learn of your recently circulated proposals that would eviscerate the Lifeline program and leave many of the most vulnerable people in the country without access to affordable commun\"\n",
      "\n",
      "   [18] (bl_prob=0.005, best_prob=0.999)\n",
      "         keyword=\"in-need\"\n",
      "         \"\"He said : \"\" We need improved security for civilians and aid workers and access to all those in need , but we must also build a bigger humanitarian muscle to provide for the suffering millions .\"\"\n",
      "\n",
      "   [45] (bl_prob=0.106, best_prob=0.896)\n",
      "         keyword=\"homeless\"\n",
      "         \"The drama 's compelling nature derived from a thorough knowledge of its cast , more numerous and improbable than any soap opera 's . The bite came from underlying tragedy : lost wives , lost jobs , pe\"\n",
      "\n",
      "   [73] (bl_prob=0.280, best_prob=0.999)\n",
      "         keyword=\"disabled\"\n",
      "         \"\"\"\" In particular , the programmes to support blind and disabled golf impress me both as an avid golfer and a passionate believer in the \"\" power of sport \"\" , to bring people together and transform l\"\n",
      "\n",
      "   [77] (bl_prob=0.002, best_prob=0.606)\n",
      "         keyword=\"hopeless\"\n",
      "         \"President Muhammadu Buhari is on a divine mission to rescue the Nigeria from the brink of collapse and hopelessness , Speaker of the House of Representatives , Rt . Hon. Yakubu Dogara has said .\"\n",
      "\n",
      "\n",
      "   --- Fixed FP: non-PCL correctly ignored by best but not baseline (5/40 shown) ---\n",
      "   [252] (bl_prob=0.999, best_prob=0.000)\n",
      "         keyword=\"women\"\n",
      "         \"So , let the NPP government appointees look down upon the hardworking and selfless men and women at their own peril .\"\n",
      "\n",
      "   [257] (bl_prob=0.915, best_prob=0.000)\n",
      "         keyword=\"women\"\n",
      "         \"Mary Kay is one of the world 's largest direct selling companies with more than $3 billion in annual wholesale sales worldwide . Mary Kay was founded in 1963 by Mary Kay Ash with the goal of helping w\"\n",
      "\n",
      "   [356] (bl_prob=1.000, best_prob=0.001)\n",
      "         keyword=\"homeless\"\n",
      "         \"His friends at the Chevron want people to know he was n't just a faceless homeless person . He was their friend and their family .\"\n",
      "\n",
      "   [374] (bl_prob=0.879, best_prob=0.000)\n",
      "         keyword=\"vulnerable\"\n",
      "         \"Governor , State of Osun , Ogbeni Rauf Aregbesola has signed an Executive Bill focusing on key areas of Youth , Women and other areas of addressing issues concerning the welfare of the vulnerable and \"\n",
      "\n",
      "   [459] (bl_prob=0.938, best_prob=0.024)\n",
      "         keyword=\"poor-families\"\n",
      "         \"\"She recalled being so proud of being part of a small group of students involved in \"\" The Goat Project \"\" where students raised money to help poor families in Africa .\"\"\n",
      "\n",
      "======================================================================\n",
      "7. KEYWORD / TOPIC ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "   Best model performance by keyword (sorted by PCL count):\n",
      "      Keyword  Total  PCL  TP  FP  FN  Recall  Precision\n",
      "poor-families    190   38  25  14  13    0.66       0.64\n",
      "      in-need    226   33  31  16   2    0.94       0.66\n",
      "     homeless    212   29  20  15   9    0.69       0.57\n",
      "     hopeless    217   26  17   9   9    0.65       0.65\n",
      "   vulnerable    209   20  12   6   8    0.60       0.67\n",
      "     disabled    194   14   7   4   7    0.50       0.64\n",
      "        women    233   14   5   3   9    0.36       0.62\n",
      "      refugee    188   13   8  10   5    0.62       0.44\n",
      "    immigrant    218    7   3   1   4    0.43       0.75\n",
      "      migrant    207    5   2   1   3    0.40       0.67\n",
      "\n",
      "   Keywords with lowest recall (most missed PCL):\n",
      "     \"women\": recall=0.36 (5/14 caught, 9 missed)\n",
      "     \"migrant\": recall=0.40 (2/5 caught, 3 missed)\n",
      "     \"immigrant\": recall=0.43 (3/7 caught, 4 missed)\n",
      "     \"disabled\": recall=0.50 (7/14 caught, 7 missed)\n",
      "     \"vulnerable\": recall=0.60 (12/20 caught, 8 missed)\n",
      "\n",
      "   Keywords with most false positives:\n",
      "     \"in-need\": 16 FP out of 226 samples\n",
      "     \"homeless\": 15 FP out of 212 samples\n",
      "     \"poor-families\": 14 FP out of 190 samples\n",
      "     \"refugee\": 10 FP out of 188 samples\n",
      "     \"hopeless\": 9 FP out of 217 samples\n",
      "\n",
      "======================================================================\n",
      "8. PCL CATEGORY ANALYSIS (best model errors)\n",
      "======================================================================\n",
      "\n",
      "   PCL samples: 199 total, 130 caught (TP), 69 missed (FN)\n",
      "\n",
      "   Category                         Caught   Missed   Recall\n",
      "   --------------------------------------------------------\n",
      "   Unbalanced power relations          102       40     0.72\n",
      "   Shallow solution                     24       12     0.67\n",
      "   Presupposition                       37       25     0.60\n",
      "   Authority voice                      25       13     0.66\n",
      "   Metaphor                             39       13     0.75\n",
      "   Compassion                           72       34     0.68\n",
      "   The poorer the merrier                7        4     0.64\n",
      "\n",
      "======================================================================\n",
      "9. CONFIDENCE ANALYSIS\n",
      "======================================================================\n",
      "   True Positives       n= 130  mean_prob=0.972  median=1.000  std=0.092\n",
      "   True Negatives       n=1816  mean_prob=0.002  median=0.000  std=0.024\n",
      "   False Positives      n=  79  mean_prob=0.956  median=0.997  std=0.089\n",
      "   False Negatives      n=  69  mean_prob=0.006  median=0.000  std=0.028\n",
      "\n",
      "   False negatives are PCL samples where the model assigned LOW probability.\n",
      "   If FN mean_prob is close to the threshold, the model is uncertain — threshold tuning may help.\n",
      "   If FN mean_prob is very low, the model genuinely cannot recognise these as PCL.\n",
      "\n",
      "======================================================================\n",
      "10. TEXT LENGTH ANALYSIS\n",
      "======================================================================\n",
      "   All samples          n=2094  mean=47.4  median=41.0  max=272\n",
      "   True Positives       n= 130  mean=53.3  median=45.0  max=165\n",
      "   False Positives      n=  79  mean=49.2  median=41.0  max=144\n",
      "   False Negatives      n=  69  mean=55.3  median=51.0  max=175\n",
      "   True Negatives       n=1816  mean=46.6  median=41.0  max=272\n",
      "\n",
      "======================================================================\n",
      "11. LEXICAL PATTERNS: words over-represented in false negatives\n",
      "======================================================================\n",
      "\n",
      "   Words more frequent in false negatives than true positives:\n",
      "   Word                   FN count    FN rate    TP rate    Ratio\n",
      "   ------------------------------------------------------------\n",
      "   women                        18     0.0047     0.0027      1.7x\n",
      "   n't                          10     0.0026     0.0010      2.6x\n",
      "   see                           8     0.0021     0.0009      2.4x\n",
      "   year                          7     0.0018     0.0009      2.1x\n",
      "   conditions                    6     0.0016     0.0001     10.9x\n",
      "   health                        5     0.0013     0.0006      2.3x\n",
      "   often                         5     0.0013     0.0006      2.3x\n",
      "   focus                         5     0.0013     0.0003      4.5x\n",
      "   among                         5     0.0013     0.0003      4.5x\n",
      "   schools                       5     0.0013     0.0001      9.1x\n",
      "   hopelessness                  5     0.0013     0.0007      1.8x\n",
      "   must                          4     0.0010     0.0003      3.6x\n",
      "   understand                    4     0.0010     0.0001      7.3x\n",
      "   2016                          4     0.0010     0.0001      7.3x\n",
      "   alone                         4     0.0010     0.0001      7.3x\n",
      "\n",
      "   Interpretation: these words appear in PCL text the model fails to detect.\n",
      "   High-ratio words suggest the model may associate them with non-PCL contexts.\n"
     ]
    }
   ],
   "source": [
    "# ---- Error analysis: compare baseline vs best model ----\n",
    "from collections import Counter\n",
    "\n",
    "# Reload baseline from checkpoint\n",
    "model_baseline = BaselineModel(model_name=BASELINE_MODEL).to(DEVICE)\n",
    "model_baseline.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/baseline_best.pt', weights_only=True, map_location=DEVICE))\n",
    "\n",
    "baseline_dev_metrics = evaluate(model_baseline,\n",
    "    DataLoader(PCLDataset(dev_df['text'].tolist(), dev_df['binary_label'].tolist(),\n",
    "                          dev_df['category_labels'].tolist(), tok_bl, MAX_LENGTH),\n",
    "               batch_size=EVAL_BATCH_SIZE, shuffle=False),\n",
    "    DEVICE, threshold=0.5)\n",
    "\n",
    "del model_baseline; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "baseline_preds = baseline_dev_metrics['preds']\n",
    "baseline_probs = baseline_dev_metrics['probs']\n",
    "best_preds = best_metrics['preds']\n",
    "best_probs = best_metrics['probs']\n",
    "true_labels = best_metrics['labels']\n",
    "dev_texts = dev_df['text'].tolist()\n",
    "dev_keywords = dev_df['keyword'].tolist()\n",
    "dev_categories = dev_df['category_labels'].tolist()\n",
    "\n",
    "# ============================================================\n",
    "# 1. Four-quadrant classification: baseline vs best model\n",
    "# ============================================================\n",
    "both_correct = []    # a) both correct\n",
    "both_wrong = []      # b) both wrong\n",
    "base_right_best_wrong = []  # c) baseline correct, best wrong\n",
    "base_wrong_best_right = []  # d) baseline wrong, best correct\n",
    "\n",
    "for i, (true, bp, mp) in enumerate(zip(true_labels, baseline_preds, best_preds)):\n",
    "    b_correct = (bp == true)\n",
    "    m_correct = (mp == true)\n",
    "    if b_correct and m_correct:\n",
    "        both_correct.append(i)\n",
    "    elif not b_correct and not m_correct:\n",
    "        both_wrong.append(i)\n",
    "    elif b_correct and not m_correct:\n",
    "        base_right_best_wrong.append(i)\n",
    "    else:\n",
    "        base_wrong_best_right.append(i)\n",
    "\n",
    "print('=' * 70)\n",
    "print('ERROR ANALYSIS: Baseline vs Best Model on Dev Set')\n",
    "print('=' * 70)\n",
    "\n",
    "n = len(true_labels)\n",
    "print(f'\\n1. PREDICTION AGREEMENT QUADRANTS (n={n})')\n",
    "print(f'   a) Both correct:                    {len(both_correct):>5} ({100*len(both_correct)/n:.1f}%)')\n",
    "print(f'   b) Both wrong:                      {len(both_wrong):>5} ({100*len(both_wrong)/n:.1f}%)')\n",
    "print(f'   c) Baseline correct, best wrong:    {len(base_right_best_wrong):>5} ({100*len(base_right_best_wrong)/n:.1f}%)')\n",
    "print(f'   d) Baseline wrong, best correct:    {len(base_wrong_best_right):>5} ({100*len(base_wrong_best_right)/n:.1f}%)')\n",
    "\n",
    "# ============================================================\n",
    "# 2. Breakdown by error type within each quadrant\n",
    "# ============================================================\n",
    "def error_type_breakdown(indices, labels, bl_preds, best_preds):\n",
    "    \"\"\"Classify errors as FP or FN for each model.\"\"\"\n",
    "    stats = {'bl_fp': 0, 'bl_fn': 0, 'best_fp': 0, 'best_fn': 0}\n",
    "    for i in indices:\n",
    "        if labels[i] == 0 and bl_preds[i] == 1: stats['bl_fp'] += 1\n",
    "        if labels[i] == 1 and bl_preds[i] == 0: stats['bl_fn'] += 1\n",
    "        if labels[i] == 0 and best_preds[i] == 1: stats['best_fp'] += 1\n",
    "        if labels[i] == 1 and best_preds[i] == 0: stats['best_fn'] += 1\n",
    "    return stats\n",
    "\n",
    "print(f'\\n2. CONFUSION MATRIX COMPARISON')\n",
    "print(f'   {\"Metric\":<25} {\"Baseline\":>10} {\"Best Model\":>12}')\n",
    "print(f'   {\"-\"*50}')\n",
    "for label_name, true_val, pred_val in [('True Positives', 1, 1), ('True Negatives', 0, 0),\n",
    "                                         ('False Positives', 0, 1), ('False Negatives', 1, 0)]:\n",
    "    bl_count = sum(1 for t, p in zip(true_labels, baseline_preds) if t == true_val and p == pred_val)\n",
    "    best_count = sum(1 for t, p in zip(true_labels, best_preds) if t == true_val and p == pred_val)\n",
    "    print(f'   {label_name:<25} {bl_count:>10} {best_count:>12}')\n",
    "\n",
    "# ============================================================\n",
    "# 3. Quadrant a) deep dive — both models correct (easy cases)\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'3. BOTH MODELS CORRECT — \"easy\" examples ({len(both_correct)} total)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "both_correct_pcl = [i for i in both_correct if true_labels[i] == 1]\n",
    "both_correct_nopcl = [i for i in both_correct if true_labels[i] == 0]\n",
    "print(f'   Both correctly identified as PCL:     {len(both_correct_pcl)}')\n",
    "print(f'   Both correctly identified as non-PCL: {len(both_correct_nopcl)}')\n",
    "\n",
    "print(f'\\n   --- PCL correctly caught by BOTH models ({min(5, len(both_correct_pcl))}/{len(both_correct_pcl)} shown) ---')\n",
    "print(f'   (These show what \"obvious\" PCL looks like to the models)')\n",
    "for idx in both_correct_pcl[:5]:\n",
    "    print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "    print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "    print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "    print()\n",
    "\n",
    "# Confidence comparison: are both models equally confident on easy cases?\n",
    "if both_correct_pcl:\n",
    "    bl_probs_easy_pcl = [baseline_probs[i] for i in both_correct_pcl]\n",
    "    best_probs_easy_pcl = [best_probs[i] for i in both_correct_pcl]\n",
    "    print(f'   Confidence on correctly detected PCL:')\n",
    "    print(f'     Baseline:   mean={np.mean(bl_probs_easy_pcl):.3f}  median={np.median(bl_probs_easy_pcl):.3f}')\n",
    "    print(f'     Best model: mean={np.mean(best_probs_easy_pcl):.3f}  median={np.median(best_probs_easy_pcl):.3f}')\n",
    "\n",
    "if both_correct_nopcl:\n",
    "    bl_probs_easy_nopcl = [baseline_probs[i] for i in both_correct_nopcl]\n",
    "    best_probs_easy_nopcl = [best_probs[i] for i in both_correct_nopcl]\n",
    "    print(f'   Confidence on correctly rejected non-PCL:')\n",
    "    print(f'     Baseline:   mean={np.mean(bl_probs_easy_nopcl):.3f}  median={np.median(bl_probs_easy_nopcl):.3f}')\n",
    "    print(f'     Best model: mean={np.mean(best_probs_easy_nopcl):.3f}  median={np.median(best_probs_easy_nopcl):.3f}')\n",
    "\n",
    "# ============================================================\n",
    "# 4. Quadrant b) deep dive — both models wrong (hardest cases)\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'4. BOTH MODELS WRONG — hardest examples ({len(both_wrong)} total)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "# Split into shared FN vs shared FP\n",
    "shared_fn = [i for i in both_wrong if true_labels[i] == 1]\n",
    "shared_fp = [i for i in both_wrong if true_labels[i] == 0]\n",
    "print(f'   Shared false negatives (PCL missed by both): {len(shared_fn)}')\n",
    "print(f'   Shared false positives (non-PCL flagged by both): {len(shared_fp)}')\n",
    "\n",
    "print(f'\\n   --- Shared FN: PCL that NEITHER model detects ({min(8, len(shared_fn))}/{len(shared_fn)} shown) ---')\n",
    "for idx in shared_fn[:8]:\n",
    "    print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "    print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "    print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "    print()\n",
    "\n",
    "print(f'   --- Shared FP: non-PCL flagged by BOTH models ({min(5, len(shared_fp))}/{len(shared_fp)} shown) ---')\n",
    "for idx in shared_fp[:5]:\n",
    "    print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "    print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "    print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "    print()\n",
    "\n",
    "# ============================================================\n",
    "# 5. Quadrant c) — regressions (baseline got right, best got wrong)\n",
    "# ============================================================\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'5. REGRESSIONS — baseline correct, best model wrong ({len(base_right_best_wrong)} total)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "regressions_fn = [i for i in base_right_best_wrong if true_labels[i] == 1]\n",
    "regressions_fp = [i for i in base_right_best_wrong if true_labels[i] == 0]\n",
    "print(f'   New false negatives (best misses PCL baseline caught): {len(regressions_fn)}')\n",
    "print(f'   New false positives (best flags non-PCL baseline ignored): {len(regressions_fp)}')\n",
    "\n",
    "for label, indices, tag in [('New FN', regressions_fn, 'PCL missed'), ('New FP', regressions_fp, 'non-PCL flagged')]:\n",
    "    print(f'\\n   --- {label}: {tag} by best but not baseline ({min(5, len(indices))}/{len(indices)} shown) ---')\n",
    "    for idx in indices[:5]:\n",
    "        print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "        print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "        print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "        print()\n",
    "\n",
    "# ============================================================\n",
    "# 6. Quadrant d) — improvements (best got right, baseline got wrong)\n",
    "# ============================================================\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'6. IMPROVEMENTS — best model correct, baseline wrong ({len(base_wrong_best_right)} total)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "improvements_fn_fixed = [i for i in base_wrong_best_right if true_labels[i] == 1]\n",
    "improvements_fp_fixed = [i for i in base_wrong_best_right if true_labels[i] == 0]\n",
    "print(f'   Fixed false negatives (best catches PCL baseline missed): {len(improvements_fn_fixed)}')\n",
    "print(f'   Fixed false positives (best correctly ignores non-PCL): {len(improvements_fp_fixed)}')\n",
    "\n",
    "for label, indices, tag in [('Fixed FN', improvements_fn_fixed, 'PCL caught'),\n",
    "                             ('Fixed FP', improvements_fp_fixed, 'non-PCL correctly ignored')]:\n",
    "    print(f'\\n   --- {label}: {tag} by best but not baseline ({min(5, len(indices))}/{len(indices)} shown) ---')\n",
    "    for idx in indices[:5]:\n",
    "        print(f'   [{idx}] (bl_prob={baseline_probs[idx]:.3f}, best_prob={best_probs[idx]:.3f})')\n",
    "        print(f'         keyword=\"{dev_keywords[idx]}\"')\n",
    "        print(f'         \"{dev_texts[idx][:200]}\"')\n",
    "        print()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Keyword analysis — which topics are hardest?\n",
    "# ============================================================\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'7. KEYWORD / TOPIC ANALYSIS')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "# Per-keyword error rates for the best model\n",
    "keyword_stats = {}\n",
    "for i, (true, pred, kw) in enumerate(zip(true_labels, best_preds, dev_keywords)):\n",
    "    kw = str(kw).strip()\n",
    "    if kw not in keyword_stats:\n",
    "        keyword_stats[kw] = {'total': 0, 'pcl': 0, 'tp': 0, 'fp': 0, 'fn': 0}\n",
    "    keyword_stats[kw]['total'] += 1\n",
    "    if true == 1:\n",
    "        keyword_stats[kw]['pcl'] += 1\n",
    "        if pred == 1:\n",
    "            keyword_stats[kw]['tp'] += 1\n",
    "        else:\n",
    "            keyword_stats[kw]['fn'] += 1\n",
    "    elif pred == 1:\n",
    "        keyword_stats[kw]['fp'] += 1\n",
    "\n",
    "kw_df = pd.DataFrame([\n",
    "    {\n",
    "        'Keyword': kw,\n",
    "        'Total': s['total'],\n",
    "        'PCL': s['pcl'],\n",
    "        'TP': s['tp'],\n",
    "        'FP': s['fp'],\n",
    "        'FN': s['fn'],\n",
    "        'Recall': s['tp'] / s['pcl'] if s['pcl'] > 0 else None,\n",
    "        'Precision': s['tp'] / (s['tp'] + s['fp']) if (s['tp'] + s['fp']) > 0 else None,\n",
    "    }\n",
    "    for kw, s in keyword_stats.items()\n",
    "]).sort_values('PCL', ascending=False)\n",
    "\n",
    "print('\\n   Best model performance by keyword (sorted by PCL count):')\n",
    "print(kw_df.to_string(index=False, float_format='{:.2f}'.format))\n",
    "\n",
    "# Keywords most associated with errors\n",
    "print(f'\\n   Keywords with lowest recall (most missed PCL):')\n",
    "low_recall = kw_df[kw_df['PCL'] >= 5].nsmallest(5, 'Recall')\n",
    "for _, row in low_recall.iterrows():\n",
    "    print(f'     \"{row[\"Keyword\"]}\": recall={row[\"Recall\"]:.2f} ({row[\"TP\"]}/{row[\"PCL\"]} caught, {row[\"FN\"]} missed)')\n",
    "\n",
    "print(f'\\n   Keywords with most false positives:')\n",
    "high_fp = kw_df.nlargest(5, 'FP')\n",
    "for _, row in high_fp.iterrows():\n",
    "    print(f'     \"{row[\"Keyword\"]}\": {row[\"FP\"]} FP out of {row[\"Total\"]} samples')\n",
    "\n",
    "# ============================================================\n",
    "# 8. PCL category analysis — which PCL types are hardest?\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'8. PCL CATEGORY ANALYSIS (best model errors)')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "CATEGORY_NAMES = [\n",
    "    'Unbalanced power relations', 'Shallow solution', 'Presupposition',\n",
    "    'Authority voice', 'Metaphor', 'Compassion', 'The poorer the merrier'\n",
    "]\n",
    "\n",
    "# For PCL samples only, check which categories are missed vs caught\n",
    "pcl_indices = [i for i, t in enumerate(true_labels) if t == 1]\n",
    "caught = [i for i in pcl_indices if best_preds[i] == 1]\n",
    "missed = [i for i in pcl_indices if best_preds[i] == 0]\n",
    "\n",
    "cat_caught = np.array([dev_categories[i] for i in caught]) if caught else np.zeros((0, 7))\n",
    "cat_missed = np.array([dev_categories[i] for i in missed]) if missed else np.zeros((0, 7))\n",
    "\n",
    "print(f'\\n   PCL samples: {len(pcl_indices)} total, {len(caught)} caught (TP), {len(missed)} missed (FN)')\n",
    "print(f'\\n   {\"Category\":<30} {\"Caught\":>8} {\"Missed\":>8} {\"Recall\":>8}')\n",
    "print(f'   {\"-\"*56}')\n",
    "for j, cat_name in enumerate(CATEGORY_NAMES):\n",
    "    n_caught = int(cat_caught[:, j].sum()) if len(cat_caught) > 0 else 0\n",
    "    n_missed = int(cat_missed[:, j].sum()) if len(cat_missed) > 0 else 0\n",
    "    total = n_caught + n_missed\n",
    "    recall = n_caught / total if total > 0 else 0\n",
    "    print(f'   {cat_name:<30} {n_caught:>8} {n_missed:>8} {recall:>8.2f}')\n",
    "\n",
    "# ============================================================\n",
    "# 9. Confidence analysis — model certainty on errors\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'9. CONFIDENCE ANALYSIS')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "best_fn_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 1 and p == 0]\n",
    "best_fp_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 0 and p == 1]\n",
    "best_tp_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 1 and p == 1]\n",
    "best_tn_indices = [i for i, (t, p) in enumerate(zip(true_labels, best_preds)) if t == 0 and p == 0]\n",
    "\n",
    "for name, indices in [('True Positives', best_tp_indices), ('True Negatives', best_tn_indices),\n",
    "                       ('False Positives', best_fp_indices), ('False Negatives', best_fn_indices)]:\n",
    "    if indices:\n",
    "        probs_subset = [best_probs[i] for i in indices]\n",
    "        print(f'   {name:<20} n={len(indices):>4}  mean_prob={np.mean(probs_subset):.3f}  '\n",
    "              f'median={np.median(probs_subset):.3f}  std={np.std(probs_subset):.3f}')\n",
    "\n",
    "print(f'\\n   False negatives are PCL samples where the model assigned LOW probability.')\n",
    "print(f'   If FN mean_prob is close to the threshold, the model is uncertain — threshold tuning may help.')\n",
    "print(f'   If FN mean_prob is very low, the model genuinely cannot recognise these as PCL.')\n",
    "\n",
    "# ============================================================\n",
    "# 10. Text length analysis\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'10. TEXT LENGTH ANALYSIS')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "for name, indices in [('All samples', list(range(n))),\n",
    "                       ('True Positives', best_tp_indices), ('False Positives', best_fp_indices),\n",
    "                       ('False Negatives', best_fn_indices), ('True Negatives', best_tn_indices)]:\n",
    "    if indices:\n",
    "        lengths = [len(dev_texts[i].split()) for i in indices]\n",
    "        print(f'   {name:<20} n={len(indices):>4}  mean={np.mean(lengths):.1f}  '\n",
    "              f'median={np.median(lengths):.1f}  max={max(lengths)}')\n",
    "\n",
    "# ============================================================\n",
    "# 11. Lexical patterns in errors — frequent words in FN vs TP\n",
    "# ============================================================\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'11. LEXICAL PATTERNS: words over-represented in false negatives')\n",
    "print(f'{\"=\"*70}')\n",
    "\n",
    "def get_word_freq(indices, texts):\n",
    "    words = []\n",
    "    for i in indices:\n",
    "        words.extend(texts[i].lower().split())\n",
    "    return Counter(words)\n",
    "\n",
    "stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "             'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "             'should', 'may', 'might', 'shall', 'can', 'to', 'of', 'in', 'for',\n",
    "             'on', 'with', 'at', 'by', 'from', 'as', 'into', 'through', 'during',\n",
    "             'and', 'but', 'or', 'nor', 'not', 'so', 'yet', 'both', 'either',\n",
    "             'neither', 'each', 'every', 'all', 'any', 'few', 'more', 'most',\n",
    "             'other', 'some', 'such', 'no', 'than', 'too', 'very', 'just',\n",
    "             'that', 'this', 'these', 'those', 'it', 'its', 'they', 'them',\n",
    "             'their', 'we', 'our', 'he', 'she', 'his', 'her', 'who', 'which',\n",
    "             'what', 'when', 'where', 'how', 'if', 'then', 'about', 'up', 'out',\n",
    "             'also', 'only', 'over', 'after', 'before', 'between', 'under', 'again',\n",
    "             'there', 'here', 'because', 'while', 'i', 'you', 'my', 'your', 'me',\n",
    "             'him', 'us', 'said', 'one', 'two', 'even', 'get', 'make', 'like',\n",
    "             'much', 'many', 'well', 'back', 'made', 'still', 'going', 'way'}\n",
    "\n",
    "fn_freq = get_word_freq(best_fn_indices, dev_texts)\n",
    "tp_freq = get_word_freq(best_tp_indices, dev_texts)\n",
    "\n",
    "# Normalise to rates and find words distinctive to FN\n",
    "fn_total = sum(fn_freq.values()) or 1\n",
    "tp_total = sum(tp_freq.values()) or 1\n",
    "\n",
    "distinctive_fn = []\n",
    "for word, count in fn_freq.most_common(500):\n",
    "    if word in stopwords or len(word) <= 2:\n",
    "        continue\n",
    "    fn_rate = count / fn_total\n",
    "    tp_rate = tp_freq.get(word, 0) / tp_total\n",
    "    if fn_rate > tp_rate * 1.5 and count >= 3:\n",
    "        distinctive_fn.append((word, count, fn_rate, tp_rate))\n",
    "\n",
    "print(f'\\n   Words more frequent in false negatives than true positives:')\n",
    "print(f'   {\"Word\":<20} {\"FN count\":>10} {\"FN rate\":>10} {\"TP rate\":>10} {\"Ratio\":>8}')\n",
    "print(f'   {\"-\"*60}')\n",
    "for word, count, fn_rate, tp_rate in sorted(distinctive_fn, key=lambda x: -x[2])[:15]:\n",
    "    ratio = fn_rate / tp_rate if tp_rate > 0 else float('inf')\n",
    "    print(f'   {word:<20} {count:>10} {fn_rate:>10.4f} {tp_rate:>10.4f} {ratio:>8.1f}x')\n",
    "\n",
    "print(f'\\n   Interpretation: these words appear in PCL text the model fails to detect.')\n",
    "print(f'   High-ratio words suggest the model may associate them with non-PCL contexts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved to /vol/bitbucket/akc123/PCL_Detection/custom_metrics.png\n"
     ]
    }
   ],
   "source": [
    "# ---- Precision-Recall Curve: Best Model vs Baseline ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Left: Precision-Recall Curves ---\n",
    "ax = axes[0]\n",
    "\n",
    "# Best model PR curve\n",
    "prec_best, rec_best, thresholds_best = precision_recall_curve(\n",
    "    best_metrics['labels'], best_metrics['probs'], pos_label=1)\n",
    "ax.plot(rec_best, prec_best, label=f'Best model ({best_key})', color='tab:blue', linewidth=2)\n",
    "\n",
    "# Baseline PR curve\n",
    "prec_base, rec_base, thresholds_base = precision_recall_curve(\n",
    "    baseline_dev_metrics['labels'], baseline_dev_metrics['probs'], pos_label=1)\n",
    "ax.plot(rec_base, prec_base, label='RoBERTa-base baseline', color='tab:orange', linewidth=2, linestyle='--')\n",
    "\n",
    "# Mark the operating points\n",
    "ax.scatter([best_metrics['recall']], [best_metrics['precision']],\n",
    "           marker='*', s=200, color='tab:blue', zorder=5, label=f'Best @ t={best_threshold:.2f}')\n",
    "ax.scatter([baseline_dev_metrics['recall']], [baseline_dev_metrics['precision']],\n",
    "           marker='*', s=200, color='tab:orange', zorder=5, label=f'Baseline @ t={thresh_bl:.2f}')\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curve', fontsize=13)\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_xlim([0, 1.02])\n",
    "ax.set_ylim([0, 1.02])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Right: Confusion Matrix Heatmap (Best Model) ---\n",
    "ax = axes[1]\n",
    "\n",
    "cm = confusion_matrix(best_metrics['labels'], best_metrics['preds'], labels=[0, 1])\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "classes = ['No PCL (0)', 'PCL (1)']\n",
    "tick_marks = [0, 1]\n",
    "ax.set_xticks(tick_marks)\n",
    "ax.set_xticklabels(classes, fontsize=11)\n",
    "ax.set_yticks(tick_marks)\n",
    "ax.set_yticklabels(classes, fontsize=11)\n",
    "\n",
    "# Annotate each cell with count\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        ax.text(j, i, f'{cm[i, j]}', ha='center', va='center', fontsize=16, fontweight='bold', color=color)\n",
    "\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix — Best Model ({best_key}, t={best_threshold:.2f})', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{BASE_DIR}/custom_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure saved to {BASE_DIR}/custom_metrics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Ablation Study Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ABLATION STUDY\n",
      "================================================================================\n",
      "                         Config Weighted CE Multi-task Thresh Opt Dev F1\n",
      "Baseline (unweighted CE, t=0.5)          No         No         No 0.6139\n",
      "                Config A (full)         Yes        Yes        Yes 0.6373\n",
      "                Config B (full)         Yes        Yes        Yes 0.6133\n",
      "               A w/o Multi-task         Yes         No        Yes 0.6005\n",
      "            A w/o Threshold Opt         Yes        Yes         No 0.6061\n",
      "              A w/o Weighted CE          No        Yes        Yes 0.6210\n",
      "\n",
      "Component contributions (F1 drop when removed from Config A):\n",
      "  Multi-task learning:    +0.0368\n",
      "  Threshold optimisation: +0.0312\n",
      "  Weighted CE:            +0.0162\n",
      "\n",
      "Total improvement over baseline: +0.0233 F1\n"
     ]
    }
   ],
   "source": [
    "# ---- Ablation Study Summary ----\n",
    "ablation_table = pd.DataFrame([\n",
    "    {'Config': 'Baseline (unweighted CE, t=0.5)', 'Weighted CE': 'No', 'Multi-task': 'No', 'Thresh Opt': 'No',\n",
    "     'Dev F1': f'{thresh_metrics_bl[\"f1\"]:.4f}'},\n",
    "    {'Config': 'Config A (full)', 'Weighted CE': 'Yes', 'Multi-task': 'Yes', 'Thresh Opt': 'Yes',\n",
    "     'Dev F1': f'{thresh_metrics_a[\"f1\"]:.4f}'},\n",
    "    {'Config': 'Config B (full)', 'Weighted CE': 'Yes', 'Multi-task': 'Yes', 'Thresh Opt': 'Yes',\n",
    "     'Dev F1': f'{thresh_metrics_b[\"f1\"]:.4f}'},\n",
    "    {'Config': 'A w/o Multi-task', 'Weighted CE': 'Yes', 'Multi-task': 'No', 'Thresh Opt': 'Yes',\n",
    "     'Dev F1': f'{thresh_metrics_abl_nomt[\"f1\"]:.4f}'},\n",
    "    {'Config': 'A w/o Threshold Opt', 'Weighted CE': 'Yes', 'Multi-task': 'Yes', 'Thresh Opt': 'No',\n",
    "     'Dev F1': f'{thresh_metrics_abl_nothresh[\"f1\"]:.4f}'},\n",
    "    {'Config': 'A w/o Weighted CE', 'Weighted CE': 'No', 'Multi-task': 'Yes', 'Thresh Opt': 'Yes',\n",
    "     'Dev F1': f'{thresh_metrics_abl_nowe[\"f1\"]:.4f}'},\n",
    "])\n",
    "\n",
    "print('='*80)\n",
    "print('ABLATION STUDY')\n",
    "print('='*80)\n",
    "print(ablation_table.to_string(index=False))\n",
    "\n",
    "# Component contributions\n",
    "full_f1 = thresh_metrics_a['f1']\n",
    "print(f'\\nComponent contributions (F1 drop when removed from Config A):')\n",
    "print(f'  Multi-task learning:    {full_f1 - thresh_metrics_abl_nomt[\"f1\"]:+.4f}')\n",
    "print(f'  Threshold optimisation: {full_f1 - thresh_metrics_abl_nothresh[\"f1\"]:+.4f}')\n",
    "print(f'  Weighted CE:            {full_f1 - thresh_metrics_abl_nowe[\"f1\"]:+.4f}')\n",
    "print(f'\\nTotal improvement over baseline: {full_f1 - thresh_metrics_bl[\"f1\"]:+.4f} F1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
